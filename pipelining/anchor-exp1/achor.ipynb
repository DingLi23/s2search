{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail to not force global because 1 worker available\n",
      "get ranker in 77418 with global setting: True and gb_ranker len 1\n",
      "[Main taks:-1] compute 92938 scores with worker 77418\n",
      "[Main taks][03/26/2022, 13:51:04] 92938 scores within 35.166157 sec \n",
      "fail to not force global because 1 worker available\n",
      "get ranker in 77418 with global setting: True and gb_ranker len 1\n",
      "[Main taks:-1] compute 92938 scores with worker 77418\n",
      "[Main taks][03/26/2022, 13:51:57] 92938 scores within 47.045227 sec \n",
      "fail to not force global because 1 worker available\n",
      "get ranker in 77418 with global setting: True and gb_ranker len 1\n",
      "[Main taks:-1] compute 92938 scores with worker 77418\n",
      "[Main taks][03/26/2022, 13:52:18] 92938 scores within 15.226854 sec \n",
      "fail to not force global because 1 worker available\n",
      "get ranker in 77418 with global setting: True and gb_ranker len 1\n",
      "[Main taks:-1] compute 92938 scores with worker 77418\n",
      "[Main taks][03/26/2022, 13:52:48] 92938 scores within 25.429801 sec \n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np, sys, os, pandas as pd,json, os\n",
    "sys.path.insert(1, '../../')\n",
    "from getting_data import load_sample, feature_key_list, categorical_feature_key_list\n",
    "from ranker_helper import get_scores\n",
    "from s2search_score_pdp import save_pdp_to_npz\n",
    "\n",
    "setting = 0\n",
    "exp_name = 'exp5'\n",
    "sample_name = 'cslg'\n",
    "\n",
    "query = 'Machine Learning'\n",
    "\n",
    "categorical_name = {}\n",
    "\n",
    "def remove_duplicate(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "categorical_name_file = os.path.join('.', 'scores', f'{sample_name}_categorical_name.npz')\n",
    "\n",
    "for i in range(len(feature_key_list)):\n",
    "    feature_name = feature_key_list[i]\n",
    "    if feature_name in categorical_feature_key_list:\n",
    "        df = load_sample(exp_name, sample_name, query=query, sort=feature_name, rank_f=get_scores)\n",
    "        if feature_name == 'authors':\n",
    "            l = [json.dumps(x) for x in df[feature_name]]\n",
    "        else:\n",
    "            l = list(df[feature_name])\n",
    "        categorical_name[i] = remove_duplicate(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_sample(exp_name, sample_name)\n",
    "paper_data = json.loads(df.to_json(orient='records'))\n",
    "\n",
    "y_pred_file = os.path.join('.', 'scores', f'{sample_name}_y_pred.npz')\n",
    "if not os.path.exists(y_pred_file):\n",
    "    y_pred = get_scores(query, paper_data)\n",
    "    save_pdp_to_npz('.', y_pred_file, y_pred)\n",
    "else:\n",
    "    y_pred = np.load(y_pred_file)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(score):\n",
    "    if score <= -17:\n",
    "        return '(,-17]'\n",
    "    elif score <= -10:\n",
    "        return '(-17, -10]'\n",
    "    elif score <= -5:\n",
    "        return '(-10, -5]'    \n",
    "    elif score <= 0:\n",
    "        return '(-5, <0]'  \n",
    "    elif score <= 3:\n",
    "        return '(0, 3]'\n",
    "    elif score <= 5:\n",
    "        return '(3, 5]'\n",
    "    elif score <= 6:\n",
    "        return '(5, 6]'\n",
    "    elif score <= 7:\n",
    "        return '(6, 7]'\n",
    "    elif score <= 8:\n",
    "        return '(7, 8]'\n",
    "    elif score <= 9:\n",
    "        return '(8, 9]'\n",
    "    else:\n",
    "        return '(9,)'\n",
    "\n",
    "# make class_name\n",
    "class_name = ['(,-17]','(-17, -10]','(-10, -5]','(-5, <0]','(0, 3]','(3, 5]','(5, 6]','(6, 7]','(7, 8]','(8, 9]','(9,)']\n",
    "\n",
    "categorical_name_map = {}\n",
    "\n",
    "for i in range(len(feature_key_list)):\n",
    "    feature_name = feature_key_list[i]\n",
    "    if feature_name in categorical_feature_key_list:\n",
    "        categorical_name_map[i] = {}\n",
    "        values = categorical_name[i]\n",
    "        for j in range(len(values)):\n",
    "            value = values[j]\n",
    "            categorical_name_map[i][value] = j\n",
    "\n",
    "# encoding data\n",
    "for i in range(len(paper_data)):\n",
    "    paper_data[i] = [\n",
    "        categorical_name_map[0][paper_data[i]['title']], categorical_name_map[1][paper_data[i]['abstract']],\n",
    "        categorical_name_map[2][paper_data[i]['venue']], categorical_name_map[3][json.dumps(paper_data[i]['authors'])],\n",
    "        paper_data[i]['year'],\n",
    "        paper_data[i]['n_citations']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55397 28139  2995 84492  2019    61]\n",
      "title          Pricing options and computing implied volatili...\n",
      "abstract       This paper proposes a data-driven approach, by...\n",
      "venue                                                      Risks\n",
      "authors        [Shuaiqiang  Liu, Cornelis W. Oosterlee, Sande...\n",
      "year                                                        2019\n",
      "n_citations                                                   61\n",
      "Name: 92937, dtype: object\n",
      "{'title': 'Pricing options and computing implied volatilities using neural networks', 'abstract': 'This paper proposes a data-driven approach, by means of an Artificial Neural Network (ANN), to value financial options and to calculate implied volatilities with the aim of accelerating the corresponding numerical methods. With ANNs being universal function approximators, this method trains an optimized ANN on a data set generated by a sophisticated financial model, and runs the trained ANN as an agent of the original solver in a fast and efficient way. We test this approach on three different types of solvers, including the analytic solution for the Black-Scholes equation, the COS method for the Heston stochastic volatility model and Brentâ€™s iterative root-finding method for the calculation of implied volatilities. The numerical results show that the ANN solver can reduce the computing time significantly.', 'venue': 'Risks', 'authors': ['Shuaiqiang  Liu', 'Cornelis W. Oosterlee', 'Sander M. Bohte'], 'year': 2019, 'n_citations': 61}\n"
     ]
    }
   ],
   "source": [
    "def decode_paper(encoded_p):\n",
    "    return dict(\n",
    "        title=categorical_name[0][encoded_p[0]],\n",
    "        abstract=categorical_name[1][encoded_p[1]],\n",
    "        venue=categorical_name[2][encoded_p[2]],\n",
    "        authors=json.loads(categorical_name[3][encoded_p[3]]),\n",
    "        year=encoded_p[4],\n",
    "        n_citations=encoded_p[5],\n",
    "    )\n",
    "\n",
    "decoded_i = 92937\n",
    "\n",
    "print(paper_data[decoded_i])\n",
    "print(df.iloc[decoded_i])\n",
    "print(decode_paper(paper_data[decoded_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor import anchor_tabular\n",
    "\n",
    "paper_data = np.array(paper_data)\n",
    "\n",
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    class_name,\n",
    "    feature_key_list,\n",
    "    paper_data,\n",
    "    categorical_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_fn(x):\n",
    "    predictions = get_scores(query, [decode_paper(p) for p in x], ptf = False)\n",
    "    encoded_pred = [class_name.index(get_class(pp)) for pp in predictions]\n",
    "    return np.array(encoded_pred)\n",
    "\n",
    "idx = 3\n",
    "exp = explainer.explain_instance(paper_data[idx], pred_fn, threshold=0.9999, tau=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  (-10, -5]\n",
      "-------------------\n",
      "Anchor: \n",
      "abstract = Lung cancer is the leading cause of cancer-related death worldwide. Early diagnosis of pulmonary nodules in Computed Tomography (CT) chest scans provides an opportunity for designing effective treatment and making financial and care plans. In this paper, we consider the problem of diagnostic classification between benign and malignant lung nodules in CT images, which aims to learn a direct mapping from 3D images to class labels. To achieve this goal, four two-pathway Convolutional Neural Networks (CNN) are proposed, including a basic 3D CNN, a novel multi-output network, a 3D DenseNet, and an augmented 3D DenseNet with multi-outputs. These four networks are evaluated on the public LIDC-IDRI dataset and outperform most existing methods. In particular, the 3D multi-output DenseNet (MoDenseNet) achieves the state-of-the-art classification accuracy on the task of end-to-end lung nodule diagnosis. In addition, the networks pretrained on the LIDC-IDRI dataset can be further extended to handle smaller datasets using transfer learning. This is demonstrated on our dataset with encouraging prediction accuracy in lung nodule classification.\n",
      "AND\n",
      "title = Diagnostic classification of lung nodules using 3D neural networks\n",
      "AND\n",
      "year <= 2018.00\n",
      "AND\n",
      "n_citations > 15.00\n",
      "AND\n",
      "venue = 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)\n",
      "\n",
      "Precision: 1.00\n",
      "Coverage: 0.00\n"
     ]
    }
   ],
   "source": [
    "print('Prediction: ', explainer.class_names[pred_fn([paper_data[idx]])[0]])\n",
    "print('-------------------')\n",
    "print('Anchor: \\n%s' % ('\\nAND\\n'.join(exp.names())))\n",
    "print('\\nPrecision: %.2f' % exp.precision())\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract 0.6831683168316832\n",
      "title 0.2574257425742573\n",
      "year 0.04378094059405946\n",
      "n_citations -0.0062728102189780754\n",
      "venue 0.021897810218978075\n"
     ]
    }
   ],
   "source": [
    "def extract_feature_keys(l):\n",
    "    if len(l) == 0:\n",
    "        return []\n",
    "    return [anchor.split(' ')[0] for anchor in l]\n",
    "\n",
    "previous_single_precision = 0\n",
    "for i in range(len(exp.names())):\n",
    "    previous_names = extract_feature_keys(exp.names(i - 1) if i > 0 else [])\n",
    "    current_names = extract_feature_keys(exp.names(i))\n",
    "    print([name for name in current_names if name not in previous_names][0], exp.precision(i) - previous_single_precision)\n",
    "    previous_single_precision = exp.precision(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract = Lung cancer is the leading cause of cancer-related death worldwide. Early diagnosis of pulmonary nodules in Computed Tomography (CT) chest scans provides an opportunity for designing effective treatment and making financial and care plans. In this paper, we consider the problem of diagnostic classification between benign and malignant lung nodules in CT images, which aims to learn a direct mapping from 3D images to class labels. To achieve this goal, four two-pathway Convolutional Neural Networks (CNN) are proposed, including a basic 3D CNN, a novel multi-output network, a 3D DenseNet, and an augmented 3D DenseNet with multi-outputs. These four networks are evaluated on the public LIDC-IDRI dataset and outperform most existing methods. In particular, the 3D multi-output DenseNet (MoDenseNet) achieves the state-of-the-art classification accuracy on the task of end-to-end lung nodule diagnosis. In addition, the networks pretrained on the LIDC-IDRI dataset can be further extended to handle smaller datasets using transfer learning. This is demonstrated on our dataset with encouraging prediction accuracy in lung nodule classification.\n",
      "title = Diagnostic classification of lung nodules using 3D neural networks\n",
      "year <= 2018.00\n",
      "n_citations > 15.00\n",
      "venue = 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': [0.2574257425742573],\n",
       " 'abstract': [0.6831683168316832],\n",
       " 'venue': [0.021897810218978075],\n",
       " 'authors': [],\n",
       " 'year': [0.04378094059405946],\n",
       " 'n_citations': [-0.0062728102189780754]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = dict(\n",
    "    title=[],\n",
    "    abstract=[],\n",
    "    venue=[],\n",
    "    authors=[],\n",
    "    year=[],\n",
    "    n_citations=[],\n",
    ")\n",
    "previous_single_precision = 0\n",
    "\n",
    "for j in range(len(exp.names())):\n",
    "    name = exp.names()[j]\n",
    "    print(name)\n",
    "    current_single_precision = exp.precision(j) - previous_single_precision\n",
    "    previous_single_precision = exp.precision(j)\n",
    "    for feature_name in metrics.keys():\n",
    "        if name.startswith(f'{feature_name}'):\n",
    "            metrics[feature_name].append(current_single_precision)\n",
    "\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cf082d97203dbc4e6105f8e92bf9fdc7b5fae703590b3e03586d98d926929c4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('s2search397')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
