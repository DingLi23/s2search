{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np, sys, os, pandas as pd,json, os\n",
    "sys.path.insert(1, '../../')\n",
    "from getting_data import load_sample, feature_key_list, categorical_feature_key_list\n",
    "from s2search_score_pipelining import get_scores\n",
    "from s2search_score_pdp import save_pdp_to_npz\n",
    "\n",
    "setting = 0\n",
    "exp_name = 'exp5'\n",
    "sample_name = 'cslg'\n",
    "\n",
    "query = 'Machine Learning'\n",
    "\n",
    "categorical_name = {}\n",
    "\n",
    "def remove_duplicate(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "categorical_name_file = os.path.join('.', 'scores', f'{sample_name}_categorical_name.npz')\n",
    "\n",
    "if not os.path.exists(categorical_name_file):\n",
    "    # get categorical_name\n",
    "    for i in range(len(feature_key_list)):\n",
    "        feature_name = feature_key_list[i]\n",
    "        if feature_name in categorical_feature_key_list:\n",
    "            df = load_sample(exp_name, sample_name, query=query, sort=feature_name, rank_f=get_scores)\n",
    "            if feature_name == 'authors':\n",
    "                l = [json.dumps(x) for x in df[feature_name]]\n",
    "            else:\n",
    "                l = list(df[feature_name])\n",
    "            categorical_name[i] = remove_duplicate(l)\n",
    "            \n",
    "    save_pdp_to_npz('.', categorical_name_file, title=categorical_name[0], abstract=categorical_name[1], \\\n",
    "    venue=categorical_name[2], authors=categorical_name[3])\n",
    "else:\n",
    "    load = np.load(categorical_name_file)\n",
    "    categorical_name[0] = load['title']\n",
    "    categorical_name[1] = load['abstract']\n",
    "    categorical_name[2] = load['venue']\n",
    "    categorical_name[3] = load['authors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_sample(exp_name, sample_name)\n",
    "paper_data = json.loads(df.to_json(orient='records'))\n",
    "\n",
    "y_pred_file = os.path.join('.', 'scores', f'{sample_name}_y_pred.npz')\n",
    "if not os.path.exists(y_pred_file):\n",
    "    y_pred = get_scores(query, paper_data)\n",
    "    save_pdp_to_npz('.', y_pred_file, y_pred)\n",
    "else:\n",
    "    y_pred = np.load(y_pred_file)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(score):\n",
    "    if score <= -17:\n",
    "        return '(,-17]'\n",
    "    elif score <= -10:\n",
    "        return '(-17, -10]'\n",
    "    elif score <= -5:\n",
    "        return '(-10, -5]'    \n",
    "    elif score <= 0:\n",
    "        return '(-5, <0]'  \n",
    "    elif score <= 3:\n",
    "        return '(0, 3]'\n",
    "    elif score <= 5:\n",
    "        return '(3, 5]'\n",
    "    elif score <= 6:\n",
    "        return '(5, 6]'\n",
    "    elif score <= 7:\n",
    "        return '(6, 7]'\n",
    "    elif score <= 8:\n",
    "        return '(7, 8]'\n",
    "    elif score <= 9:\n",
    "        return '(8, 9]'\n",
    "    else:\n",
    "        return '(9,)'\n",
    "\n",
    "# make class_name\n",
    "class_name = ['(,-17]','(-17, -10]','(-10, -5]','(-5, <0]','(0, 3]','(3, 5]','(5, 6]','(6, 7]','(7, 8]','(8, 9]','(9,)']\n",
    "\n",
    "categorical_name_map = {}\n",
    "\n",
    "for i in range(len(feature_key_list)):\n",
    "    feature_name = feature_key_list[i]\n",
    "    if feature_name in categorical_feature_key_list:\n",
    "        categorical_name_map[i] = {}\n",
    "        values = categorical_name[i]\n",
    "        for j in range(len(values)):\n",
    "            value = values[j]\n",
    "            categorical_name_map[i][value] = j\n",
    "\n",
    "# encoding data\n",
    "for i in range(len(paper_data)):\n",
    "    paper_data[i] = [\n",
    "        categorical_name_map[0][paper_data[i]['title']], categorical_name_map[1][paper_data[i]['abstract']],\n",
    "        categorical_name_map[2][paper_data[i]['venue']], categorical_name_map[3][json.dumps(paper_data[i]['authors'])],\n",
    "        paper_data[i]['year'],\n",
    "        paper_data[i]['n_citations']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55397, 28139, 2995, 84492, 2019, 61]\n",
      "title          Pricing options and computing implied volatili...\n",
      "abstract       This paper proposes a data-driven approach, by...\n",
      "venue                                                      Risks\n",
      "authors        [Shuaiqiang  Liu, Cornelis W. Oosterlee, Sande...\n",
      "year                                                        2019\n",
      "n_citations                                                   61\n",
      "Name: 92937, dtype: object\n",
      "{'title': 'Pricing options and computing implied volatilities using neural networks', 'abstract': 'This paper proposes a data-driven approach, by means of an Artificial Neural Network (ANN), to value financial options and to calculate implied volatilities with the aim of accelerating the corresponding numerical methods. With ANNs being universal function approximators, this method trains an optimized ANN on a data set generated by a sophisticated financial model, and runs the trained ANN as an agent of the original solver in a fast and efficient way. We test this approach on three different types of solvers, including the analytic solution for the Black-Scholes equation, the COS method for the Heston stochastic volatility model and Brentâ€™s iterative root-finding method for the calculation of implied volatilities. The numerical results show that the ANN solver can reduce the computing time significantly.', 'venue': 'Risks', 'authors': ['Shuaiqiang  Liu', 'Cornelis W. Oosterlee', 'Sander M. Bohte'], 'year': 2019, 'n_citations': 61}\n"
     ]
    }
   ],
   "source": [
    "def decode_paper(encoded_p):\n",
    "    return dict(\n",
    "        title=categorical_name[0][encoded_p[0]],\n",
    "        abstract=categorical_name[1][encoded_p[1]],\n",
    "        venue=categorical_name[2][encoded_p[2]],\n",
    "        authors=json.loads(categorical_name[3][encoded_p[3]]),\n",
    "        year=encoded_p[4],\n",
    "        n_citations=encoded_p[5],\n",
    "    )\n",
    "\n",
    "decoded_i = 92937\n",
    "\n",
    "print(paper_data[decoded_i])\n",
    "print(df.iloc[decoded_i])\n",
    "print(decode_paper(paper_data[decoded_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anchor import anchor_tabular\n",
    "\n",
    "paper_data = np.array(paper_data)\n",
    "\n",
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    class_name,\n",
    "    feature_key_list,\n",
    "    paper_data,\n",
    "    categorical_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading process ranker model...\n",
      "Load the process s2 ranker within 10.85 sec\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 100 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n",
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n"
     ]
    }
   ],
   "source": [
    "def pred_fn(x):\n",
    "    predictions = get_scores(query, [decode_paper(p) for p in x], pt = False)\n",
    "    encoded_pred = [class_name.index(get_class(pp)) for pp in predictions]\n",
    "    return np.array(encoded_pred)\n",
    "\n",
    "idx = 9999\n",
    "exp = explainer.explain_instance(paper_data[idx], pred_fn, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get ranker in 66189 with global setting: True and gb_ranker len 1\n",
      "[Main taks:0] compute 1 scores with worker 66189\n",
      "Prediction:  (-10, -5]\n",
      "-------------------\n",
      "Anchor: \n",
      "abstract = In this paper, a novel unsupervised low-rank representation model, i.e., Auto-weighted Low-Rank Representation (ALRR), is proposed to construct a more favorable similarity graph (SG) for clustering. In particular, ALRR enhances the discriminability of SG by capturing the multi-subspace structure and extracting the salient features simultaneously. Specifically, an auto-weighted penalty is introduced to learn a similarity graph by highlighting the effective features, and meanwhile, overshadowing the disturbed features. Consequently, ALRR obtains a similarity graph that can preserve the intrinsic geometrical structures within the data by enforcing a smaller similarity on two dissimilar samples. Moreover, we employ a block-diagonal regularizer to guarantee the learned graph contains k diagonal blocks. This can facilitate a more discriminative representation learning for clustering tasks. Extensive experimental results on synthetic and real databases demonstrate the superiority of ALRR over other state-of-the-art methods with a margin of 1.8%âˆ¼10.8%.\n",
      "AND\n",
      "title = Auto-weighted low-rank representation for clustering\n",
      "AND\n",
      "n_citations <= 4.00\n",
      "\n",
      "Precision: 1.00\n",
      "Coverage: 0.00\n"
     ]
    }
   ],
   "source": [
    "print('Prediction: ', explainer.class_names[pred_fn([paper_data[idx]])[0]])\n",
    "print('-------------------')\n",
    "print('Anchor: \\n%s' % ('\\nAND\\n'.join(exp.names())))\n",
    "print('\\nPrecision: %.2f' % exp.precision())\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 03/12/2022, 12:53:23\n",
      "1 03/12/2022, 12:53:25\n",
      "2 03/12/2022, 12:53:27\n",
      "3 03/12/2022, 12:53:29\n",
      "4 03/12/2022, 12:53:31\n",
      "5 03/12/2022, 12:53:33\n",
      "6 03/12/2022, 12:53:36\n",
      "7 03/12/2022, 12:53:41\n",
      "8 03/12/2022, 12:53:42\n",
      "9 03/12/2022, 12:53:44\n",
      "10 03/12/2022, 12:53:47\n",
      "11 03/12/2022, 12:53:50\n",
      "12 03/12/2022, 12:53:53\n",
      "13 03/12/2022, 12:53:55\n",
      "14 03/12/2022, 12:53:57\n",
      "15 03/12/2022, 12:53:58\n",
      "16 03/12/2022, 12:54:00\n",
      "17 03/12/2022, 12:54:02\n",
      "18 03/12/2022, 12:54:04\n",
      "19 03/12/2022, 12:54:05\n",
      "20 03/12/2022, 12:54:07\n",
      "21 03/12/2022, 12:54:18\n",
      "22 03/12/2022, 12:54:31\n",
      "23 03/12/2022, 12:54:33\n",
      "24 03/12/2022, 12:54:36\n",
      "25 03/12/2022, 12:54:38\n",
      "26 03/12/2022, 12:54:40\n",
      "27 03/12/2022, 12:54:41\n",
      "28 03/12/2022, 12:54:43\n",
      "29 03/12/2022, 12:54:45\n",
      "30 03/12/2022, 12:54:47\n",
      "31 03/12/2022, 12:54:49\n",
      "32 03/12/2022, 12:54:50\n",
      "33 03/12/2022, 12:54:53\n",
      "34 03/12/2022, 12:54:55\n",
      "35 03/12/2022, 12:54:57\n",
      "36 03/12/2022, 12:54:59\n",
      "37 03/12/2022, 12:55:01\n",
      "38 03/12/2022, 12:55:30\n",
      "39 03/12/2022, 12:55:32\n",
      "40 03/12/2022, 12:55:34\n",
      "41 03/12/2022, 12:55:36\n",
      "42 03/12/2022, 12:55:37\n",
      "43 03/12/2022, 12:55:42\n",
      "44 03/12/2022, 12:55:49\n",
      "45 03/12/2022, 12:55:51\n",
      "46 03/12/2022, 12:55:53\n",
      "47 03/12/2022, 12:55:54\n",
      "48 03/12/2022, 12:55:55\n",
      "49 03/12/2022, 12:55:57\n",
      "50 03/12/2022, 12:55:59\n",
      "51 03/12/2022, 12:56:00\n",
      "52 03/12/2022, 12:56:02\n",
      "53 03/12/2022, 12:56:05\n",
      "54 03/12/2022, 12:56:07\n",
      "55 03/12/2022, 12:56:08\n",
      "56 03/12/2022, 12:56:10\n",
      "57 03/12/2022, 12:56:26\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "utc_tz = pytz.timezone('America/Montreal')\n",
    "\n",
    "metrics = dict(\n",
    "    title=0,\n",
    "    abstract=0,\n",
    "    venue=0,\n",
    "    authors=0,\n",
    "    year=0,\n",
    "    n_citations=0,\n",
    ")\n",
    "\n",
    "for i in range(len(paper_data)):\n",
    "    print(i, datetime.datetime.now(tz=utc_tz).strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "    exp = explainer.explain_instance(paper_data[i], pred_fn, threshold=0.95)\n",
    "    for name in exp.names():\n",
    "        for feature_name in metrics.keys():\n",
    "            if name.startswith(f'{feature_name} = '):\n",
    "                metrics[feature_name] += 1\n",
    "\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cf082d97203dbc4e6105f8e92bf9fdc7b5fae703590b3e03586d98d926929c4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('s2search397')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
