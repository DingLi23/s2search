{"id": 15198, "s2_id": "7ea5d034a98776207fd6e1ae021016ea0c1dab5d", "title": "Stabilizing Equilibrium Models by Jacobian Regularization", "abstract": "Deep equilibrium networks (DEQs) are a new class of models that eschews traditional depth in favor of finding the fixed point of a single nonlinear layer. These models have been shown to achieve performance competitive with the stateof-the-art deep networks while using significantly less memory. Yet they are also slower, brittle to architectural choices, and introduce potential instability to the model. In this paper, we propose a regularization scheme for DEQ models that explicitly regularizes the Jacobian of the fixed-point update equations to stabilize the learning of equilibrium models. We show that this regularization adds only minimal computational cost, significantly stabilizes the fixed-point convergence in both forward and backward passes, and scales well to high-dimensional, realistic domains (e.g., WikiText-103 language modeling and ImageNet classification). Using this method, we demonstrate, for the first time, an implicit-depth model that runs with approximately the same speed and level of performance as popular conventional deep networks such as ResNet-101, while still maintaining the constant memory footprint and architectural simplicity of DEQs. Code is available here.", "venue": "ICML", "authors": ["Shaojie  Bai", "Vladlen  Koltun", "J. Zico Kolter"], "year": 2021, "n_citations": 2}
{"id": 15835, "s2_id": "50c6889b547cc08a203842d5cf5bcb4c58e052b5", "title": "TPsgtR: Neural-Symbolic Tensor Product Scene-Graph-Triplet Representation for Image Captioning", "abstract": "Image captioning can be improved if the structure of the graphical representations can be formulated with conceptual positional binding. In this work, we have introduced a novel technique for caption generation using the neural-symbolic encoding of the scene-graphs, derived from regional visual information of the images and we call it Tensor Product Scene-Graph-Triplet Representation (TP$_{sgt}$R). While, most of the previous works concentrated on identification of the object features in images, we introduce a neuro-symbolic embedding that can embed identified relationships among different regions of the image into concrete forms, instead of relying on the model to compose for any/all combinations. These neural symbolic representation helps in better definition of the neural symbolic space for neuro-symbolic attention and can be transformed to better captions. With this approach, we introduced two novel architectures (TP$_{sgt}$R-TDBU and TP$_{sgt}$R-sTDBU) for comparison and experiment result demonstrates that our approaches outperformed the other models, and generated captions are more comprehensive and natural.", "venue": "ArXiv", "authors": ["Chiranjib  Sur"], "year": 2019, "n_citations": 8}
{"id": 18031, "s2_id": "76b7ed94113927ae1695d4a188a66ee8ac997290", "title": "Machine Vision in the Context of Robotics: A Systematic Literature Review", "abstract": "Machine vision is critical to robotics due to a wide range of applications which rely on input from visual sensors such as autonomous mobile robots and smart production systems. To create the smart homes and systems of tomorrow, an overview about current challenges in the research field would be of use to identify further possible directions, created in a systematic and reproducible manner. In this work a systematic literature review was conducted covering research from the last 10 years. We screened 172 papers from four databases and selected 52 relevant papers. While robustness and computation time were improved greatly, occlusion and lighting variance are still the biggest problems faced. From the number of recent publications, we conclude that the observed field is of relevance and interest to the research community. Further challenges arise in many areas of the field.", "venue": "ArXiv", "authors": ["Javad  Ghofrani", "Robert  Kirschne", "Daniel  Rossburg", "Dirk  Reichelt", "Tom  Dimter"], "year": 2019, "n_citations": 0}
{"id": 18271, "s2_id": "f7dbae4a4ec9995d8dfccc725842e7080a63d835", "title": "Relevance as a Metric for Evaluating Machine Learning Algorithms", "abstract": "In machine learning, the choice of a learning algorithm that is suitable for the application domain is critical. The performance metric used to compare different algorithms must also reflect the concerns of users in the application domain under consideration. In this paper, we propose a novel probability-based performance metric called Relevance Score for evaluating supervised learning algorithms. We evaluate the proposed metric through empirical analysis on a dataset gathered from an intelligent lighting pilot installation. In comparison to the commonly used Classification Accuracy metric, the Relevance Score proves to be more appropriate for a certain class of applications.", "venue": "MLDM", "authors": ["Aravind Kota Gopalakrishna", "Tanir  Ozcelebi", "Antonio  Liotta", "Johan J. Lukkien"], "year": 2013, "n_citations": 18}
{"id": 38902, "s2_id": "5429605fa95ea4eab11e0bc28a687572e7c42abf", "title": "Learning strange attractors with reservoir systems", "abstract": "This paper shows that the celebrated Embedding Theorem of Takens is a particular case of a much more general statement according to which, randomly generated linear state-space representations of generic observations of an invertible dynamical system carry in their wake an embedding of the phase space dynamics into the chosen Euclidean state space. This embedding coincides with a natural generalized synchronization that arises in this setup and that yields a topological conjugacy between the state-space dynamics driven by the generic observations of the dynamical system and the dynamical system itself. This result provides additional tools for the representation, learning, and analysis of chaotic attractors and sheds additional light on the reservoir computing phenomenon that appears in the context of recurrent neural networks.", "venue": "ArXiv", "authors": ["Lyudmila  Grigoryeva", "Allen  Hart", "Juan-Pablo  Ortega"], "year": 2021, "n_citations": 2}
{"id": 45933, "s2_id": "6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2", "title": "Tree-to-tree Neural Networks for Program Translation", "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.", "venue": "NeurIPS", "authors": ["Xinyun  Chen", "Chang  Liu", "Dawn Xiaodong Song"], "year": 2018, "n_citations": 127}
{"id": 51107, "s2_id": "a370d85c593eaf86176743ad406d3f2ea54202a7", "title": "SA-GD: Improved Gradient Descent Learning Strategy with Simulated Annealing", "abstract": "Gradient descent algorithm is the most utilized method when optimizing machine learning issues. However, there exists many local minimums and saddle points in the loss function, especially for high dimensional non-convex optimization problems like deep learning. Gradient descent may make loss function trapped in these local intervals which impedes further optimization, resulting in poor generalization ability. This paper proposes the SA-GD algorithm which introduces the thought of simulated annealing algorithm to gradient descent. SA-GD method offers model the ability of mounting hills in probability, tending to enable the model to jump out of these local areas and converge to a optimal state finally. We took CNN models as an example and tested the basic CNN models on various benchmark datasets. Compared to the baseline models with traditional gradient descent algorithm, models with SA-GD algorithm possess better generalization ability without sacrificing the efficiency and stability of model convergence. In addition, SAGD can be utilized as an effective ensemble learning approach which improves the final performance significantly.", "venue": "ArXiv", "authors": ["Zhicheng  Cai"], "year": 2021, "n_citations": 0}
{"id": 52943, "s2_id": "5f82206bc41cd0c3626c97602b37312a185b992b", "title": "Risks of Using Non-verified Open Data: A case study on using Machine Learning techniques for predicting Pregnancy Outcomes in India", "abstract": "Artificial intelligence (AI) has evolved considerably in the last few years. While applications of AI is now becoming more common in fields like retail and marketing, application of AI in solving problems related to developing countries is still an emerging topic. Specially, AI applications in resource-poor settings remains relatively nascent. There is a huge scope of AI being used in such settings. For example, researchers have started exploring AI applications to reduce poverty and deliver a broad range of critical public services. However, despite many promising use cases, there are many dataset related challenges that one has to overcome in such projects. These challenges often take the form of missing data, incorrectly collected data and improperly labeled variables, among other factors. As a result, we can often end up using data that is not representative of the problem we are trying to solve. In this case study, we explore the challenges of using such an open dataset from India, to predict an important health outcome. We highlight how the use of AI without proper understanding of reporting metrics can lead to erroneous conclusions.", "venue": "ArXiv", "authors": ["Anusua  Trivedi", "Sumit  Mukherjee", "Edmund  Tse", "Anne  Ewing", "Juan Lavista Ferres"], "year": 2019, "n_citations": 3}
{"id": 59668, "s2_id": "91d523be4d85f6d700148b57c896eac3212fa8d8", "title": "The Devils in the Point Clouds: Studying the Robustness of Point Cloud Convolutions", "abstract": "Recently, there has been a significant interest in performing convolution over irregularly sampled point clouds. Since point clouds are very different from regular raster images, it is imperative to study the generalization of the convolution networks more closely, especially their robustness under variations in scale and rotations of the input data. This paper investigates different variants of PointConv, a convolution network on point clouds, to examine their robustness to input scale and rotation changes. Of the variants we explored, two are novel and generated significant improvements. The first is replacing the multilayer perceptron based weight function with much simpler third degree polynomials, together with a Sobolev norm regularization. Secondly, for 3D datasets, we derive a novel viewpoint-invariant descriptor by utilizing 3D geometric properties as the input to PointConv, in addition to the regular 3D coordinates. We have also explored choices of activation functions, neighborhood, and subsampling methods. Experiments are conducted on the 2D MNIST & CIFAR-10 datasets as well as the 3D SemanticKITTI & ScanNet datasets. Results reveal that on 2D, using third degree polynomials greatly improves PointConv\u2019s robustness to scale changes and rotations, even surpassing traditional 2D CNNs for the MNIST dataset. On 3D datasets, the novel viewpoint-invariant descriptor significantly improves the performance as well as robustness of PointConv. We achieve the state-of-the-art semantic segmentation performance on the SemanticKITTI dataset, as well as comparable performance with the current highest framework on the ScanNet dataset among point-based approaches.", "venue": "ArXiv", "authors": ["Xingyi  Li", "Wenxuan  Wu", "Xiaoli Z. Fern", "Li  Fuxin"], "year": 2021, "n_citations": 0}
{"id": 71691, "s2_id": "4f54b792842c310ae047d45439c4aea5514c8226", "title": "Implicit Regularization in Over-parameterized Neural Networks", "abstract": "Over-parameterized neural networks generalize well in practice without any explicit regularization. Although it has not been proven yet, empirical evidence suggests that implicit regularization plays a crucial role in deep learning and prevents the network from overfitting. In this work, we introduce the gradient gap deviation and the gradient deflection as statistical measures corresponding to the network curvature and the Hessian matrix to analyze variations of network derivatives with respect to input parameters, and investigate how implicit regularization works in ReLU neural networks from both theoretical and empirical perspectives. Our result reveals that the network output between each pair of input samples is properly controlled by random initialization and stochastic gradient descent to keep interpolating between samples almost straight, which results in low complexity of over-parameterized neural networks.", "venue": "ArXiv", "authors": ["Masayoshi  Kubo", "Ryotaro  Banno", "Hidetaka  Manabe", "Masataka  Minoji"], "year": 2019, "n_citations": 11}
{"id": 81584, "s2_id": "b905f50d997d02072ff801ccec53edbf6e2850d1", "title": "ArSentD-LEV: A Multi-Topic Corpus for Target-based Sentiment Analysis in Arabic Levantine Tweets", "abstract": "Sentiment analysis is a highly subjective and challenging task. Its complexity further increases when applied to the Arabic language, mainly because of the large variety of dialects that are unstandardized and widely used in the Web, especially in social media. While many datasets have been released to train sentiment classifiers in Arabic, most of these datasets contain shallow annotation, only marking the sentiment of the text unit, as a word, a sentence or a document. In this paper, we present the Arabic Sentiment Twitter Dataset for the Levantine dialect (ArSenTD-LEV). Based on findings from analyzing tweets from the Levant region, we created a dataset of 4,000 tweets with the following annotations: the overall sentiment of the tweet, the target to which the sentiment was expressed, how the sentiment was expressed, and the topic of the tweet. Results confirm the importance of these annotations at improving the performance of a baseline sentiment classifier. They also confirm the gap of training in a certain domain, and testing in another domain.", "venue": "ArXiv", "authors": ["Ramy  Baly", "Alaa  Khaddaj", "Hazem M. Hajj", "Wassim  El-Hajj", "Khaled Bashir Shaban"], "year": 2019, "n_citations": 35}
{"id": 125264, "s2_id": "844f549adcf158883e06cd04a70c48cba18c8584", "title": "Mitigating Information Leakage in Image Representations: A Maximum Entropy Approach", "abstract": "Image recognition systems have demonstrated tremendous progress over the past few decades thanks, in part, to our ability of learning compact and robust representations of images. As we witness the wide spread adoption of these systems, it is imperative to consider the problem of unintended leakage of information from an image representation, which might compromise the privacy of the data owner. This paper investigates the problem of learning an image representation that minimizes such leakage of user information. We formulate the problem as an adversarial non-zero sum game of finding a good embedding function with two competing goals: to retain as much task dependent discriminative image information as possible, while simultaneously minimizing the amount of information, as measured by entropy, about other sensitive attributes of the user. We analyze the stability and convergence dynamics of the proposed formulation using tools from non-linear systems theory and compare to that of the corresponding adversarial zero-sum game formulation that optimizes likelihood as a measure of information content. Numerical experiments on UCI, Extended Yale B, CIFAR-10 and CIFAR-100 datasets indicate that our proposed approach is able to learn image representations that exhibit high task performance while mitigating leakage of predefined sensitive information.", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "authors": ["Proteek Chandan Roy", "Vishnu Naresh Boddeti"], "year": 2019, "n_citations": 41}
{"id": 130690, "s2_id": "4f2926f255bd738a34ea88dfbf021feed9e61f9b", "title": "Deep Learning of Cell Classification Using Microscope Images of Intracellular Microtubule Networks", "abstract": "Microtubule networks (MTs) are a component of a cell that may indicate the presence of various chemical compounds and can be used to recognize properties such as treatment resistance. Therefore, the classification of MT images is of great relevance for cell diagnostics. Human experts find it particularly difficult to recognize the levels of chemical compound exposure of a cell. Improving the accuracy with automated techniques would have a significant impact on cell therapy. In this paper we present the application of Deep Learning to MT image classification and evaluate it on a large MT image dataset of animal cells with three degrees of exposure to a chemical agent. The results demonstrate that the learned deep network performs on par or better at the corresponding cell classification task than human experts. Specifically, we show that the task of recognizing different levels of chemical agent exposure can be handled significantly better by the neural network than by human experts.", "venue": "2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)", "authors": ["Aleksei  Shpilman", "Dmitry  Boikiy", "Marina  Polyakova", "Daniel  Kudenko", "Anton  Burakov", "Elena  Nadezhdina"], "year": 2017, "n_citations": 6}
{"id": 137877, "s2_id": "ef3152106e7f4d05ad8d32a5b90d3790c5cdef24", "title": "Recurrent Orthogonal Networks and Long-Memory Tasks", "abstract": "Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.", "venue": "ICML", "authors": ["Mikael  Henaff", "Arthur  Szlam", "Yann  LeCun"], "year": 2016, "n_citations": 99}
{"id": 155691, "s2_id": "3a055ac7a0597f333df15e30eebcc24b3b12c958", "title": "Pareto-Optimal Bit Allocation for Collaborative Intelligence", "abstract": "In recent studies, collaborative intelligence (CI) has emerged as a promising framework for deployment of Artificial Intelligence (AI)-based services on mobile/edge devices. In CI, the AI model (a deep neural network) is split between the edge and the cloud, and intermediate features are sent from the edge sub-model to the cloud sub-model. In this article, we study bit allocation for feature coding in multi-stream CI systems. We model task distortion as a function of rate using convex surfaces similar to those found in distortion-rate theory. Using such models, we are able to provide closed-form bit allocation solutions for single-task systems and scalarized multi-task systems. Moreover, we provide analytical characterization of the full Pareto set for 2-stream $k$ -task systems, and bounds on the Pareto set for 3-stream 2-task systems. Analytical results are examined on a variety of DNN models from the literature to demonstrate wide applicability of the results.", "venue": "IEEE Transactions on Image Processing", "authors": ["Saeed Ranjbar Alvar", "Ivan V. Baji\u0107"], "year": 2021, "n_citations": 5}
{"id": 158579, "s2_id": "d10541cf722edcfd1eb0243ca2ca72e4dad0440c", "title": "Bayes-Newton Methods for Approximate Bayesian Inference with PSD Guarantees", "abstract": "We formulate natural gradient variational inference (VI), expectation propagation (EP), and posterior linearisation (PL) as extensions of Newton\u2019s method for optimising the parameters of a Bayesian posterior distribution. This viewpoint explicitly casts inference algorithms under the framework of numerical optimisation. We show that common approximations to Newton\u2019s method from the optimisation literature, namely Gauss\u2013Newton and quasi-Newton methods (e.g., the BFGS algorithm), are still valid under this \u2018Bayes\u2013 Newton\u2019 framework. This leads to a suite of novel algorithms which are guaranteed to result in positive semi-definite covariance matrices, unlike standard VI and EP. Our unifying viewpoint provides new insights into the connections between various inference schemes. All the presented methods apply to any model with a Gaussian prior and non-conjugate likelihood, which we demonstrate with (sparse) Gaussian processes and state space models.", "venue": "ArXiv", "authors": ["William J. Wilkinson", "Simo  Sarkka", "Arno  Solin"], "year": 2021, "n_citations": 0}
{"id": 169699, "s2_id": "046b664d0dfcdca74a11fd963ce79f51f69d15b2", "title": "3D Local Features for Direct Pairwise Registration", "abstract": "We present a novel, data driven approach for solving the problem of registration of two point cloud scans. Our approach is direct in the sense that a single pair of corresponding local patches already provides the necessary transformation cue for the global registration. To achieve that, we first endow the state of the art PPF-FoldNet auto-encoder (AE) with a pose-variant sibling, where the discrepancy between the two leads to pose-specific descriptors. Based upon this, we introduce RelativeNet, a relative pose estimation network to assign correspondence-specific orientations to the keypoints, eliminating any local reference frame computations. Finally, we devise a simple yet effective hypothesize-and-verify algorithm to quickly use the predictions and align two point sets. Our extensive quantitative and qualitative experiments suggests that our approach outperforms the state of the art in challenging real datasets of pairwise registration and that augmenting the keypoints with local pose information leads to better generalization and a dramatic speed-up.", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "authors": ["Haowen  Deng", "Tolga  Birdal", "Slobodan  Ilic"], "year": 2019, "n_citations": 55}
{"id": 183739, "s2_id": "889066fadc1015eb0d9246ef2a1e8b2820cb3d19", "title": "ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation", "abstract": "Current pre-training works in natural language generation pay little attention to the problem of exposure bias on downstream tasks. To address this issue, we propose an enhanced multi-flow sequence to sequence pre-training and fine-tuning framework named ERNIE-GEN, which bridges the discrepancy between training and inference with an infilling generation mechanism and a noise-aware generation method. To make generation closer to human writing patterns, this framework introduces a span-by-span generation flow that trains the model to predict semantically-complete spans consecutively rather than predicting word by word. Unlike existing pre-training methods, ERNIE-GEN incorporates multi-granularity target sampling to construct pre-training data, which enhances the correlation between encoder and decoder. Experimental results demonstrate that ERNIE-GEN achieves state-of-the-art results with a much smaller amount of pre-training data and parameters on a range of language generation tasks, including abstractive summarization (Gigaword and CNN/DailyMail), question generation (SQuAD), dialogue generation (Persona-Chat) and generative question answering (CoQA).", "venue": "ArXiv", "authors": ["Dongling  Xiao", "Han  Zhang", "Yukun  Li", "Yu  Sun", "Hao  Tian", "Hua  Wu", "Haifeng  Wang"], "year": 2020, "n_citations": 28}
{"id": 191263, "s2_id": "d87fa82f36dc87fa2deb68d8296d7b3d40705568", "title": "Adaptive Policies for Perimeter Surveillance Problems", "abstract": "Maximising the detection of intrusions is a fundamental and often critical aim of perimeter surveillance. Commonly, this requires a decision-maker to optimally allocate multiple searchers to segments of the perimeter. We consider a scenario where the decision-maker may sequentially update the searchers' allocation, learning from the observed data to improve decisions over time. In this work we propose a formal model and solution methods for this sequential perimeter surveillance problem. Our model is a combinatorial multi-armed bandit (CMAB) with Poisson rewards and a novel filtered feedback mechanism - arising from the failure to detect certain intrusions. Our solution method is an upper confidence bound approach and we derive upper and lower bounds on its expected performance. We prove that the gap between these bounds is of constant order, and demonstrate empirically that our approach is more reliable in simulated problems than competing algorithms.", "venue": "Eur. J. Oper. Res.", "authors": ["James A. Grant", "David S. Leslie", "Kevin  Glazebrook", "Roberto  Szechtman", "Adam N. Letchford"], "year": 2020, "n_citations": 9}
{"id": 216783, "s2_id": "a2f8b06a0f85dd62f1abfc306c8ff23037e49866", "title": "Structured Sparsification of Gated Recurrent Neural Networks", "abstract": "Recently, a lot of techniques were developed to sparsify the weights of neural networks and to remove networks' structure units, e.g. neurons. We adjust the existing sparsification approaches to the gated recurrent architectures. Specifically, in addition to the sparsification of weights and neurons, we propose sparsifying the preactivations of gates. This makes some gates constant and simplifies LSTM structure. We test our approach on the text classification and language modeling tasks. We observe that the resulting structure of gate sparsity depends on the task and connect the learned structure to the specifics of the particular tasks. Our method also improves neuron-wise compression of the model in most of the tasks.", "venue": "AAAI", "authors": ["Ekaterina  Lobacheva", "Nadezhda  Chirkova", "Alexander  Markovich", "Dmitry  Vetrov"], "year": 2020, "n_citations": 2}
{"id": 220737, "s2_id": "a924535d54bd3417211008e65ead568daba5d525", "title": "Editing in Style: Uncovering the Local Semantics of GANs", "abstract": "While the quality of GAN image synthesis has improved tremendously in recent years, our ability to control and condition the output is still limited. Focusing on StyleGAN, we introduce a simple and effective method for making local, semantically-aware edits to a target output image. This is accomplished by borrowing elements from a source image, also a GAN output, via a novel manipulation of style vectors. Our method requires neither supervision from an external model, nor involves complex spatial morphing operations. Instead, it relies on the emergent disentanglement of semantic objects that is learned by StyleGAN during its training. Semantic editing is demonstrated on GANs producing human faces, indoor scenes, cats, and cars. We measure the locality and photorealism of the edits produced by our method, and find that it accomplishes both.", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "authors": ["Edo  Collins", "Raja  Bala", "Bob  Price", "Sabine  S\u00fcsstrunk"], "year": 2020, "n_citations": 76}
{"id": 228060, "s2_id": "f5e63fb54e1e338c246b20aacf1b3d629b8baf3d", "title": "A Neural Autoregressive Approach to Collaborative Filtering", "abstract": "This paper proposes CF-NADE, a neural autoregressive architecture for collaborative filtering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing parameters between different ratings. A factored version of CF-NADE is also proposed for better scalability. Furthermore, we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize CF-NADE, which shows superior performance. Finally, CF-NADE can be extended to a deep model, with only moderately increased computational complexity. Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance.", "venue": "ICML", "authors": ["Yin  Zheng", "Bangsheng  Tang", "Wenkui  Ding", "Hanning  Zhou"], "year": 2016, "n_citations": 161}
{"id": 241346, "s2_id": "5f48badf9b0f72fa509d3126438178b5f9ef1845", "title": "Metric learning for generalizing spatial relations to new objects", "abstract": "Human-centered environments are rich with a wide variety of spatial relations between everyday objects. For autonomous robots to operate effectively in such environments, they should be able to reason about these relations and generalize them to objects with different shapes and sizes. For example, having learned to place a toy inside a basket, a robot should be able to generalize this concept using a spoon and a cup. This requires a robot to have the flexibility to learn arbitrary relations in a lifelong manner, making it challenging for an expert to pre-program it with sufficient knowledge to do so beforehand. In this paper, we address the problem of learning spatial relations by introducing a novel method from the perspective of distance metric learning. Our approach enables a robot to reason about the similarity between pairwise spatial relations, thereby enabling it to use its previous knowledge when presented with a new relation to imitate. We show how this makes it possible to learn arbitrary spatial relations from non-expert users using a small number of examples and in an interactive manner. Our extensive evaluation with real-world data demonstrates the effectiveness of our method in reasoning about a continuous spectrum of spatial relations and generalizing them to new objects.", "venue": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "authors": ["Oier  Mees", "Nichola  Abdo", "Mladen  Mazuran", "Wolfram  Burgard"], "year": 2017, "n_citations": 17}
{"id": 276819, "s2_id": "9fddba1b003f2bc9cb42c0654ae1c92faa3f955f", "title": "Local Advantage Actor-Critic for Robust Multi-Agent Deep Reinforcement Learning", "abstract": "Policy gradient methods have become popular in multi-agent reinforcement learning, but they suffer from high variance due to the presence of environmental stochasticity and exploring agents (i.e., non-stationarity), which is potentially worsened by the difficulty in credit assignment. As a result, there is a need for a method that is not only capable of efficiently solving the above two problems but also robust enough to solve a variety of tasks. To this end, we propose a new multi-agent policy gradient method, called Robust Local Advantage (ROLA) Actor-Critic. ROLA allows each agent to learn an individual action-value function as a local critic as well as ameliorating environment non-stationarity via a novel centralized training approach based on a centralized critic. By using this local critic, each agent calculates a baseline to reduce variance on its policy gradient estimation, which results in an expected advantage action-value over other agents' choices that implicitly improves credit assignment. We evaluate ROLA across diverse benchmarks and show its robustness and effectiveness over a number of state-of-the-art multi-agent policy gradient algorithms.", "venue": "2021 International Symposium on Multi-Robot and Multi-Agent Systems (MRS)", "authors": ["Yuchen  Xiao", "Xueguang  Lyu", "Christopher  Amato"], "year": 2021, "n_citations": 0}
{"id": 285277, "s2_id": "85a109790efaf1821347f12af7565e5158587929", "title": "Severity detection tool for patients with infectious disease", "abstract": "Hand foot and mouth disease (HFMD) and tetanus are serious infectious diseases in low- and middle-income countries. Tetanus, in particular, has a high mortality rate and its treatment is resource-demanding. Furthermore, HFMD often affects a large number of infants and young children. As a result, its treatment consumes enormous healthcare resources, especially when outbreaks occur. Autonomic nervous system dysfunction (ANSD) is the main cause of death for both HFMD and tetanus patients. However, early detection of ANSD is a difficult and challenging problem. The authors aim to provide a proof-of-principle to detect the ANSD level automatically by applying machine learning techniques to physiological patient data, such as electrocardiogram waveforms, which can be collected using low-cost wearable sensors. Efficient features are extracted that encode variations in the waveforms in the time and frequency domains. The proposed approach is validated on multiple datasets of HFMD and tetanus patients in Vietnam. Results show that encouraging performance is achieved. Moreover, the proposed features are simple, more generalisable and outperformed the standard heart rate variability analysis. The proposed approach would facilitate both the diagnosis and treatment of infectious diseases in low- and middle-income countries, and thereby improve patient care.", "venue": "Healthcare technology letters", "authors": ["Girmaw Abebe Tadesse", "Tingting  Zhu", "Nhan Le Nguyen Thanh", "Nguyen Thanh Hung", "Ha Thi Hai Duong", "Truong Huu Khanh", "Pham Van Quang", "Duc Duong Tran", "LamMinh  Yen", "H. Rogier Van Doorn", "Nguyen Van Hao", "John  Prince", "Hamza  Javed", "Dani  Kiyasseh", "Le Van Tan", "Louise  Thwaites", "David A. Clifton"], "year": 2020, "n_citations": 2}
{"id": 310397, "s2_id": "40fa45e3bcbfd170891ba928bf4fab1b53cddbf4", "title": "Fast Rates for Contextual Linear Optimization", "abstract": "Incorporating side observations of predictive features can help reduce uncertainty in operational decision making, but it also requires we tackle a potentially complex predictive relationship. Although one may use a variety of off-the-shelf machine learning methods to learn a predictive model and then plug it into our decision-making problem, a variety of recent work has instead advocated integrating estimation and optimization by taking into consideration downstream decision performance. Surprisingly, in the case of contextual linear optimization, we show that the naive plug-in approach actually achieves regret convergence rates that are significantly faster than the best-possible by methods that directly optimize down-stream decision performance. We show this by leveraging the fact that specific problem instances do not have arbitrarily bad near-degeneracy. While there are other pros and cons to consider as we discuss, our results highlight a very nuanced landscape for the enterprise to integrate estimation and optimization.", "venue": "ArXiv", "authors": ["Yichun  Hu", "Nathan  Kallus", "Xiaojie  Mao"], "year": 2020, "n_citations": 7}
{"id": 326167, "s2_id": "ba660056737f3aa01b7ebc053b6ffe7d6be7202b", "title": "Game Redesign in No-regret Game Playing", "abstract": "We study the game redesign problem in which an external designer has the ability to change the payoff function in each round, but incurs a design cost for deviating from the original game. The players apply no-regret learning algorithms to repeatedly play the changed games with limited feedback. The goals of the designer are to (i) incentivize all players to take a specific target action profile frequently; and (ii) incur small cumulative design cost. We present game redesign algorithms with the guarantee that the target action profile is played inT \u2212o (T ) rounds while incurring only o (T ) cumulative design cost. Game redesign describes both positive and negative applications: a benevolent designer who incentivizes players to take a target action profile with better social welfare compared to the solution of the original game, or a malicious attacker whose target action profile benefits themselves but not the players. Simulations on four classic games confirm the effectiveness of our proposed redesign algorithms.", "venue": "ArXiv", "authors": ["Yuzhe  Ma", "Young  Wu", "Xiaojin  Zhu"], "year": 2021, "n_citations": 0}
{"id": 333083, "s2_id": "bb41162e1283783057ea04cea17f20bbfcd3f9a6", "title": "Introduction to Normalizing Flows for Lattice Field Theory", "abstract": "Michael S. Albergo, \u2217 Denis Boyda, 3, 4, \u2020 Daniel C. Hackett, 4, \u2021 Gurtej Kanwar, 4, \u00a7 Kyle Cranmer, S\u00e9bastien Racani\u00e8re, Danilo Jimenez Rezende, and Phiala E. Shanahan 4 Center for Cosmology and Particle Physics, New York University, New York, NY 10003, USA Argonne Leadership Computing Facility, Argonne National Laboratory, Lemont IL 60439, USA Center for Theoretical Physics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA The NSF AI Institute for Artificial Intelligence and Fundamental Interactions DeepMind, London, UK (Dated: August 9, 2021)", "venue": "ArXiv", "authors": ["Michael S. Albergo", "Denis  Boyda", "Daniel C. Hackett", "Gurtej  Kanwar", "Kyle  Cranmer", "S'ebastien  Racaniere", "Danilo Jimenez Rezende", "Phiala E. Shanahan"], "year": 2021, "n_citations": 10}
{"id": 343407, "s2_id": "e6cffb82ef59729d0474c613feb23f863113a8bd", "title": "Answer Span Correction in Machine Reading Comprehension", "abstract": "Answer validation in machine reading comprehension (MRC) consists of verifying an extracted answer against an input context and question pair. Previous work has looked at re-assessing the \u201canswerability\u201d of the question given the extracted answer. Here we address a different problem: the tendency of existing MRC systems to produce partially correct answers when presented with answerable questions. We explore the nature of such errors and propose a post-processing correction method that yields statistically significant performance improvements over state-of-the-art MRC systems in both monolingual and multilingual evaluation.", "venue": "FINDINGS", "authors": ["Revanth Gangi Reddy", "Md Arafat Sultan", "Efsun Sarioglu Kayi", "Rong  Zhang", "Vittorio  Castelli", "Avirup  Sil"], "year": 2020, "n_citations": 2}
{"id": 362281, "s2_id": "6a66cf7dd6d1f70e99018518e765917fb76491ad", "title": "Variable-rate discrete representation learning", "abstract": "Semantically meaningful information content in perceptual signals is usually unevenly distributed. In speech signals for example, there are often many silences, and the speed of pronunciation can vary considerably. In this work, we propose slow autoencoders (SlowAEs) for unsupervised learning of high-level variable-rate discrete representations of sequences, and apply them to speech. We show that the resulting event-based representations automatically grow or shrink depending on the density of salient information in the input signals, while still allowing for faithful signal reconstruction. We develop run-length Transformers (RLTs) for event-based representation modelling and use them to construct language models in the speech domain, which are able to generate grammatical and semantically coherent utterances and continuations.", "venue": "ArXiv", "authors": ["Sander  Dieleman", "Charlie  Nash", "Jesse  Engel", "Karen  Simonyan"], "year": 2021, "n_citations": 3}
{"id": 375186, "s2_id": "d106b7c4fa08d62ba979b741dbf12e9f7a9e240a", "title": "Image Retrieval based on Bag-of-Words model", "abstract": "This article gives a survey for bag-of-words (BoW) or bag-of-features model in image retrieval system. In recent years, large-scale image retrieval shows significant potential in both industry applications and research problems. As local descriptors like SIFT demonstrate great discriminative power in solving vision problems like object recognition, image classification and annotation, more and more state-of-the-art large scale image retrieval systems are trying to rely on them. A common way to achieve this is first quantizing local descriptors into visual words, and then applying scalable textual indexing and retrieval schemes. We call this model as bag-of-words or bag-of-features model. The goal of this survey is to give an overview of this model and introduce different strategies when building the system based on this model.", "venue": "ArXiv", "authors": ["Jialu  Liu"], "year": 2013, "n_citations": 48}
{"id": 400916, "s2_id": "f5e65be030d0a158264fa3e68fc3d531632ff8d5", "title": "When coding meets ranking: A joint framework based on local learning", "abstract": "Sparse coding, which represents a data point as a sparse reconstruction code with regard to a dictionary, has been a popular data representation method. Meanwhile, in database retrieval problems, learning the ranking scores from data points plays an important role. Up to now, these two problems have always been considered separately, assuming that data coding and ranking are two independent and irrelevant problems. However, is there any internal relationship between sparse coding and ranking score learning? If yes, how to explore and make use of this internal relationship? In this paper, we try to answer these questions by developing the first joint sparse coding and ranking score learning algorithm. To explore the local distribution in the sparse code space, and also to bridge coding and ranking problems, we assume that in the neighborhood of each data point, the ranking scores can be approximated from the corresponding sparse codes by a local linear function. By considering the local approximation error of ranking scores, the reconstruction error and sparsity of sparse coding, and the query information provided by the user, we construct a unified objective function for learning of sparse codes, the dictionary and ranking scores. We further develop an iterative algorithm to solve this optimization problem.", "venue": "ArXiv", "authors": ["Jim Jing-Yan Wang"], "year": 2014, "n_citations": 0}
{"id": 428866, "s2_id": "a659f92df947a96a86bb85cb3876231ef4be9737", "title": "A Combination of Temporal Sequence Learning and Data Description for Anomaly-based NIDS", "abstract": "Through continuous observation and modelling of normal behavior in networks, Anomaly-based Network Intrusion Detection System (A-NIDS) offers a way to find possible threats via deviation from the normal model. The analysis of network traffic based on time series model has the advantage of exploiting the relationship between packages within network traffic and observing trends of behaviors over a period of time. It will generate new sequences with good features that support anomaly detection in network traffic and provide the ability to detect new attacks. Besides, an anomaly detection technique, which focuses on the normal data and aims to build a description of it, will be an effective technique for anomaly detection in imbalanced data. In this paper, we propose a combination model of Long Short Term Memory (LSTM) architecture for processing time series and a data description Support Vector Data Description (SVDD) for anomaly detection in A-NIDS to obtain the advantages of them. This model helps parameters in LSTM and SVDD are jointly trained with joint optimization method. Our experimental results with KDD99 dataset show that the proposed combined model obtains high performance in intrusion detection, especially DoS and Probe attacks with 98.0% and 99.8%, respectively.", "venue": "International Journal of Network Security & Its Applications", "authors": ["Nguyen Thanh Van", "Tran Ngoc Thinh", "Le Thanh Sach"], "year": 2019, "n_citations": 0}
{"id": 461308, "s2_id": "01c48e415509cfe8d29cc844ba5887c0324245c0", "title": "Deep Convolutional Neural Networks for Map-Type Classification", "abstract": "Maps are an important medium that enable people to comprehensively understand the configuration of cultural activities and natural elements over different times and places. Although massive maps are available in the digital era, how to effectively and accurately access the required map remains a challenge today. Previous works partially related to map-type classification mainly focused on map comparison and map matching at the local scale. The features derived from local map areas might be insufficient to characterize map content. To facilitate establishing an automatic approach for accessing the needed map, this paper reports our investigation into using deep learning techniques to recognize seven types of map, including topographic map, terrain map, physical map, urban scene map, the National Map, 3D map, nighttime map, orthophoto map, and land cover classification map. Experimental results show that the state-of-the-art deep convolutional neural networks can support automatic map-type classification. Additionally, the classification accuracy varies according to different map-types. We hope our work can contribute to the implementation of deep learning techniques in cartographical community and advance the progress of Geographical Artificial Intelligence (GeoAI).", "venue": "ArXiv", "authors": ["Xiran  Zhou", "Wenwen  Li", "Samantha T. Arundel", "Jun  Liu"], "year": 2018, "n_citations": 9}
{"id": 471256, "s2_id": "1673ba124ae9bef97f1f99ea04f7b2ca356b525b", "title": "Embedding Text in Hyperbolic Spaces", "abstract": "Natural language text exhibits hierarchical structure in a variety of respects. Ideally, we could incorporate our prior knowledge of this hierarchical structure into unsupervised learning algorithms that work on text data. Recent work by Nickel and Kiela (2017) proposed using hyperbolic instead of Euclidean embedding spaces to represent hierarchical data and demonstrated encouraging results when embedding graphs. In this work, we extend their method with a re-parameterization technique that allows us to learn hyperbolic embeddings of arbitrarily parameterized objects. We apply this framework to learn word and sentence embeddings in hyperbolic space in an unsupervised manner from text corpora. The resulting embeddings seem to encode certain intuitive notions of hierarchy, such as word-context frequency and phrase constituency. However, the implicit continuous hierarchy in the learned hyperbolic space makes interrogating the model\u2019s learned hierarchies more difficult than for models that learn explicit edges between items. The learned hyperbolic embeddings show improvements over Euclidean embeddings in some \u2013 but not all \u2013 downstream tasks, suggesting that hierarchical organization is more useful for some tasks than others.", "venue": "TextGraphs@NAACL-HLT", "authors": ["Bhuwan  Dhingra", "Christopher J. Shallue", "Mohammad  Norouzi", "Andrew M. Dai", "George E. Dahl"], "year": 2018, "n_citations": 76}
{"id": 473202, "s2_id": "1b272e2b32c1be06a9e2a472a526771e2d9170de", "title": "Skopus: Mining top-k sequential patterns under leverage", "abstract": "This paper presents a framework for exact discovery of the top-k sequential patterns under Leverage. It combines (1) a novel definition of the expected support for a sequential pattern\u2014a concept on which most interestingness measures directly rely\u2014with (2) Skopus: a new branch-and-bound algorithm for the exact discovery of top-k sequential patterns under a given measure of interest. Our interestingness measure employs the partition approach. A pattern is interesting to the extent that it is more frequent than can be explained by assuming independence between any of the pairs of patterns from which it can be composed. The larger the support compared to the expectation under independence, the more interesting is the pattern. We build on these two elements to exactly extract the k sequential patterns with highest leverage, consistent with our definition of expected support. We conduct experiments on both synthetic data with known patterns and real-world datasets; both experiments confirm the consistency and relevance of our approach with regard to the state of the art.", "venue": "Data Mining and Knowledge Discovery", "authors": ["Fran\u00e7ois  Petitjean", "Tao  Li", "Nikolaj  Tatti", "Geoffrey I. Webb"], "year": 2016, "n_citations": 18}
{"id": 486502, "s2_id": "fbca90d16a5a751582c1764c2db6f4deb4639f92", "title": "Solving Optical Tomography with Deep Learning", "abstract": "This paper presents a neural network approach for solving two-dimensional optical tomography (OT) problems based on the radiative transfer equation. The mathematical problem of OT is to recover the optical properties of an object based on the albedo operator that is accessible from boundary measurements. Both the forward map from the optical properties to the albedo operator and the inverse map are high-dimensional and nonlinear. For the circular tomography geometry, a perturbative analysis shows that the forward map can be approximated by a vectorized convolution operator in the angular direction. Motivated by this, we propose effective neural network architectures for the forward and inverse maps based on convolution layers, with weights learned from training datasets. Numerical results demonstrate the efficiency of the proposed neural networks.", "venue": "ArXiv", "authors": ["Yuwei  Fan", "Lexing  Ying"], "year": 2019, "n_citations": 10}
{"id": 533419, "s2_id": "9f5534e19c4ff6e2d2fb61dccced57a0b85cb5da", "title": "Discovering Latent States for Model Learning: Applying Sensorimotor Contingencies Theory and Predictive Processing to Model Context", "abstract": "Autonomous robots need to be able to adapt to unforeseen situations and to acquire new skills through trial and error. Reinforcement learning in principle offers a suitable methodological framework for this kind of autonomous learning. However current computational reinforcement learning agents mostly learn each individual skill entirely from scratch. How can we enable artificial agents, such as robots, to acquire some form of generic knowledge, which they could leverage for the learning of new skills? This paper argues that, like the brain, the cognitive system of artificial agents has to develop a world model to support adaptive behavior and learning. Inspiration is taken from two recent developments in the cognitive science literature: predictive processing theories of cognition, and the sensorimotor contingencies theory of perception. Based on these, a hypothesis is formulated about what the content of information might be that is encoded in an internal world model, and how an agent could autonomously acquire it. A computational model is described to formalize this hypothesis, and is evaluated in a series of simulation experiments.", "venue": "ArXiv", "authors": ["Nikolas J. Hemion"], "year": 2016, "n_citations": 4}
{"id": 537371, "s2_id": "b151f15f1ad12cb87263061c6bb267939d083458", "title": "Biologically Inspired Hexagonal Deep Learning For Hexagonal Image Generation", "abstract": "Whereas conventional state-of-the-art image processing systems of recording and output devices almost exclusively utilize square arranged methods, biological models, however, suggest an alternative, evolutionarily-based structure. Inspired by the human visual perception system, hexagonal image processing in the context of machine learning offers a number of key advantages that can benefit both researchers and users alike. The hexagonal deep learning framework Hexnet leveraged in this contribution serves therefore the generation of hexagonal images by utilizing hexagonal deep neural networks (H-DNN). As the results of our created test environment show, the proposed models can surpass current approaches of conventional image generation. While resulting in a reduction of the models\u2019 complexity in the form of trainable parameters, they furthermore allow an increase of test rates in comparison to their square counterparts.", "venue": "2020 IEEE International Conference on Image Processing (ICIP)", "authors": ["Tobias  Schlosser", "Frederik  Beuth", "Danny  Kowerko"], "year": 2020, "n_citations": 1}
{"id": 540691, "s2_id": "f35f56b7048a74244ab0530e4bc705cc71125862", "title": "Bayesian Approaches to Distribution Regression", "abstract": "Distribution regression has recently attracted much interest as a generic solution to the problem of supervised learning where labels are available at the group level, rather than at the individual level. Current approaches, however, do not propagate the uncertainty in observations due to sampling variability in the groups. This effectively assumes that small and large groups are estimated equally well, and should have equal weight in the final regression. We account for this uncertainty with a Bayesian distribution regression formalism, improving the robustness and performance of the model when group sizes vary. We frame our models in a neural network style, allowing for simple MAP inference using backpropagation to learn the parameters, as well as MCMC-based inference which can fully propagate uncertainty. We demonstrate our approach on illustrative toy datasets, as well as on a challenging problem of predicting age from images.", "venue": "AISTATS", "authors": ["Ho Chung Leon Law", "Dougal J. Sutherland", "Dino  Sejdinovic", "Seth  Flaxman"], "year": 2018, "n_citations": 26}
{"id": 544977, "s2_id": "d23dcb9f1ed4d1e49e0914da2566f1557676cce9", "title": "Knowledge Elicitation using Deep Metric Learning and Psychometric Testing", "abstract": "Knowledge present in a domain is well expressed as relationships between corresponding concepts. For example, in zoology, animal species form complex hierarchies; in genomics, the different (parts of) molecules are organized in groups and subgroups based on their functions; plants, molecules, and astronomical objects all form complex taxonomies. Nevertheless, when applying supervised machine learning (ML) in such domains, we commonly reduce the complex and rich knowledge to a fixed set of labels, and induce a model shows good generalization performance with respect to these labels. The main reason for such a reductionist approach is the difficulty in eliciting the domain knowledge from the experts. Developing a label structure with sufficient fidelity and providing comprehensive multi-label annotation can be exceedingly labor-intensive in many real-world applications. In this paper, we provide a method for efficient hierarchical knowledge elicitation (HKE) from experts working with high-dimensional data such as images or videos. Our method is based on psychometric testing and active deep metric learning. The developed models embed the high-dimensional data in a metric space where distances are semantically meaningful, and the data can be organized in a hierarchical structure. We provide empirical evidence with a series of experiments on a synthetically generated dataset of simple shapes, and Cifar 10 and Fashion-MNIST benchmarks that our method is indeed successful in uncovering hierarchical structures.", "venue": "IJCAI", "authors": ["Lu  Yin", "Vlado  Menkovski", "Mykola  Pechenizkiy"], "year": 2020, "n_citations": 2}
{"id": 554756, "s2_id": "f25103d7dcbf5095a43ffdc7a1f89ef21ac5a980", "title": "Zero-shot Task Transfer for Invoice Extraction via Class-aware QA Ensemble", "abstract": "We present VESPA, an intentionally simple yet novel zero-shot system for a layout, locale and domain agnostic document extraction. In spite of the availability of large corpora of documents, the lack of labeled and validated datasets makes it a challenge to discriminatively train document extraction models for enterprises. We show that this problem can be addressed by simply transferring the information extraction (IE) task to a natural language QuestionAnswering (QA) task without engineering task-specific architectures. We demonstrate the effectiveness of our system by evaluating on a closed corpus of real-world retail and tax invoices with multiple complex layouts, domains and geographies. The empirical evaluation shows that our system outperforms 4 prominent commercial invoice solutions 1 that use discriminatively trained models with architectures specifically crafted for invoice extraction. We extracted 6 fields with zero upfront human annotation or training with an Avg. F1 of 87.50.", "venue": "ArXiv", "authors": ["Prithiviraj  Damodaran", "Prabhkaran  Singh", "Josemon  Achankuju"], "year": 2021, "n_citations": 0}
{"id": 572557, "s2_id": "ae66df12d9612e8f45edfdddf6f5ee4d0321dbf6", "title": "Improving Accuracy and Speeding Up Document Image Classification Through Parallel Systems", "abstract": "This paper presents a study showing the benefits of the EfficientNet models compared with heavier Convolutional Neural Networks (CNNs) in the Document Classification task, essential problem in the digitalization process of institutions. We show in the RVL-CDIP dataset that we can improve previous results with a much lighter model and present its transfer learning capabilities on a smaller in-domain dataset such as Tobacco3482. Moreover, we present an ensemble pipeline which is able to boost solely image input by combining image model predictions with the ones generated by BERT model on extracted text by OCR. We also show that the batch size can be effectively increased without hindering its accuracy so that the training process can be sped up by parallelizing throughout multiple GPUs, decreasing the computational time needed. Lastly, we expose the training performance differences between PyTorch and Tensorflow Deep Learning frameworks.", "venue": "ICCS", "authors": ["Javier  Ferrando", "Juan Luis Dominguez", "Jordi  Torres", "Raul  Garcia", "David  Garcia", "Daniel  Garrido", "Jordi  Cortada", "Mateo  Valero"], "year": 2020, "n_citations": 4}
{"id": 573686, "s2_id": "8fbed60ac866b282f7a6c64e0abc091f71f4d649", "title": "Learning and Planning in the Feature Deception Problem", "abstract": "Today\u2019s high-stakes adversarial interactions feature attackers who constantly breach the ever-improving security measures. Deception mitigates the defender\u2019s loss by misleading the attacker to make suboptimal decisions. In order to formally reason about deception, we introduce the feature deception problem (FDP), a domain-independent model and present a learning and planning framework for finding the optimal deception strategy, taking into account the adversary\u2019s preferences which are initially unknown to the defender. We make the following contributions. (1) We show that we can uniformly learn the adversary\u2019s preferences using data from a modest number of deception strategies. (2) We propose an approximation algorithm for finding the optimal deception strategy given the learned preferences and show that the problem is NP-hard. (3) We perform extensive experiments to validate our methods and results. In addition, we provide a case study of the credit bureau network to illustrate how FDP implements deception on a real-world problem.", "venue": "GameSec", "authors": ["Zheyuan Ryan Shi", "Ariel D. Procaccia", "Kevin S. Chan", "Sridhar  Venkatesan", "Noam", "Ben-Asher", "Nandi O. Leslie", "Charles  Kamhoua", "Fei  Fang"], "year": 2020, "n_citations": 3}
{"id": 575128, "s2_id": "11904a501707463c1033bfdbe4b3e08141a46e1b", "title": "Light-Weighted CNN for Text Classification", "abstract": "For management, documents are categorized into a specific category, and to do these, most of the organizations use manual labor. In today's automation era, manual efforts on such a task are not justified, and to avoid this, we have so many software out there in the market. However, efficiency and minimal resource consumption is the focal point which is also creating a competition. The categorization of such documents into specified classes by machine provides excellent help. One of categorization technique is text classification using a Convolutional neural network(TextCNN). TextCNN uses multiple sizes of filters, as in the case of the inception layer introduced in Googlenet. The network provides good accuracy but causes high memory consumption due to a large number of trainable parameters. As a solution to this problem, we introduced a whole new architecture based on separable convolution. The idea of separable convolution already exists in the field of image classification but not yet introduces to text classification tasks. With the help of this architecture, we can achieve a drastic reduction in trainable parameters.", "venue": "ArXiv", "authors": ["Ritu  Yadav"], "year": 2020, "n_citations": 1}
{"id": 581842, "s2_id": "c5dd6b50150fab2e60ec14d5c52b1c1f0a62567f", "title": "Fully Automated Segmentation of Hyperreflective Foci in Optical Coherence Tomography Images", "abstract": "The automatic detection of disease related entities in retinal imaging data is relevant for disease- and treatment monitoring. It enables the quantitative assessment of large amounts of data and the corresponding study of disease characteristics. The presence of hyperreflective foci (HRF) is related to disease progression in various retinal diseases. Manual identification of HRF in spectral-domain optical coherence tomography (SD-OCT) scans is error-prone and tedious. We present a fully automated machine learning approach for segmenting HRF in SD-OCT scans. Evaluation on annotated OCT images of the retina demonstrates that a residual U-Net allows to segment HRF with high accuracy. As our dataset comprised data from different retinal diseases including age-related macular degeneration, diabetic macular edema and retinal vein occlusion, the algorithm can safely be applied in all of them though different pathophysiological origins are known.", "venue": "ArXiv", "authors": ["Thomas  Schlegl", "Hrvoje  Bogunovic", "Sophie  Klimscha", "Philipp  Seeb\u00f6ck", "Amir  Sadeghipour", "Bianca S. Gerendas", "Sebastian M. Waldstein", "Georg  Langs", "Ursula  Schmidt-Erfurth"], "year": 2018, "n_citations": 7}
{"id": 584346, "s2_id": "126c6489d126583db48c24a21826f2352b649d21", "title": "ADAIL: Adaptive Adversarial Imitation Learning", "abstract": "We present the ADaptive Adversarial Imitation Learning (ADAIL) algorithm for learning adaptive policies that can be transferred between environments of varying dynamics, by imitating a small number of demonstrations collected from a single source domain. This is an important problem in robotic learning because in real world scenarios 1) reward functions are hard to obtain, 2) learned policies from one domain are difficult to deploy in another due to varying source to target domain statistics, 3) collecting expert demonstrations in multiple environments where the dynamics are known and controlled is often infeasible. We address these constraints by building upon recent advances in adversarial imitation learning; we condition our policy on a learned dynamics embedding and we employ a domain-adversarial loss to learn a dynamics-invariant discriminator. The effectiveness of our method is demonstrated on simulated control tasks with varying environment dynamics and the learned adaptive agent outperforms several recent baselines.", "venue": "ArXiv", "authors": ["Yiren  Lu", "Jonathan  Tompson"], "year": 2020, "n_citations": 1}
{"id": 611592, "s2_id": "9458a72a8e40dae0d0cd3827669c805d63876d51", "title": "Fuzzy Clustering to Identify Clusters at Different Levels of Fuzziness: An Evolutionary Multiobjective Optimization Approach", "abstract": "Fuzzy clustering methods identify naturally occurring clusters in a dataset, where the extent to which different clusters are overlapped can differ. Most methods have a parameter to fix the level of fuzziness. However, the appropriate level of fuzziness depends on the application at hand. This paper presents an entropy ${c}$ -means (ECM), a method of fuzzy clustering that simultaneously optimizes two contradictory objective functions, resulting in the creation of fuzzy clusters with different levels of fuzziness. This allows ECM to identify clusters with different degrees of overlap. ECM optimizes the two objective functions using two multiobjective optimization methods, nondominated sorting genetic algorithm II (NSGA-II) and multiobjective evolutionary algorithm based on decomposition (MOEA/D). We also propose a method to select a suitable tradeoff clustering from the Pareto front. Experiments on challenging synthetic datasets as well as real-world datasets show that ECM leads to better cluster detection compared to the conventional fuzzy clustering methods as well as previously used multiobjective methods for fuzzy clustering.", "venue": "IEEE Transactions on Cybernetics", "authors": ["Avisek  Gupta", "Shounak  Datta", "Swagatam  Das"], "year": 2021, "n_citations": 7}
{"id": 613073, "s2_id": "19b2747d6983e38cee842435c3e455f584ca5f66", "title": "Differentiable Generalised Predictive Coding", "abstract": "This paper deals with differentiable dynamical models congruent with neural process theories that cast brain function as the hierarchical refinement of an internal generative model explaining observations. Our work extends existing implementations of gradient-based predictive coding with automatic differentiation and allows to integrate deep neural networks for non-linear state parameterization. Gradient-based predictive coding optimises inferred states and weights locally in for each layer by optimising precision-weighted prediction errors that propagate from stimuli towards latent states. Predictions flow backwards, from latent states towards lower layers. The model suggested here optimises hierarchical and dynamical predictions of latent states. Hierarchical predictions encode expected content and hierarchical structure. Dynamical predictions capture changes in the encoded content along with higher order derivatives. Hierarchical and dynamical predictions interact and address different aspects of the same latent states. We apply the model to various perception and planning tasks on sequential data and show their mutual dependence. In particular, we demonstrate how learning sampling distances in parallel address meaningful locations data sampled at discrete time steps. We discuss possibilities to relax the assumption of linear hierarchies in favor of more flexible graph structure with emergent properties. We compare the granular structure of the model with canonical microcircuits describing predictive coding in biological networks and review the connection to Markov Blankets as a tool to characterize modularity. A final section sketches out ideas for efficient perception and planning in nested spatio-temporal hierarchies. We open source the Torch code for the suggested generalized predictive coding optimizer, GPC and hope that our work initiates more research on generalized state-space models in the context of deep neural networks, reinforcement learning and aspects of biological plausibility.", "venue": "ArXiv", "authors": ["Andr'e  Ofner", "Sebastian  Stober"], "year": 2021, "n_citations": 0}
{"id": 620997, "s2_id": "09d95eb8c8882f60d72fda78f16d8108c55f4f38", "title": "Identifying microlensing events using neural networks", "abstract": "Current gravitational microlensing surveys are observing hundreds of millions of stars in the Galactic bulge - which makes finding rare microlensing events a challenging tasks. In almost all previous works, microlensing events have been detected either by applying very strict selection cuts or manually inspecting tens of thousands of light curves. However, the number of microlensing events expected in the future space-based microlensing experiments forces us to consider fully-automated approaches. They are especially important for selecting binary-lens events that often exhibit complex light curve morphologies and are otherwise difficult to find. There are no dedicated selection algorithms for binary-lens events in the literature, which hampers their statistical studies. Here, we present two simple neural-network-based classifiers for detecting single and binary microlensing events. We demonstrate their robustness using OGLE-III and OGLE-IV data sets and show they perform well on microlensing events detected in data from the Zwicky Transient Facility (ZTF). Classifiers are able to correctly recognize ~98% of single-lens events and 80-85% of binary-lens events.", "venue": "ArXiv", "authors": ["Przemek  Mroz"], "year": 2020, "n_citations": 4}
{"id": 622788, "s2_id": "6f4b7f6185b859478655bada08da13e6a843b67f", "title": "Path-Enhanced Multi-Relational Question Answering with Knowledge Graph Embeddings", "abstract": "The multi-relational Knowledge Base Question Answering (KBQA) system performs multi-hop reasoning over the knowledge graph (KG) to achieve the answer. Recent approaches attempt to introduce the knowledge graph embedding (KGE) technique to handle the KG incompleteness but only consider the triple facts and neglect the significant semantic correlation between paths and multirelational questions. In this paper, we propose a Path and Knowledge Embedding-Enhanced multi-relational Question Answering model (PKEEQA), which leverages multi-hop paths between entities in the KG to evaluate the ambipolar correlation between a path embedding and a multi-relational question embedding via a customizable path representation mechanism, benefiting for achieving more accurate answers from the perspective of both the triple facts and the extra paths. Experimental results illustrate that PKEEQA improves KBQA models\u2019 performance for multirelational question answering with explainability to some extent derived from paths.", "venue": "ArXiv", "authors": ["Guanglin  Niu", "Yang  Li", "Chengguang  Tang", "Zhongkai  Hu", "Shibin  Yang", "Peng  Li", "Chengyu  Wang", "Hao  Wang", "Jian  Sun"], "year": 2021, "n_citations": 0}
{"id": 647813, "s2_id": "9d6c108cdbb6c8617a57324ea1d85f534316e1e0", "title": "Robust Prediction When Features are Missing", "abstract": "Predictors are learned using past training data which may contain features that are unavailable at the time of prediction. We develop an approach that is robust against outlying missing features, based on the optimality properties of an oracle predictor which observes them. The robustness properties of the approach are demonstrated on both real and synthetic data.", "venue": "IEEE Signal Processing Letters", "authors": ["Xiuming  Liu", "Dave  Zachariah", "Petre  Stoica"], "year": 2020, "n_citations": 1}
{"id": 660582, "s2_id": "a6904a2933292397f9a9867fb0ebb0dbf46e1cf9", "title": "Revisiting the Role of Euler Numerical Integration on Acceleration and Stability in Convex Optimization", "abstract": "Viewing optimization methods as numerical integrators for ordinary differential equations (ODEs) provides a thought-provoking modern framework for studying accelerated first-order optimizers. In this literature, acceleration is often supposed to be linked to the quality of the integrator (accuracy, energy preservation, symplecticity). In this work, we propose a novel ordinary differential equation that questions this connection: both the explicit and the semi-implicit (a.k.a symplectic) Euler discretizations on this ODE lead to an accelerated algorithm for convex programming. Although semi-implicit methods are well-known in numerical analysis to enjoy many desirable features for the integration of physical systems, our findings show that these properties do not necessarily relate to acceleration.", "venue": "AISTATS", "authors": ["Peiyuan  Zhang", "Antonio  Orvieto", "Hadi  Daneshmand", "Thomas  Hofmann", "Roy  Smith"], "year": 2021, "n_citations": 0}
{"id": 671711, "s2_id": "f9d442acee372fd2f2fca3295780b85677c616a9", "title": "CommPOOL: An Interpretable Graph Pooling Framework for Hierarchical Graph Representation Learning", "abstract": "Recent years have witnessed the emergence and flourishing of hierarchical graph pooling neural networks (HGPNNs) which are effective graph representation learning approaches for graph level tasks such as graph classification. However, current HGPNNs do not take full advantage of the graph's intrinsic structures (e.g., community structure). Moreover, the pooling operations in existing HGPNNs are difficult to be interpreted. In this paper, we propose a new interpretable graph pooling framework - CommPOOL, that can capture and preserve the hierarchical community structure of graphs in the graph representation learning process. Specifically, the proposed community pooling mechanism in CommPOOL utilizes an unsupervised approach for capturing the inherent community structure of graphs in an interpretable manner. CommPOOL is a general and flexible framework for hierarchical graph representation learning that can further facilitate various graph-level tasks. Evaluations on five public benchmark datasets and one synthetic dataset demonstrate the superior performance of CommPOOL in graph representation learning for graph classification compared to the state-of-the-art baseline methods, and its effectiveness in capturing and preserving the community structure of graphs.", "venue": "Neural Networks", "authors": ["Haoteng  Tang", "Guixiang  Ma", "Lifang  He", "Heng  Huang", "Liang  Zhan"], "year": 2021, "n_citations": 1}
{"id": 671714, "s2_id": "f6f435b85f79948ed8abd85a81040b29dfee2640", "title": "Guided Generative Adversarial Neural Network for Representation Learning and High Fidelity Audio Generation using Fewer Labelled Audio Data", "abstract": "Recent improvements in Generative Adversarial Neural Networks (GANs) have shown their ability to generate higher quality samples as well as to learn good representations for transfer learning. Most of the representation learning methods based on GANs learn representations ignoring their post-use scenario, which can lead to increased generalisation ability. However, the model can become redundant if it is intended for a specific task. For example, assume we have a vast unlabelled audio dataset, and we want to learn a representation from this dataset so that it can be used to improve the emotion recognition performance of a small labelled audio dataset. During the representation learning training, if the model does not know the post emotion recognition task, it can completely ignore emotion-related characteristics in the learnt representation. This is a fundamental challenge for any unsupervised representation learning model. In this paper, we aim to address this challenge by proposing a novel GAN framework: Guided Generative Neural Network (GGAN), which guides a GAN to focus on learning desired representations and generating superior quality samples for audio data leveraging fewer labelled samples. Experimental results show that using a very small amount of labelled data as guidance, a GGAN learns significantly better representations.", "venue": "ArXiv", "authors": ["Kazi Nazmul Haque", "Rajib  Rana", "Bjorn  Schuller"], "year": 2020, "n_citations": 2}
{"id": 683227, "s2_id": "b098747871f57d3d6833c8d56d8151388191bbc0", "title": "Optimizing Majority Voting Based Systems Under a Resource Constraint for Multiclass Problems", "abstract": "Ensemble-based approaches are very effective in various fields in raising the accuracy of its individual members, when some voting rule is applied for aggregating the individual decisions. In this paper, we investigate how to find and characterize the ensembles having the highest accuracy if the total cost of the ensemble members is bounded. This question leads to Knapsack problem with non-linear and non-separable objective function in binary and multiclass classification if the majority voting is chosen for the aggregation. As the conventional solving methods cannot be applied for this task, a novel stochastic approach was introduced in the binary case where the energy function is discussed as the joint probability function of the member accuracy. We show some theoretical results with respect to the expected ensemble accuracy and its variance in the multiclass classification problem which can help us to solve the Knapsack problem.", "venue": "Progress in Industrial Mathematics at ECMI 2018", "authors": ["Attila  Tiba", "Andr\u00e1s  Hajdu", "Gy\u00f6rgy  Terdik", "Henrietta  Tom\u00e1n"], "year": 2019, "n_citations": 2}
{"id": 687025, "s2_id": "1f6a8e0b630cd1557fe88484622da796af79a95a", "title": "End-to-End Training of a Large Vocabulary End-to-End Speech Recognition System", "abstract": "In this paper, we present an end-to-end training framework for building state-of-the-art end-to-end speech recognition systems. Our training system utilizes a cluster of Central Processing Units (CPUs) and Graphics Processing Units (GPUs). The entire data reading, large scale data augmentation, neural network parameter updates are all performed \u201con-the-fly\u201d. We use vocal tract length perturbation [1] and an acoustic simulator [2] for data augmentation. The processed features and labels are sent to the GPU cluster. The Horovod allreduce approach is employed to train neural network parameters. We evaluated the effectiveness of our system on the standard Librispeech corpus [3] and the 10,000-hr anonymized Bixby English dataset. Our end-to-end speech recognition system built using this training infrastructure showed a 2.44 % WER on test-clean of the LibriSpeech test set after applying shallow fusion with a Transformer language model (LM). For the proprietary English Bixby open domain test set, we obtained a WER of 7.92 % using a Bidirectional Full Attention (BFA) end-to-end model after applying shallow fusion with an RNN-LM. When the monotonic chunckwise attention (MoCha) based approach is employed for streaming speech recognition, we obtained a WER of 9.95 % on the same Bixby open domain test set.", "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)", "authors": ["Chanwoo  Kim", "Sungsoo  Kim", "Kwangyoun  Kim", "Mehul  Kumar", "Jiyeon  Kim", "Kyungmin  Lee", "Changwoo  Han", "Abhinav  Garg", "Eunhyang  Kim", "Minkyoo  Shin", "Shatrughan  Singh", "Larry  Heck", "Dhananjaya  Gowda"], "year": 2019, "n_citations": 16}
{"id": 689977, "s2_id": "d12eff33ea0dc96f28d83218e32696f6c6fc0043", "title": "Non-parametric Models for Non-negative Functions", "abstract": "Linear models have shown great effectiveness and flexibility in many fields such as machine learning, signal processing and statistics. They can represent rich spaces of functions while preserving the convexity of the optimization problems where they are used, and are simple to evaluate, differentiate and integrate. However, for modeling non-negative functions, which are crucial for unsupervised learning, density estimation, or non-parametric Bayesian methods, linear models are not applicable directly. Moreover, current state-of-the-art models like generalized linear models either lead to non-convex optimization problems, or cannot be easily integrated. In this paper we provide the first model for non-negative functions which benefits from the same good properties of linear models. In particular, we prove that it admits a representer theorem and provide an efficient dual formulation for convex problems. We study its representation power, showing that the resulting space of functions is strictly richer than that of generalized linear models. Finally we extend the model and the theoretical results to functions with outputs in convex cones. The paper is complemented by an experimental evaluation of the model showing its effectiveness in terms of formulation, algorithmic derivation and practical results on the problems of density estimation, regression with heteroscedastic errors, and multiple quantile regression.", "venue": "NeurIPS", "authors": ["Ulysse  Marteau-Ferey", "Francis  Bach", "Alessandro  Rudi"], "year": 2020, "n_citations": 16}
{"id": 708237, "s2_id": "1cb41bf7d941088b113d36a9f6e1a25ed16ed747", "title": "Pendant Drop Tensiometry: A Machine Learning Approach", "abstract": "Modern pendant drop tensiometry relies on the numerical solution of the Young-Laplace equation and allows us to determine the surface tension from a single picture of a pendant drop with high precision. Most of these techniques solve the Young-Laplace equation many times over to find the material parameters that provide a fit to a supplied image of a real droplet. Here, we introduce a machine learning approach to solve this problem in a computationally more efficient way. We train a deep neural network to determine the surface tension of a given droplet shape using a large training set of numerically generated droplet shapes. We show that the deep learning approach is superior to the current state of the art shape fitting approach in speed and precision, in particular if shapes in the training set reflect the sensitivity of the droplet shape with respect to surface tension. In order to derive such an optimized training set, we clarify the role of the Worthington number as a quality indicator in conventional shape fitting and in the machine learning approach. Our approach demonstrates the capabilities of deep neural networks in the material parameter determination from rheological deformation experiments, in general.", "venue": "The Journal of chemical physics", "authors": ["Felix  Kratz", "Jan  Kierfeld"], "year": 2020, "n_citations": 4}
{"id": 715998, "s2_id": "5458d8c21735dc40998a6d8077f65fb0afcc5cbb", "title": "Procedural Content Generation: Better Benchmarks for Transfer Reinforcement Learning", "abstract": "The idea of transfer in reinforcement learning (TRL) is intriguing: being able to transfer knowledge from one problem to another problem without learning everything from scratch. This promises quicker learning and learning more complex methods. To gain an insight into the field and to detect emerging trends, we performed a database search. We note a surprisingly late adoption of deep learning that starts in 2018. The introduction of deep learning has not yet solved the greatest challenge of TRL: generalization. Transfer between different domains works well when domains have strong similarities (e.g. MountainCar to Cartpole), and most TRL publications focus on different tasks within the same domain that have few differences. Most TRL applications we encountered compare their improvements against self-defined baselines, and the field is still missing unified benchmarks. We consider this to be a disappointing situation. For the future, we note that: (1) A clear measure of task similarity is needed. (2) Generalization needs to improve. Promising approaches merge deep learning with planning via MCTS or introduce memory through LSTMs. (3) The lack of benchmarking tools will be remedied to enable meaningful comparison and measure progress. Already Alchemy and Meta-World are emerging as interesting benchmark suites. We note that another development, the increase in procedural content generation (PCG), can improve both benchmarking and generalization in TRL.", "venue": "2021 IEEE Conference on Games (CoG)", "authors": ["Matthias  M\u00fcller-Brockhausen", "Mike  Preuss", "Aske  Plaat"], "year": 2021, "n_citations": 1}
{"id": 734208, "s2_id": "aa42316a94e55edf562a816c3fe7520ac667a2e3", "title": "The Many Faces of Exponential Weights in Online Learning", "abstract": "A standard introduction to online learning might place Online Gradient Descent at its center and then proceed to develop generalizations and extensions like Online Mirror Descent and second-order methods. Here we explore the alternative approach of putting exponential weights (EW) first. We show that many standard methods and their regret bounds then follow as a special case by plugging in suitable surrogate losses and playing the EW posterior mean. For instance, we easily recover Online Gradient Descent by using EW with a Gaussian prior on linearized losses, and, more generally, all instances of Online Mirror Descent based on regular Bregman divergences also correspond to EW with a prior that depends on the mirror map. Furthermore, appropriate quadratic surrogate losses naturally give rise to Online Gradient Descent for strongly convex losses and to Online Newton Step. We further interpret several recent adaptive methods (iProd, Squint, and a variation of Coin Betting for experts) as a series of closely related reductions to exp-concave surrogate losses that are then handled by Exponential Weights. Finally, a benefit of our EW interpretation is that it opens up the possibility of sampling from the EW posterior distribution instead of playing the mean. As already observed by Bubeck and Eldan, this recovers the best-known rate in Online Bandit Linear Optimization.", "venue": "COLT", "authors": ["Dirk van der Hoeven", "Tim van Erven", "Wojciech  Kotlowski"], "year": 2018, "n_citations": 25}
{"id": 785520, "s2_id": "0e50d7b5535791f08a68232805a89cad158a992c", "title": "A Method for Curation of Web-Scraped Face Image Datasets", "abstract": "Web-scraped, in-the-wild datasets have become the norm in face recognition research. The numbers of subjects and images acquired in web-scraped datasets are usually very large, with number of images on the millions scale. A variety of issues occur when collecting a dataset in-the-wild, including images with the wrong identity label, duplicate images, duplicate subjects and variation in quality. With the number of images being in the millions, a manual cleaning procedure is not feasible. But fully automated methods used to date result in a less-than-ideal level of clean dataset. We propose a semi-automated method, where the goal is to have a clean dataset for testing face recognition methods, with similar quality across men and women, to support comparison of accuracy across gender. Our approach removes near-duplicate images, merges duplicate subjects, corrects mislabeled images, and removes images outside a defined range of pose and quality. We conduct the curation on the Asian Face Dataset (AFD) and VGGFace2 test dataset. The experiments show that a state-of-the-art method achieves a much higher accuracy on the datasets after they are curated. Finally, we release our cleaned versions of both datasets to the research community.", "venue": "2020 8th International Workshop on Biometrics and Forensics (IWBF)", "authors": ["Kai  Zhang", "V'itor  Albiero", "Kevin W. Bowyer"], "year": 2020, "n_citations": 2}
{"id": 813411, "s2_id": "73108cc520ca8fa2e46130f84680d21176d7d375", "title": "A Biologically Plausible Benchmark for Contextual Bandit Algorithms in Precision Oncology Using in vitro Data", "abstract": "Precision oncology, the genetic sequencing of tumors to identify druggable targets, has emerged as the standard of care in the treatment of many cancers. Nonetheless, due to the pace of therapy development and variability in patient information, designing effective protocols for individual treatment assignment in a sample-efficient way remains a major challenge. One promising approach to this problem is to frame precision oncology treatment as a contextual bandit problem and to apply sequential decision-making algorithms designed to minimize regret in this setting. However, a clear prerequisite for considering this methodology in high-stakes clinical decisions is careful benchmarking to understand realistic costs and benefits. Here, we propose a benchmark dataset to evaluate contextual bandit algorithms based on real in vitro drug response of approximately 900 cancer cell lines. Specifically, we curated a dataset of complete treatment responses for a subset of 7 treatments from prior in vitro studies. This allows us to compute the regret of proposed decision policies using biologically plausible counterfactuals. We ran a suite of Bayesian bandit algorithms on our benchmark, and found that the methods accumulate less regret over a sequence of treatment assignment tasks than a rule-based baseline derived from current clinical practice. This effect was more pronounced when genomic information was included as context. We expect this work to be a starting point for evaluation of both the unique structural requirements and ethical implications for real-world testing of bandit based clinical decision support.", "venue": "ArXiv", "authors": ["Niklas T. Rindtorff", "MingYu  Lu", "Nisarg A. Patel", "Huahua  Zheng", "Alexander  D'Amour"], "year": 2019, "n_citations": 2}
{"id": 829298, "s2_id": "8dcf997ac9cf47b3d7967c49225ab5f3d0815a38", "title": "A more globally accurate dimensionality reduction method using triplets", "abstract": "We first show that the commonly used dimensionality reduction (DR) methods such as t-SNE and LargeVis poorly capture the global structure of the data in the low dimensional embedding. We show this via a number of tests for the DR methods that can be easily applied by any practitioner to the dataset at hand. Surprisingly enough, t-SNE performs the best w.r.t. the commonly used measures that reward the local neighborhood accuracy such as precision-recall while having the worst performance in our tests for global structure. We then contrast the performance of these two DR method against our new method called TriMap. The main idea behind TriMap is to capture higher orders of structure with triplet information (instead of pairwise information used by t-SNE and LargeVis), and to minimize a robust loss function for satisfying the chosen triplets. We provide compelling experimental evidence on large natural datasets for the clear advantage of the TriMap DR results. As LargeVis, TriMap scales linearly with the number of data points.", "venue": "ArXiv", "authors": ["Ehsan  Amid", "Manfred K. Warmuth"], "year": 2018, "n_citations": 11}
{"id": 844112, "s2_id": "71230c7e1deecb0ab413effe214b83c19fe1af45", "title": "Locality-sensitive hashing in function spaces", "abstract": "We discuss the problem of performing similarity search over function spaces. To perform search over such spaces in a reasonable amount of time, we use {\\it locality-sensitive hashing} (LSH). We present two methods that allow LSH functions on $\\mathbb{R}^N$ to be extended to $L^p$ spaces: one using function approximation in an orthonormal basis, and another using (quasi-)Monte Carlo-style techniques. We use the presented hashing schemes to construct an LSH family for Wasserstein distance over one-dimensional, continuous probability distributions.", "venue": "ArXiv", "authors": ["Will  Shand", "Stephen  Becker"], "year": 2020, "n_citations": 0}
{"id": 861343, "s2_id": "0b6885dfa803b953d7e068557e33bf0672af017a", "title": "Geometry of Deep Learning for Magnetic Resonance Fingerprinting", "abstract": "Current popular methods for Magnetic Resonance Fingerprint (MRF) recovery are bottlenecked by the heavy storage and computation requirements of a dictionary-matching (DM) step due to the growing size and complexity of the fingerprint dictionaries in multi-parametric quantitative MRI applications. In this paper we study a deep learning approach to address these shortcomings. Coupled with a dimensionality reduction first layer, the proposed MRF-Net is able to reconstruct quantitative maps by saving more than 60 times in memory and computations required for a DM baseline. Fine-grid manifold enumeration i.e. the MRF dictionary is only used for training the network and not during image reconstruction. We show that the MRF-Net provides a piece-wise affine approximation to the Bloch response manifold projection and that rather than memorizing the dictionary, the network efficiently clusters this manifold and learns a set of hierarchical matched-filters for affine regression of the NMR characteristics in each segment.", "venue": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Mohammad  Golbabaee", "Dongdong  Chen", "Pedro A. G\u00f3mez", "Marion I. Menzel", "Mike E. Davies"], "year": 2019, "n_citations": 28}
{"id": 873676, "s2_id": "6abef12dfe2ba6afae50e91e132b59e4f974fd64", "title": "Fighting Accounting Fraud Through Forensic Data Analytics", "abstract": "Accounting fraud is a global concern representing a significant threat to the financial system stability due to the resulting diminishing of the market confidence and trust of regulatory authorities. Several tricks can be used to commit accounting fraud, hence the need for non-static regulatory interventions that take into account different fraudulent patterns. Accordingly, this study aims to improve the detection of accounting fraud via the implementation of several machine learning methods to better differentiate between fraud and non-fraud companies, and to further assist the task of examination within the riskier firms by evaluating relevant financial indicators. Out-of-sample results suggest there is a great potential in detecting falsified financial statements through statistical modelling and analysis of publicly available accounting information. The proposed methodology can be of assistance to public auditors and regulatory agencies as it facilitates auditing processes, and supports more targeted and effective examinations of accounting reports.", "venue": "ArXiv", "authors": ["Maria  Jofre", "Richard  Gerlach"], "year": 2018, "n_citations": 4}
{"id": 874849, "s2_id": "6e177105c528cd0c5ea6bfe4576cc3b0fa3e0c23", "title": "Probabilistic and team PFIN-type learning: general properties", "abstract": "We consider the probability hierarchy for Popperian FINite learning and study the general properties of this hierarchy. We prove that the probability hierarchy is decidable, i.e. there exists an algorithm that receives p_1 and p_2 and answers whether PFIN-type learning with the probability of success p_1 is equivalent to PFIN-type learning with the probability of success p_2. \nTo prove our result, we analyze the topological structure of the probability hierarchy. We prove that it is well-ordered in descending ordering and order-equivalent to ordinal epsilon_0. This shows that the structure of the hierarchy is very complicated. \nUsing similar methods, we also prove that, for PFIN-type learning, team learning and probabilistic learning are of the same power.", "venue": "COLT '96", "authors": ["Andris  Ambainis"], "year": 1996, "n_citations": 8}
{"id": 895418, "s2_id": "fb9d156630f323ce1ff6203c1a3cda19d5901327", "title": "Knowledge Distillation Circumvents Nonlinearity for Optical Convolutional Neural Networks", "abstract": "In recent years, Convolutional Neural Networks (CNNs) have enabled ubiquitous image processing applications. As such, CNNs require fast runtime (forward propagation) to process high-resolution visual streams in real time. This is still a challenging task even with state-of-the-art graphics and tensor processing units. The bottleneck in computational efficiency primarily occurs in the convolutional layers. Performing operations in the Fourier domain is a promising way to accelerate forward propagation since it transforms convolutions into elementwise multiplications, which are considerably faster to compute for large kernels. Furthermore, such computation could be implemented using an optical 4f system with orders of magnitude faster operation. However, a major challenge in using this spectral approach, as well as in an optical implementation of CNNs, is the inclusion of a nonlinearity between each convolutional layer, without which CNN performance drops dramatically. Here, we propose a Spectral CNN Linear Counterpart (SCLC) network architecture and develop a Knowledge Distillation (KD) approach to circumvent the need for a nonlinearity and successfully train such networks. While the KD approach is known in machine learning as an effective process for network pruning, we adapt the approach to transfer the knowledge from a nonlinear network (teacher) to a linear counterpart (student). We show that the KD approach can achieve performance that easily surpasses the standard linear version of a CNN and could approach the performance of the nonlinear network. Our simulations show that the possibility of increasing the resolution of the input image allows our proposed 4f optical linear network to perform more efficiently than a nonlinear network with the same accuracy on two fundamental image processing tasks: (i) object classification and (ii) semantic segmentation.", "venue": "ArXiv", "authors": ["Jinlin  Xiang", "Shane  Colburn", "Arka  Majumdar", "Eli  Shlizerman"], "year": 2021, "n_citations": 0}
{"id": 896057, "s2_id": "e63b052d555b4870fae0ec44de4319db6bebde84", "title": "Stochastic Deconvolutional Neural Network Ensemble Training on Generative Pseudo-Adversarial Networks", "abstract": "The training of Generative Adversarial Networks is a difficult task mainly due to the nature of the networks. One such issue is when the generator and discriminator start oscillating, rather than converging to a fixed point. Another case can be when one agent becomes more adept than the other which results in the decrease of the other agent's ability to learn, reducing the learning capacity of the system as a whole. Additionally, there exists the problem of Mode Collapse which involves the generators output collapsing to a single sample or a small set of similar samples. To train GANs a careful selection of the architecture that is used along with a variety of other methods to improve training. Even when applying these methods there is low stability of training in relation to the parameters that are chosen. Stochastic ensembling is suggested as a method for improving the stability while training GANs.", "venue": "ArXiv", "authors": ["Alexey  Chaplygin", "Joshua  Chacksfield"], "year": 2018, "n_citations": 0}
{"id": 909622, "s2_id": "98f7b4a0d6ca8fbc193781120e40231d0de61247", "title": "A Comparative Evaluation of Quantification Methods", "abstract": "Quantification represents the problem of predicting class distributions in a given target set. It also represents a growing research field in supervised machine learning, for which a large variety of different algorithms has been proposed in recent years. However, a comprehensive empirical comparison of quantification methods that supports algorithm selection is not available yet. In this work, we close this research gap by conducting a thorough empirical performance comparison of 24 different quantification methods. To consider a broad range of different scenarios for binary as well as multiclass quantification settings, we carried out almost 3 million experimental runs on 40 data sets. We observe that no single algorithm generally outperforms all competitors, but identify a group of methods including the Median Sweep and the DyS framework that perform significantly better in binary settings. For the multiclass setting, we observe that a different, broad group of algorithms yields good performance, including the Generalized Probabilistic Adjusted Count, the readme method, the energy distance minimization method, the EM algorithm for quantification, and Friedman\u2019s method. More generally, we find that the performance on multiclass quantification is inferior to the results obtained in the binary setting. Our results can guide practitioners who intend to apply quantification algorithms and help researchers to identify opportunities for future research.", "venue": "ArXiv", "authors": ["Tobias  Schumacher", "Markus  Strohmaier", "Florian  Lemmerich"], "year": 2021, "n_citations": 3}
{"id": 910957, "s2_id": "d2768ea34b8e3ed8f7e91584bf6c517b49642c2f", "title": "Low-Rank Boolean Matrix Approximation by Integer Programming", "abstract": "Low-rank approximations of data matrices are an important dimensionality reduction tool in machine learning and regression analysis. We consider the case of categorical variables, where it can be formulated as the problem of finding low-rank approximations to Boolean matrices. In this paper we give what is to the best of our knowledge the first integer programming formulation that relies on only polynomially many variables and constraints, we discuss how to solve it computationally and report numerical tests on synthetic and real-world data.", "venue": "ArXiv", "authors": ["Reka  Kovacs", "Oktay  G\u00fcnl\u00fck", "Raphael  Hauser"], "year": 2018, "n_citations": 3}
{"id": 921562, "s2_id": "f90c9f34a1214d07e4e36b5e9f430fc8a733f4d1", "title": "Graph Neural Networks with Continual Learning for Fake News Detection from Social Media", "abstract": "Although significant effort has been applied to fact-checking, the prevalence of fake news over social media, which has profound impact on justice, public trust and our society, remains a serious problem. In this work, we focus on propagation-based fake news detection, as recent studies have demonstrated that fake news and real news spread differently online. Specifically, considering the capability of graph neural networks (GNNs) in dealing with non-Euclidean data, we use GNNs to differentiate between the propagation patterns of fake and real news on social media. In particular, we concentrate on two questions: (1) Without relying on any text information, e.g., tweet content, replies and user descriptions, how accurately can GNNs identify fake news? Machine learning models are known to be vulnerable to adversarial attacks, and avoiding the dependence on text-based features can make the model less susceptible to the manipulation of advanced fake news fabricators. (2) How to deal with new, unseen data? In other words, how does a GNN trained on a given dataset perform on a new and potentially vastly different dataset? If it achieves unsatisfactory performance, how do we solve the problem without re-training the model on the entire data from scratch? We study the above questions on two datasets with thousands of labelled news items, and our results show that: (1) GNNs can achieve comparable or superior performance without any text information to state-of-the-art methods. (2) GNNs trained on a given dataset may perform poorly on new, unseen data, and direct incremental training cannot solve the problem---this issue has not been addressed in the previous work that applies GNNs for fake news detection. In order to solve the problem, we propose a method that achieves balanced performance on both existing and new datasets, by using techniques from continual learning to train GNNs incrementally.", "venue": "ArXiv", "authors": ["Yi  Han", "Shanika  Karunasekera", "Christopher  Leckie"], "year": 2020, "n_citations": 17}
{"id": 932835, "s2_id": "0fedc892cfb7641db1004b969f7657783b189500", "title": "Deep Sequential Models for Suicidal Ideation From Multiple Source Data", "abstract": "This paper presents a novel method for predicting suicidal ideation from electronic health records (EHR) and ecological momentary assessment (EMA) data using deep sequential models. Both EHR longitudinal data and EMA question forms are defined by asynchronous, variable length, randomly sampled data sequences. In our method, we model each of them with a recurrent neural network, and both sequences are aligned by concatenating the hidden state of each of them using temporal marks. Furthermore, we incorporate attention schemes to improve performance in long sequences and time-independent pre-trained schemes to cope with very short sequences. Using a database of 1023 patients, our experimental results show that the addition of EMA records boosts the system recall to predict the suicidal ideation diagnosis from 48.13% obtained exclusively from EHR-based state-of-the-art methods to 67.78%. Additionally, our method provides interpretability through the t-distributed stochastic neighbor embedding (t-SNE) representation of the latent space. Furthermore, the most relevant input features are identified and interpreted medically.", "venue": "IEEE Journal of Biomedical and Health Informatics", "authors": ["Ignacio  Peis", "Pablo M. Olmos", "Constanza  Vera-Varela", "Mar\u00eda Luisa Barrig\u00f3n", "Philippe  Courtet", "Enrique  Baca-Garc\u00eda", "Antonio  Art\u00e9s-Rodr\u00edguez"], "year": 2019, "n_citations": 6}
{"id": 933749, "s2_id": "ef2a773c3c7848a6cc16b18164be5f8876a310af", "title": "Scaling Up Influence Functions", "abstract": "We address efficient calculation of influence functions (Koh and Liang 2017) for tracking predictions back to the training data. We propose and analyze a new approach to speeding up the inverse Hessian calculation based on Arnoldi iteration (Arnoldi 1951). With this improvement, we achieve, to the best of our knowledge, the first successful implementation of influence functions that scales to full-size (language and vision) Transformer models with several hundreds of millions of parameters. We evaluate our approach on image classification and sequence-to-sequence tasks with tens to a hundred of millions of training examples. Our code will be available at https://github.com/google-research/jax-influence.", "venue": "ArXiv", "authors": ["Andrea  Schioppa", "Polina  Zablotskaia", "David  Vilar", "Artem  Sokolov"], "year": 2021, "n_citations": 0}
{"id": 940767, "s2_id": "c9272ce68ac1ec498568ed78df564b8322921462", "title": "A Strong Baseline for Weekly Time Series Forecasting", "abstract": "Many businesses and industries require accurate forecasts for weekly time series nowadays. The forecasting literature however does not currently provide easy-to-use, automatic, reproducible and accurate approaches dedicated to this task. We propose a forecasting method that can be used as a strong baseline in this domain, leveraging state-of-the-art forecasting techniques, forecast combination, and global modelling. Our approach uses four base forecasting models specifically suitable for forecasting weekly data: a global Recurrent Neural Network model, Theta, Trigonometric Box-Cox ARMA Trend Seasonal (TBATS), and Dynamic Harmonic Regression ARIMA (DHR-ARIMA). Those are then optimally combined using a lasso regression stacking approach. We evaluate the performance of our method against a set of state-of-the-art weekly forecasting models on six datasets. Across four evaluation metrics, we show that our method consistently outperforms the benchmark methods by a considerable margin with statistical significance. In particular, our model can produce the most accurate forecasts, in terms of mean sMAPE, for the M4 weekly dataset.", "venue": "ArXiv", "authors": ["Rakshitha  Godahewa", "Christoph  Bergmeir", "Geoffrey I. Webb", "Pablo  Montero-Manso"], "year": 2020, "n_citations": 2}
{"id": 941865, "s2_id": "b82af699979d56de7190a7639802089076a7cd9f", "title": "Correlated Feature Selection with Extended Exclusive Group Lasso", "abstract": "In many high dimensional classification or regression problems set in a biological context, the complete identification of the set of informative features is often as important as predictive accuracy, since this can provide mechanistic insight and conceptual understanding. Lasso and related algorithms have been widely used since their sparse solutions naturally identify a set of informative features. However, Lasso performs erratically when features are correlated. This limits the use of such algorithms in biological problems, where features such as genes often work together in pathways, leading to sets of highly correlated features. In this paper, we examine the performance of a Lasso derivative, the exclusive group Lasso, in this setting. We propose fast algorithms to solve the exclusive group Lasso, and introduce a solution to the case when the underlying group structure is unknown. The solution combines stability selection with random group allocation and introduction of artificial features. Experiments with both synthetic and real-world data highlight the advantages of this proposed methodology over Lasso in comprehensive selection of informative features.", "venue": "ArXiv", "authors": ["Yuxin  Sun", "Benny  Chain", "Samuel  Kaski", "John  Shawe-Taylor"], "year": 2020, "n_citations": 3}
{"id": 957202, "s2_id": "8f2386acebdcc7ff4363a1c83e324f1142030c78", "title": "End-to-End Spoken Language Understanding Without Full Transcripts", "abstract": "An essential component of spoken language understanding (SLU) is slot filling: representing the meaning of a spoken utterance using semantic entity labels. In this paper, we develop end-to-end (E2E) spoken language understanding systems that directly convert speech input to semantic entities and investigate if these E2E SLU models can be trained solely on semantic entity annotations without word-for-word transcripts. Training such models is very useful as they can drastically reduce the cost of data collection. We created two types of such speech-to-entities models, a CTC model and an attention-based encoder-decoder model, by adapting models trained originally for speech recognition. Given that our experiments involve speech input, these systems need to recognize both the entity label and words representing the entity value correctly. For our speech-to-entities experiments on the ATIS corpus, both the CTC and attention models showed impressive ability to skip non-entity words: there was little degradation when trained on just entities versus full transcripts. We also explored the scenario where the entities are in an order not necessarily related to spoken order in the utterance. With its ability to do re-ordering, the attention model did remarkably well, achieving only about 2% degradation in speech-to-bag-of-entities F1 score.", "venue": "INTERSPEECH", "authors": ["Hong-Kwang J. Kuo", "Zolt'an  Tuske", "Samuel  Thomas", "Yinghui  Huang", "Kartik  Audhkhasi", "Brian  Kingsbury", "Gakuto  Kurata", "Zvi  Kons", "Ron  Hoory", "Luis  Lastras"], "year": 2020, "n_citations": 13}
{"id": 960462, "s2_id": "9be11141d461505f89d911cc4d092c25b4972a28", "title": "Efficient Optimal Selection for Composited Advertising Creatives with Tree Structure", "abstract": "Ad creatives are one of the prominent mediums for online e-commerce advertisements. Ad creatives with enjoyable visual appearance may increase the click-through rate (CTR) of products. Ad creatives are typically handcrafted by advertisers and then delivered to the advertising platforms for advertisement. In recent years, advertising platforms are capable of instantly compositing ad creatives with arbitrarily designated elements of each ingredient, so advertisers are only required to provide basic materials. While facilitating the advertisers, a great number of potential ad creatives can be composited, making it difficult to accurately estimate CTR for them given limited real-time feedback. To this end, we propose an Adaptive and Efficient ad creative Selection (AES) framework based on a tree structure. The tree structure on compositing ingredients enables dynamic programming for efficient ad creative selection on the basis of CTR. Due to limited feedback, the CTR estimator is usually of high variance. Exploration techniques based on Thompson sampling are widely used for reducing variances of the CTR estimator, alleviating feedback sparsity. Based on the tree structure, Thompson sampling is adapted with dynamic programming, leading to efficient exploration for potential ad creatives with the largest CTR. We finally evaluate the proposed algorithm on the synthetic dataset and the real-world dataset. The results show that our approach can outperform competing baselines in terms of convergence rate and overall CTR.", "venue": "AAAI", "authors": ["Jin  Chen", "Tiezheng  Ge", "Gangwei  Jiang", "Zhiqiang  Zhang", "Defu  Lian", "Kai  Zheng"], "year": 2021, "n_citations": 0}
{"id": 980998, "s2_id": "3cacac57376ad7c32df9e50ddc39cd20a3668cc1", "title": "Crush Optimism with Pessimism: Structured Bandits Beyond Asymptotic Optimality", "abstract": "We study stochastic structured bandits for minimizing regret. The fact that the popular optimistic algorithms do not achieve the asymptotic instance-dependent regret optimality (asymptotic optimality for short) has recently allured researchers. On the other hand, it is known that one can achieve a bounded regret (i.e., does not grow indefinitely with $n$) in certain instances. Unfortunately, existing asymptotically optimal algorithms rely on forced sampling that introduces an $\\omega(1)$ term w.r.t. the time horizon $n$ in their regret, failing to adapt to the ``easiness'' of the instance. In this paper, we focus on the finite hypothesis class and ask if one can achieve the asymptotic optimality while enjoying bounded regret whenever possible. We provide a positive answer by introducing a new algorithm called CRush Optimism with Pessimism (CROP) that eliminates optimistic hypotheses by pulling the informative arms indicated by a pessimistic hypothesis. Our finite-time analysis shows that CROP $(i)$ achieves a constant-factor asymptotic optimality and, thanks to the forced-exploration-free design, $(ii)$ adapts to bounded regret, and $(iii)$ its regret bound scales not with the number of arms $K$ but with an effective number of arms $K_\\psi$ that we introduce. We also discuss a problem class where CROP can be exponentially better than existing algorithms in \\textit{nonasymptotic} regimes. Finally, we observe that even a clairvoyant oracle who plays according to the asymptotically optimal arm pull scheme may suffer a linear worst-case regret, indicating that it may not be the end of optimism.", "venue": "NeurIPS", "authors": ["Kwang-Sung  Jun", "Chicheng  Zhang"], "year": 2020, "n_citations": 4}
{"id": 1013164, "s2_id": "b8b0e4270e10f3d185fcd20c9d1b2673a6ace96e", "title": "Momentum Improves Normalized SGD", "abstract": "We provide an improved analysis of normalized SGD showing that adding momentum provably removes the need for large batch sizes on non-convex objectives. Then, we consider the case of objectives with bounded second derivative and show that in this case a small tweak to the momentum formula allows normalized SGD with momentum to find an $\\epsilon$-critical point in $O(1/\\epsilon^{3.5})$ iterations, matching the best-known rates without accruing any logarithmic factors or dependence on dimension. We also provide an adaptive method that automatically improves convergence rates when the variance in the gradients is small. Finally, we show that our method is effective when employed on popular large scale tasks such as ResNet-50 and BERT pretraining, matching the performance of the disparate methods used to get state-of-the-art results on both tasks.", "venue": "ICML", "authors": ["Ashok  Cutkosky", "Harsh  Mehta"], "year": 2020, "n_citations": 24}
{"id": 1019013, "s2_id": "f8516330bec050967379dcbe0983dd4fd6ae2a77", "title": "Generalized Value Iteration Networks: Life Beyond Lattices", "abstract": "In this paper, we introduce a generalized value iteration network (GVIN), which is an end-to-end neural network planning module. GVIN emulates the value iteration algorithm by using a novel graph convolution operator, which enables GVIN to learn and plan on irregular spatial graphs. We propose three novel differentiable kernels as graph convolution operators and show that the embedding based kernel achieves the best performance. We further propose episodic Q-learning, an improvement upon traditional n-step Q-learning that stabilizes training for networks that contain a planning module. Lastly, we evaluate GVIN on planning problems in 2D mazes, irregular graphs, and real-world street networks, showing that GVIN generalizes well for both arbitrary graphs and unseen graphs of larger scale and outperforms a naive generalization of VIN (discretizing a spatial graph into a 2D image).", "venue": "AAAI", "authors": ["Sufeng  Niu", "Siheng  Chen", "Hanyu  Guo", "Colin  Targonski", "Melissa C. Smith", "Jelena  Kovacevic"], "year": 2018, "n_citations": 34}
{"id": 1036819, "s2_id": "e517f30bf08c4a2432efbecc42376f474f1e0199", "title": "Universal Learning Theory", "abstract": "This encyclopedic article gives a mini-introduction into the theory of universal learning, founded by Ray Solomonoff in the 1960s and significantly developed and extended in the last decade. It explains the spirit of universal learning, but necessarily glosses over technical subtleties.", "venue": "Encyclopedia of Machine Learning", "authors": ["Marcus  Hutter"], "year": 2010, "n_citations": 5}
{"id": 1040869, "s2_id": "7af648e0985ec7b68877e676582d9dd9ab845693", "title": "Model-based classification and novelty detection for point pattern data", "abstract": "Point patterns are sets or multi-sets of unordered elements that can be found in numerous data sources. However, in data analysis tasks such as classification and novelty detection, appropriate statistical models for point pattern data have not received much attention. This paper proposes the modelling of point pattern data via random finite sets (RFS). In particular, we propose appropriate likelihood functions, and a maximum likelihood estimator for learning a tractable family of RFS models. In novelty detection, we propose novel ranking functions based on RFS models, which substantially improve performance.", "venue": "2016 23rd International Conference on Pattern Recognition (ICPR)", "authors": ["Ba-Ngu  Vo", "Nhat-Quang  Tran", "Dinh Q. Phung", "Ba-Tuong  Vo"], "year": 2016, "n_citations": 12}
{"id": 1044871, "s2_id": "ce8a96637709c8baaed30ccc7329a2e715d9c1ea", "title": "Deep Contextual Embeddings for Address Classification in E-commerce", "abstract": "E-commerce customers in developing nations like India tend to follow no fixed format while entering shipping addresses. Parsing such addresses is challenging because of a lack of inherent structure or hierarchy. It is imperative to understand the language of addresses, so that shipments can be routed without delays. In this paper, we propose a novel approach towards understanding customer addresses by deriving motivation from recent advances in Natural Language Processing (NLP). We also formulate different pre-processing steps for addresses using a combination of edit distance and phonetic algorithms. Then we approach the task of creating vector representations for addresses using Word2Vec with TF-IDF, Bi-LSTM and BERT based approaches. We compare these approaches with respect to sub-region classification task for North and South Indian cities. Through experiments, we demonstrate the effectiveness of generalized RoBERTa model, pre-trained over a large address corpus for language modelling task. Our proposed RoBERTa model achieves a classification accuracy of around 90% with minimal text preprocessing for sub-region classification task outperforming all other approaches. Once pre-trained, the RoBERTa model can be fine-tuned for various downstream tasks in supply chain like pincode suggestion and geo-coding. The model generalizes well for such tasks even with limited labelled data. To the best of our knowledge, this is the first of its kind research proposing a novel approach of understanding customer addresses in e-commerce domain by pre-training language models and fine-tuning them for different purposes.", "venue": "ArXiv", "authors": ["Shreyas  Mangalgi", "Lakshya  Kumar", "Ravindra Babu Tallamraju"], "year": 2020, "n_citations": 1}
{"id": 1050725, "s2_id": "9356db38a4786bc4ac3d6fd57ac5db7dc1b134d2", "title": "Consensus Neural Network for Medical Imaging Denoising with Only Noisy Training Samples", "abstract": "Deep neural networks have been proved efficient for medical image denoising. Current training methods require both noisy and clean images. However, clean images cannot be acquired for many practical medical applications due to naturally noisy signal, such as dynamic imaging, spectral computed tomography, arterial spin labeling magnetic resonance imaging, etc. In this paper we proposed a training method which learned denoising neural networks from noisy training samples only. Training data in the acquisition domain was split to two subsets and the network was trained to map one noisy set to the other. A consensus loss function was further proposed to efficiently combine the outputs from both subsets. A mathematical proof was provided that the proposed training scheme was equivalent to training with noisy and clean samples when the noise in the two subsets was uncorrelated and zero-mean. The method was validated on Low-dose CT Challenge dataset and NYU MRI dataset and achieved improved performance compared to existing unsupervised methods.", "venue": "MICCAI", "authors": ["Dufan  Wu", "Kuang  Gong", "Kyungsang  Kim", "Quanzheng  Li"], "year": 2019, "n_citations": 20}
{"id": 1102060, "s2_id": "a29ecff7f2e11f80a9ece1faa644389549f92798", "title": "Asymmetric Heavy Tails and Implicit Bias in Gaussian Noise Injections", "abstract": "Gaussian noise injections (GNIs) are a family of simple and widely-used regularisation methods for training neural networks where one injects additive or multiplicative Gaussian noise to the network activations at every iteration of the optimisation algorithm, which is typically chosen as stochastic gradient descent (SGD). In this paper we focus on the so-called \u2018implicit effect\u2019 of GNIs, which is the effect of the injected noise on the dynamics of SGD. We show that this effect induces an asymmetric heavy-tailed noise on SGD gradient updates. In order to model this modified dynamics, we first develop a Langevin-like stochastic differential equation that is driven by a general family of asymmetric heavy-tailed noise. Using this model we then formally prove that GNIs induce an \u2018implicit bias\u2019, which varies depending on the heaviness of the tails and the level of asymmetry. Our empirical results confirm that different types of neural networks trained with GNIs are well-modelled by the proposed dynamics and that the implicit effect of these injections induces a bias that degrades the performance of networks.", "venue": "ICML", "authors": ["Alexander  Camuto", "Xiaoyu  Wang", "Lingjiong  Zhu", "Chris C. Holmes", "Mert  G\u00fcrb\u00fczbalaban", "Umut  Simsekli"], "year": 2021, "n_citations": 3}
{"id": 1102947, "s2_id": "343e37767d67a86aa936bfc6a7c7c857ce8796a3", "title": "Tilting at windmills: Data augmentation for deep pose estimation does not help with occlusions", "abstract": "Occlusion degrades the performance of human pose estimation. In this paper, we introduce targeted keypoint and body part occlusion attacks. The effects of the attacks are systematically analyzed on the best performing methods. In addition, we propose occlusion specific data augmentation techniques against keypoint and part attacks. Our extensive experiments show that human pose estimation methods are not robust to occlusion and data augmentation does not solve the occlusion problems.11For the code and the extended version: https://github.com/rpytell/ocdusion-vs-data-augmentations", "venue": "2020 25th International Conference on Pattern Recognition (ICPR)", "authors": ["Rafal  Pytel", "Osman Semih Kayhan", "Jan C. van Gemert"], "year": 2021, "n_citations": 2}
{"id": 1144340, "s2_id": "58dba823fc1d5f5d45c6fb7414aba7fed7011fb8", "title": "A Winnow-Based Approach to Context-Sensitive Spelling Correction", "abstract": "A large class of machine-learning problems in natural language require the characterization of linguistic context. Two characteristic properties of such problems are that their feature space is of very high dimensionality, and their target concepts depend on only a small subset of the features in the space. Under such conditions, multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good theoretical properties. In the work reported here, we present an algorithm combining variants of Winnow and weighted-majority voting, and apply it to a problem in the aforementioned class: context-sensitive spelling correction. This is the task of fixing spelling errors that happen to result in valid words, such as substituting to for too, casual for causal, and so on. We evaluate our algorithm, WinSpell, by comparing it against BaySpell, a statistics-based method representing the state of the art for this task. We find: (1) When run with a full (unpruned) set of features, WinSpell achieves accuracies significantly higher than BaySpell was able to achieve in either the pruned or unpruned condition; (2) When compared with other systems in the literature, WinSpell exhibits the highest performance; (3) While several aspects of WinSpell's architecture contribute to its superiority over BaySpell, the primary factor is that it is able to learn a better linear separator than BaySpell learns; (4) When run on a test set drawn from a different corpus than the training set was drawn from, WinSpell is better able than BaySpell to adapt, using a strategy we will present that combines supervised learning on the training set with unsupervised learning on the (noisy) test set.", "venue": "Machine Learning", "authors": ["Andrew R. Golding", "Dan  Roth"], "year": 2004, "n_citations": 297}
{"id": 1146215, "s2_id": "318459b0348d7985c77b7c0ca7b8d5b01a950eee", "title": "Dreaming machine learning: Lipschitz extensions for reinforcement learning on financial markets", "abstract": "We consider a quasi-metric topological structure for the construction of a new reinforcement learning model in the framework of financial markets. It is based on a Lipschitz type extension of reward functions defined in metric spaces. Specifically, the McShane and Whitney extensions are considered for a reward function which is defined by the total evaluation of the benefits produced by the investment decision at a given time. We define the metric as a linear combination of a Euclidean distance and an angular metric component. All information about the evolution of the system from the beginning of the time interval is used to support the extension of the reward function, but in addition this data set is enriched by adding some artificially produced states. Thus, the main novelty of our method is the way we produce more states -- which we call \"dreams\" -- to enrich learning. Using some known states of the dynamical system that represents the evolution of the financial market, we use our technique to simulate new states by interpolating real states and introducing some random variables. These new states are used to feed a learning algorithm designed to improve the investment strategy by following a typical reinforcement learning scheme.", "venue": "Neurocomputing", "authors": ["J. M. Calabuig", "H.  Falciani", "Enrique Alfonso S\u00e1nchez-P\u00e9rez"], "year": 2020, "n_citations": 8}
{"id": 1162262, "s2_id": "6c6be6fc4c874579dd331dd37c3a1da42f3dc868", "title": "Hey Alexa what did I just type? Decoding smartphone sounds with a voice assistant", "abstract": "Voice assistants are now ubiquitous and listen in on our everyday lives. Ever since they became commercially available, privacy advocates worried that the data they collect can be abused: might private conversations be extracted by third parties? In this paper we show that privacy threats go beyond spoken conversations and include sensitive data typed on nearby smartphones. Using two different smartphones and a tablet we demonstrate that the attacker can extract PIN codes and text messages from recordings collected by a voice assistant located up to half a meter away. This shows that remote keyboard-inference attacks are not limited to physical keyboards but extend to virtual keyboards too. As our homes become full of always-on microphones, we need to work through the implications.", "venue": "ArXiv", "authors": ["Almos  Zarandy", "Ilia  Shumailov", "Ross  Anderson"], "year": 2020, "n_citations": 0}
{"id": 1217946, "s2_id": "e6988260ed82de57dd28eb357547e12d41c55c43", "title": "Examining the Benefits of Capsule Neural Networks", "abstract": "Capsule networks are a recently developed class of neural networks that potentially address some of the deficiencies with traditional convolutional neural networks. By replacing the standard scalar activations with vectors, and by connecting the artificial neurons in a new way, capsule networks aim to be the next great development for computer vision applications. However, in order to determine whether these networks truly operate differently than traditional networks, one must look at the differences in the capsule features. To this end, we perform several analyses with the purpose of elucidating capsule features and determining whether they perform as described in the initial publication. First, we perform a deep visualization analysis to visually compare capsule features and convolutional neural network features. Then, we look at the ability for capsule features to encode information across the vector components and address what changes in the capsule architecture provides the most benefit. Finally, we look at how well the capsule features are able to encode instantiation parameters of class objects via visual transformations.", "venue": "ArXiv", "authors": ["Arjun  Punjabi", "Jonas  Schmid", "Aggelos K. Katsaggelos"], "year": 2020, "n_citations": 3}
{"id": 1322943, "s2_id": "dcc400915414f3889b386795a109de5b37948de1", "title": "Large-Scale Intelligent Microservices", "abstract": "Deploying Machine Learning (ML) algorithms within databases is a challenge due to the varied computational footprints of modern ML algorithms and the myriad of database technologies each with their own restrictive syntax. We introduce an Apache Spark-based micro-service orchestration framework that extends database operations to include web service primitives. Our system can orchestrate web services across hundreds of machines and takes full advantage of cluster, thread, and asynchronous parallelism. Using this framework, we provide large scale clients for intelligent services such as speech, vision, search, anomaly detection, and text analysis. This allows users to integrate ready-to-use intelligence into any datastore with an Apache Spark connector. To eliminate the majority of overhead from network communication, we also introduce a low-latency containerized version of our architecture. Finally, we demonstrate that the services we investigate are competitive on a variety of benchmarks, and present two applications of this framework to create intelligent search engines, and real time auto race analytics systems.", "venue": "2020 IEEE International Conference on Big Data (Big Data)", "authors": ["Mark  Hamilton", "Nick  Gonsalves", "Christina  Lee", "Anand  Raman", "Brendan  Walsh", "Siddhartha  Prasad", "Dalitso  Banda", "Lucy  Zhang", "Lei  Zhang", "William T. Freeman"], "year": 2020, "n_citations": 1}
{"id": 1325095, "s2_id": "ad0064a1f3d5e2b86a904fe44d2c17680870e2e9", "title": "Learning Segmentation Masks with the Independence Prior", "abstract": "An instance with a bad mask might make a composite image that uses it look fake. This encourages us to learn segmentation by generating realistic composite images. To achieve this, we propose a novel framework that exploits a new proposed prior called the independence prior based on Generative Adversarial Networks (GANs). The generator produces an image with multiple category-specific instance providers, a layout module and a composition module. Firstly, each provider independently outputs a category-specific instance image with a soft mask. Then the provided instances\u2019 poses are corrected by the layout module. Lastly, the composition module combines these instances into a final image. Training with adversarial loss and penalty for mask area, each provider learns a mask that is as small as possible but enough to cover a complete category-specific instance. Weakly supervised semantic segmentation methods widely use grouping cues modeling the association between image parts, which are either artificially designed or learned with costly segmentation labels or only modeled on local pairs. Unlike them, our method automatically models the dependence between any parts and learns instance segmentation. We apply our framework in two cases: (1) Foreground segmentation on category-specific images with box-level annotation. (2) Unsupervised learning of instance appearances and masks with only one image of homogeneous object cluster (HOC). We get appealing results in both tasks, which shows the independence prior is useful for instance segmentation and it is possible to unsupervisedly learn instance masks with only one image.", "venue": "AAAI", "authors": ["Songmin  Dai", "Xiaoqiang  Li", "Lu  Wang", "Pin  Wu", "Weiqin  Tong", "Yimin  Chen"], "year": 2019, "n_citations": 2}
{"id": 1368423, "s2_id": "de9674178175460dba788469c694d700276fa96d", "title": "Modularity in Query-Based Concept Learning", "abstract": "We define and study the problem of modular concept learning, that is, learning a concept that is a cross product of component concepts. If an element's membership in a concept depends solely on it's membership in the components, learning the concept as a whole can be reduced to learning the components. We analyze this problem with respect to different types of oracle interfaces, defining different sets of queries. If a given oracle interface cannot answer questions about the components, learning can be difficult, even when the components are easy to learn with the same type of oracle queries. While learning from superset queries is easy, learning from membership, equivalence, or subset queries is harder. However, we show that these problems become tractable when oracles are given a positive example and are allowed to ask membership queries.", "venue": "ArXiv", "authors": ["Benjamin  Caulfield", "Sanjit A. Seshia"], "year": 2019, "n_citations": 0}
{"id": 1369628, "s2_id": "c66373aaf7188c84b517faca1e0428803f0aa508", "title": "Against All Odds: Winning the Defense Challenge in an Evasion Competition with Diversification", "abstract": "Machine learning-based systems for malware detection operate in a hostile environment. Consequently, adversaries will also target the learning system and use evasion attacks to bypass the detection of malware. In this paper, we outline our learning-based system PEberus that got the first place in the defender challenge of the Microsoft Evasion Competition, resisting a variety of attacks from independent attackers. Our system combines multiple, diverse defenses: we address the semantic gap, use various classification models, and apply a stateful defense. This competition gives us the unique opportunity to examine evasion attacks under a realistic scenario. It also highlights that existing machine learning methods can be hardened against attacks by thoroughly analyzing the attack surface and implementing concepts from adversarial learning. Our defense can serve as an additional baseline in the future to strengthen the research on secure learning.", "venue": "ArXiv", "authors": ["Erwin  Quiring", "Lukas  Pirch", "Michael  Reimsbach", "Daniel  Arp", "Konrad  Rieck"], "year": 2020, "n_citations": 3}
{"id": 1374997, "s2_id": "c801915fadd79b3807462621511ba4aaab19c89d", "title": "Neural Subgraph Matching", "abstract": "Subgraph matching is the problem of determining the presence and location(s) of a given query graph in a large target graph. Despite being an NP-complete problem, the subgraph matching problem is crucial in domains ranging from network science and database systems to biochemistry and cognitive science. However, existing techniques based on combinatorial matching and integer programming cannot handle matching problems with both large target and query graphs. Here we propose NeuroMatch, an accurate, efficient, and robust neural approach to subgraph matching. NeuroMatch decomposes query and target graphs into small subgraphs and embeds them using graph neural networks. Trained to capture geometric constraints corresponding to subgraph relations, NeuroMatch then efficiently performs subgraph matching directly in the embedding space. Experiments demonstrate NeuroMatch is 100x faster than existing combinatorial approaches and 18% more accurate than existing approximate subgraph matching methods.", "venue": "ArXiv", "authors": ["Rex  Ying", "Zhaoyu  Lou", "Jiaxuan  You", "Chengtao  Wen", "Arquimedes  Canedo", "Jure  Leskovec"], "year": 2020, "n_citations": 8}
{"id": 1383175, "s2_id": "499a70c1b2d5cc332ef5a7f3a5bc4bfeaf37691a", "title": "An evaluation of randomized machine learning methods for redundant data: Predicting short and medium-term suicide risk from administrative records and risk assessments", "abstract": "Accurate prediction of suicide risk in mental health patients remains an open problem. Existing methods including clinician judgments have acceptable sensitivity, but yield many false positives. Exploiting administrative data has a great potential, but the data has high dimensionality and redundancies in the recording processes. We investigate the efficacy of three most effective randomized machine learning techniques random forests, gradient boosting machines, and deep neural nets with dropout in predicting suicide risk. Using a cohort of mental health patients from a regional Australian hospital, we compare the predictive performance with popular traditional approaches clinician judgments based on a checklist, sparse logistic regression and decision trees. The randomized methods demonstrated robustness against data redundancies and superior predictive performance on AUC and F-measure.", "venue": "ArXiv", "authors": ["Nguyen Cong Thuong", "Truyen  Tran", "Shivapratap  Gopakumar", "Dinh Q. Phung", "Svetha  Venkatesh"], "year": 2016, "n_citations": 7}
{"id": 1383369, "s2_id": "318f50c4f1cce7f6338d120d36c43542b229fa46", "title": "HUBERT Untangles BERT to Improve Transfer across NLP Tasks", "abstract": "We introduce HUBERT which combines the structured-representational power of Tensor-Product Representations (TPRs) and BERT, a pre-trained bidirectional Transformer language model. We show that there is shared structure between different NLP datasets that HUBERT, but not BERT, is able to learn and leverage. We validate the effectiveness of our model on the GLUE benchmark and HANS dataset. Our experiment results show that untangling data-specific semantics from general language structure is key for better transfer among NLP tasks.", "venue": "ArXiv", "authors": ["Mehrad  Moradshahi", "Hamid  Palangi", "Monica S. Lam", "Paul  Smolensky", "Jianfeng  Gao"], "year": 2019, "n_citations": 10}
{"id": 1402651, "s2_id": "a34d201fdf37ae3546ab976b47744be7c02a76e0", "title": "Machine Learning Based Network Coverage Guidance System", "abstract": "With the advent of 4G, there has been a huge consumption of data and the availability of mobile networks has become paramount. Also, with the burst of network traffic based on user consumption, data availability and network anomalies have increased substantially. In this paper, we introduce a novel approach, to identify the regions that have poor network connectivity thereby providing feedback to both the service providers to improve the coverage as well as to the customers to choose the network judiciously. In addition to this, the solution enables customers to navigate to a better mobile network coverage area with stronger signal strength location using Machine Learning Clustering Algorithms, whilst deploying it as a Mobile Application. It also provides a dynamic visual representation of varying network strength and range across nearby geographical areas.", "venue": "ArXiv", "authors": ["Srikanth  Chandar", "Muvazima  Mansoor", "Mohina  Ahmadi", "Hrishikesh  Badve", "Deepesh  Sahoo", "Bharath  Katragadda"], "year": 2020, "n_citations": 0}
{"id": 1415157, "s2_id": "4234e230c5d49edc69b880763404b7a58d924a36", "title": "A Framework and Method for Online Inverse Reinforcement Learning", "abstract": "Inverse reinforcement learning (IRL) is the problem of learning the preferences of an agent from the observations of its behavior on a task. While this problem has been well investigated, the related problem of {\\em online} IRL---where the observations are incrementally accrued, yet the demands of the application often prohibit a full rerun of an IRL method---has received relatively less attention. We introduce the first formal framework for online IRL, called incremental IRL (I2RL), and a new method that advances maximum entropy IRL with hidden variables, to this setting. Our formal analysis shows that the new method has a monotonically improving performance with more demonstration data, as well as probabilistically bounded error, both under full and partial observability. Experiments in a simulated robotic application of penetrating a continuous patrol under occlusion shows the relatively improved performance and speed up of the new method and validates the utility of online IRL.", "venue": "ArXiv", "authors": ["Saurabh  Arora", "Prashant  Doshi", "Bikramjit  Banerjee"], "year": 2018, "n_citations": 1}
{"id": 1417174, "s2_id": "6416f74798aaedb1ea27321a1fd9f42be71a7ac4", "title": "Radiological images and machine learning: trends, perspectives, and prospects", "abstract": "The application of machine learning to radiological images is an increasingly active research area that is expected to grow in the next five to ten years. Recent advances in machine learning have the potential to recognize and classify complex patterns from different radiological imaging modalities such as x-rays, computed tomography, magnetic resonance imaging and positron emission tomography imaging. In many applications, machine learning based systems have shown comparable performance to human decision-making. The applications of machine learning are the key ingredients of future clinical decision making and monitoring systems. This review covers the fundamental concepts behind various machine learning techniques and their applications in several radiological imaging areas, such as medical image segmentation, brain function studies and neurological disease diagnosis, as well as computer-aided systems, image registration, and content-based image retrieval systems. Synchronistically, we will briefly discuss current challenges and future directions regarding the application of machine learning in radiological imaging. By giving insight on how take advantage of machine learning powered applications, we expect that clinicians can prevent and diagnose diseases more accurately and efficiently.", "venue": "Comput. Biol. Medicine", "authors": ["Zhenwei  Zhang", "Ervin  Sejdi\u0107"], "year": 2019, "n_citations": 52}
{"id": 1417215, "s2_id": "7ea159814a4db042b58e9c0123069c2c81b0d781", "title": "An Enhanced Machine Learning Topic Classification Methodology for Cybersecurity", "abstract": "In this research, we use user defined labels from three internet text sources (Reddit, Stackexchange, Arxiv) to train 21 different machine learning models for the topic classification task of detecting cybersecurity discussions in natural text. We analyze the false positive and false negative rates of each of the 21 model\u2019s in a cross validation experiment. Then we present a Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of the 21 trained machine learning models as the decision mechanism for detecting cybersecurity related text. We also show that the majority vote mechanism of the CTC tool provides lower false negative and false positive rates on average than any of the 21 individual models. We show that the CTC tool is scalable to the hundreds of thousands of documents with a wall clock time on the order of hours.", "venue": "Natural Language Processing", "authors": ["Elijah  Pelofske", "Lorie M. Liebrock", "Vincent  Urias"], "year": 2021, "n_citations": 0}
{"id": 1419198, "s2_id": "9db1f913f56ff745099a71065161656631989a72", "title": "Provably Fair Representations", "abstract": "Machine learning systems are increasingly used to make decisions about people's lives, such as whether to give someone a loan or whether to interview someone for a job. This has led to considerable interest in making such machine learning systems fair. One approach is to transform the input data used by the algorithm. This can be achieved by passing each input data point through a representation function prior to its use in training or testing. Techniques for learning such representation functions from data have been successful empirically, but typically lack theoretical fairness guarantees. We show that it is possible to prove that a representation function is fair according to common measures of both group and individual fairness, as well as useful with respect to a target task. These provable properties can be used in a governance model involving a data producer, a data user and a data regulator, where there is a separation of concerns between fairness and target task utility to ensure transparency and prevent perverse incentives. We formally define the 'cost of mistrust' of using this model compared to the setting where there is a single trusted party, and provide bounds on this cost in particular cases. We present a practical approach to learning fair representation functions and apply it to financial and criminal justice datasets. We evaluate the fairness and utility of these representation functions using measures motivated by our theoretical results.", "venue": "ArXiv", "authors": ["Daniel  McNamara", "Cheng Soon Ong", "Robert C. Williamson"], "year": 2017, "n_citations": 29}
{"id": 1429189, "s2_id": "92411d2e88ef29f4995896e95febc807a3dfc0f3", "title": "Generalising Recursive Neural Models by Tensor Decomposition", "abstract": "Most machine learning models for structured data encode the structural knowledge of a node by leveraging simple aggregation functions (in neural models, typically a weighted sum) of the information in the node\u2019s neighbourhood. Nevertheless, the choice of simple context aggregation functions, such as the sum, can be widely sub-optimal. In this work we introduce a general approach to model aggregation of structural context leveraging a tensor-based formulation. We show how the exponential growth in the size of the parameter space can be controlled through an approximation based on the Tucker tensor decomposition. This approximation allows limiting the parameters space size, decoupling it from its strict relation with the size of the hidden encoding space. By this means, we can effectively regulate the trade-off between expressivity of the encoding, controlled by the hidden size, computational complexity and model generalisation, influenced by parameterisation. Finally, we introduce a new Tensorial Tree-LSTM derived as an instance of our framework and we use it to experimentally assess our working hypotheses on tree classification scenarios.", "venue": "2020 International Joint Conference on Neural Networks (IJCNN)", "authors": ["Daniele  Castellana", "Davide  Bacciu"], "year": 2020, "n_citations": 4}
{"id": 1431411, "s2_id": "5c2a691c2040d219c5ccf13fdec6307e5bbbc1b7", "title": "An Underparametrized Deep Decoder Architecture for Graph Signals", "abstract": "While deep convolutional architectures have achieved remarkable results in a gamut of supervised applications dealing with images and speech, recent works show that deep untrained non-convolutional architectures can also outperform state-of-the-art methods in several tasks such as image compression and denoising. Motivated by the fact that many contemporary datasets have an irregular structure different from a 1D/2D grid, this paper generalizes untrained and underparametrized non-convolutional architectures to signals defined over irregular domains represented by graphs. The proposed architecture consists of a succession of layers, each of them implementing an upsampling operator, a linear feature combination, and a scalar nonlinearity. A novel element is the incorporation of upsampling operators accounting for the structure of the supporting graph, which is achieved by considering a systematic graph coarsening approach based on hierarchical clustering. The numerical results carried out in synthetic and real-world datasets showcase that the reconstruction performance can improve drastically if the information of the supporting graph topology is taken into account.", "venue": "2019 IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)", "authors": ["Samuel  Rey", "Antonio G. Marqu\u00e9s", "Santiago  Segarra"], "year": 2019, "n_citations": 7}
{"id": 1448695, "s2_id": "c6ff87e9ca60a9ef2cbb89f5a40afc2f6d7d055f", "title": "Do Proportionate Algorithms Exploit Sparsity?", "abstract": "Adaptive filters exploiting sparsity have been a very active research field, among which the algorithms that follow the \u201cproportional updates\u201d principle, the so-called proportionatetype algorithms, are very popular. Indeed, there are hundreds of works on proportionate-type algorithms and, therefore, their advantages are widely known. This paper addresses the unexplored drawbacks and limitations of using proportional updates and their practical impacts. Our findings include the theoretical justification for the poor performance of these algorithms in several sparse scenarios, and also when dealing with non-stationary and compressible systems. Simulation results corroborating the theory are presented.", "venue": "ArXiv", "authors": ["Markus V. S. Lima", "Gabriel S. Chaves", "Tadeu N. Ferreira", "Paulo S. R. Diniz"], "year": 2021, "n_citations": 0}
{"id": 1467418, "s2_id": "75e32e6d8813ae27a6bd353bba6e9d64e9c0f288", "title": "Leveraging Local Domains for Image-to-Image Translation", "abstract": "Image-to-image (i2i) networks struggle to capture local changes because they do not affect the global scene structure. For example, translating from highway scenes to offroad, i2i networks easily focus on global color features but ignore obvious traits for humans like the absence of lane markings. In this paper, we leverage human knowledge about spatial domain characteristics which we refer to as \u2019local domains\u2019 and demonstrate its benefit for image-to-image translation. Relying on a simple geometrical guidance, we train a patch-based GAN on few source data and hallucinate a new unseen domain which subsequently eases transfer learning to target. We experiment on three tasks ranging from unstructured environments to adverse weather. Our comprehensive evaluation setting shows we are able to generate realistic translations, with minimal priors, and training only on a few images. Furthermore, when trained on our translations images we show that all tested proxy tasks are significantly improved, without ever seeing target domain at training.", "venue": "ArXiv", "authors": ["Anthony  Dell'Eva", "Fabio  Pizzati", "Massimo  Bertozzi", "Raoul de Charette"], "year": 2021, "n_citations": 1}
{"id": 1471242, "s2_id": "f2d1688c52d37de7b7b531b93ceaa3a07b704d07", "title": "DRIVE: One-bit Distributed Mean Estimation", "abstract": "We consider the problem where n clients transmit d-dimensional real-valued vectors using dp1 ` op1qq bits each, in a manner that allows the receiver to approximately reconstruct their mean. Such compression problems naturally arise in distributed and federated learning. We provide novel mathematical results and derive computationally efficient algorithms that are more accurate than previous compression techniques. We evaluate our methods on a collection of distributed and federated learning tasks, using a variety of datasets, and show a consistent improvement over the state of the art.", "venue": "ArXiv", "authors": ["Shay  Vargaftik", "Ran Ben Basat", "Amit  Portnoy", "Gal  Mendelson", "Yaniv  Ben-Itzhak", "Michael  Mitzenmacher"], "year": 2021, "n_citations": 2}
{"id": 1528211, "s2_id": "c142b8509c24951a609000332e78884a60183e7c", "title": "One-Pass Multi-Task Networks With Cross-Task Guided Attention for Brain Tumor Segmentation", "abstract": "Class imbalance has emerged as one of the major challenges for medical image segmentation. The model cascade (MC) strategy, a popular scheme, significantly alleviates the class imbalance issue via running a set of individual deep models for coarse-to-fine segmentation. Despite its outstanding performance, however, this method leads to undesired system complexity and also ignores the correlation among the models. To handle these flaws in the MC approach, we propose in this paper a light-weight deep model, i.e., the One-pass Multi-task Network (OM-Net) to solve class imbalance better than MC does, while requiring only one-pass computation for brain tumor segmentation. First, OM-Net integrates the separate segmentation tasks into one deep model, which consists of shared parameters to learn joint features, as well as task-specific parameters to learn discriminative features. Second, to more effectively optimize OM-Net, we take advantage of the correlation among tasks to design both an online training data transfer strategy and a curriculum learning-based training strategy. Third, we further propose sharing prediction results between tasks, which enables us to design a cross-task guided attention (CGA) module. By following the guidance of the prediction results provided by the previous task, CGA can adaptively recalibrate channel-wise feature responses based on the category-specific statistics. Finally, a simple yet effective post-processing method is introduced to refine the segmentation results of the proposed attention network. Extensive experiments are conducted to demonstrate the effectiveness of the proposed techniques. Most impressively, we achieve state-of-the-art performance on the BraTS 2015 testing set and BraTS 2017 online validation set. Using these proposed approaches, we also won joint third place in the BraTS 2018 challenge among 64 participating teams. The code is publicly available at https://github.com/chenhong-zhou/OM-Net.", "venue": "IEEE Transactions on Image Processing", "authors": ["Chenhong  Zhou", "Changxing  Ding", "Xinchao  Wang", "Zhentai  Lu", "Dacheng  Tao"], "year": 2020, "n_citations": 39}
{"id": 1528452, "s2_id": "2b69e0cb7de14b82a8cab1289ee7eff8e4802dab", "title": "Near-Optimal No-Regret Learning in General Games", "abstract": "We show that Optimistic Hedge \u2013 a common variant of multiplicative-weights-updates with recency bias \u2013 attains poly(log T ) regret in multi-player general-sum games. In particular, when every player of the game uses Optimistic Hedge to iteratively update her strategy in response to the history of play so far, then after T rounds of interaction, each player experiences total regret that is poly(log T ). Our bound improves, exponentially, the O(T ) regret attainable by standard no-regret learners in games, the O(T ) regret attainable by no-regret learners with recency bias [SALS15], and the O(T ) bound that was recently shown for Optimistic Hedge in the special case of two-player games [CP20]. A corollary of our bound is that Optimistic Hedge converges to coarse correlated equilibrium in general games at a rate of \u00d5 ( 1 T ) .", "venue": "ArXiv", "authors": ["Constantinos  Daskalakis", "Maxwell  Fishelson", "Noah  Golowich"], "year": 2021, "n_citations": 4}
{"id": 1558425, "s2_id": "3c5eeecba3589dd5decb3f57b037218a15a078e7", "title": "DAC: Deep Autoencoder-based Clustering, a General Deep Learning Framework of Representation Learning", "abstract": "Clustering performs an essential role in many real world applications, such as market research, pattern recognition, data analysis, and image processing. However, due to the high dimensionality of the input feature values, the data being fed to clustering algorithms usually contains noise and thus could lead to in-accurate clustering results. While traditional dimension reduction and feature selection algorithms could be used to address this problem, the simple heuristic rules used in those algorithms are based on some particular assumptions. When those assumptions does not hold, these algorithms then might not work. In this paper, we propose DAC, Deep Autoencoder-based Clustering, a generalized datadriven framework to learn clustering representations using deep neuron networks. Experiment results show that our approach could effectively boost performance of the K-Means clustering algorithm on a variety types of datasets.", "venue": "IntelliSys", "authors": ["Si  Lu", "Ruisi  Li"], "year": 2021, "n_citations": 0}
{"id": 1581838, "s2_id": "b62edbf6e619eeed886c63e51fdff2c3d94f998f", "title": "Graph convolutions that can finally model local structure", "abstract": "Despite quick progress in the last few years, recent studies have shown that modern graph neural networks can still fail at simple tasks, such as detecting small cycles (Loukas, 2019; Chen et al., 2020). This suggests that current networks fail to catch information about the local structure, which is problematic if the downstream task heavily relies on graph substructure analysis, as in the context of chemistry. We propose a straightforward correction to the now standard GIN convolution (Xu et al., 2019) that enables the network to detect small cycles with nearly no cost in computation time and number of parameters. Tested on real life molecule property datasets, our model consistently improves performance on large multitasked datasets over all baselines, both globally and on a per-task setting.", "venue": "ArXiv", "authors": ["R'emy  Brossard", "Oriel  Frigo", "David  Dehaene"], "year": 2020, "n_citations": 8}
{"id": 1614753, "s2_id": "c3f329c528887c082ec0b51ea3815371c70f99ac", "title": "CPAS: the UK\u2019s national machine learning-based hospital capacity planning system for COVID-19", "abstract": "The coronavirus disease 2019 (COVID-19) global pandemic poses the threat of overwhelming healthcare systems with unprecedented demands for intensive care resources. Managing these demands cannot be effectively conducted\u00a0without a nationwide collective effort that relies on data to forecast hospital demands on the national, regional, hospital and individual levels. To this end, we developed the COVID-19 Capacity Planning and Analysis System (CPAS)\u2014a machine learning-based system for hospital resource planning that we have successfully deployed at individual hospitals and across regions in the UK in coordination with NHS Digital. In this paper, we discuss the main challenges of deploying a machine learning-based decision support system at national scale, and explain how CPAS addresses these challenges by (1) defining the appropriate learning problem,\u00a0(2) combining bottom-up and top-down analytical approaches, (3) using state-of-the-art machine learning algorithms, (4) integrating heterogeneous data sources, and (5) presenting the result with an interactive and transparent interface. CPAS is one of the first machine learning-based systems to be deployed in hospitals on a national scale to address the COVID-19 pandemic\u2014we conclude the paper with a summary of the lessons learned from this experience.", "venue": "Mach. Learn.", "authors": ["Zhaozhi  Qian", "Ahmed M. Alaa", "Mihaela van der Schaar"], "year": 2021, "n_citations": 12}
{"id": 1620982, "s2_id": "a6ddfbad5067496b941cfcf857637cdb2c2b18ac", "title": "Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning", "abstract": "Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as \"black boxes\" in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. Massif's tightly coupled views help people better understand which input features are most vulnerable and important for correct predictions.", "venue": "CHI Extended Abstracts", "authors": ["Nilaksh  Das", "Haekyu  Park", "Zijie J. Wang", "Fred  Hohman", "Robert  Firstman", "Emily  Rogers", "Duen Horng Chau"], "year": 2020, "n_citations": 4}
{"id": 1623286, "s2_id": "a8db3f25c563d84615ccb128d330a506c65dfc5a", "title": "Nonparametric Estimation of Multi-View Latent Variable Models", "abstract": "Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixtures of discrete or Gaussian distributions. In this paper, we propose a kernel method for learning multi-view latent variable models, allowing each mixture component to be nonparametric. The key idea of the method is to embed the joint distribution of a multi-view latent variable into a reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. We establish that the sample complexity for the proposed method is quadratic in the number of latent components and is a low order polynomial in the other relevant parameters. Thus, our non-parametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. Moreover, the non-parametric tensor power method compares favorably to EM algorithm and other existing spectral algorithms in our experiments.", "venue": "ICML", "authors": ["Le  Song", "Anima  Anandkumar", "Bo  Dai", "Bo  Xie"], "year": 2014, "n_citations": 40}
{"id": 1632290, "s2_id": "793513485b1016c62160d68409525824e6aa01d8", "title": "Convexification of Neural Graph", "abstract": "Traditionally, most complex intelligence architectures are extremely non-convex, which could not be well performed by convex optimization. However, this paper decomposes complex structures into three types of nodes: operators, algorithms and functions. Further, iteratively propagating from node to node along edge, we prove that \"regarding the neural graph without triangles, it is nearly convex in each variable, when the other variables are fixed.\" In fact, the non-convex properties stem from triangles and functions, which could be transformed to be convex with our proposed \\textit{\\textbf{convexification inequality}}. In conclusion, we generally depict the landscape for the objective of neural graph and propose the methodology to convexify neural graph.", "venue": "ArXiv", "authors": ["Han  Xiao"], "year": 2018, "n_citations": 0}
{"id": 1662418, "s2_id": "acfaf61783872c5434553c5d174dd8d6afb1d0d6", "title": "On the Relation between Syntactic Divergence and Zero-Shot Performance", "abstract": "We explore the link between the extent to which syntactic relations are preserved in translation and the ease of correctly constructing a parse tree in a zero-shot setting. While previous work suggests such a relation, it tends to focus on the macro level and not on the level of individual edges\u2014a gap we aim to address. As a test case, we take the transfer of Universal Dependencies (UD) parsing from English to a diverse set of languages and conduct two sets of experiments. In one, we analyze zero-shot performance based on the extent to which English source edges are preserved in translation. In another, we apply three linguistically motivated transformations to UD, creating more cross-lingually stable versions of it, and assess their zero-shot parsability. In order to compare parsing performance across different schemes, we perform extrinsic evaluation on the downstream task of cross-lingual relation extraction (RE) using a subset of a popular English RE benchmark translated to Russian and Korean.1 In both sets of experiments, our results suggest a strong relation between cross-lingual stability and zero-shot parsing performance.", "venue": "EMNLP", "authors": ["Ofir  Arviv", "Dmitry  Nikolaev", "Taelin  Karidi", "Omri  Abend"], "year": 2021, "n_citations": 0}
{"id": 1690594, "s2_id": "eebd56ffe46851b147be4b498c5be1e1e5247948", "title": "Empirical Risk Minimization under Random Censorship: Theory and Practice", "abstract": "We consider the classic supervised learning problem, where a continuous non-negative random label $Y$ (i.e. a random duration) is to be predicted based upon observing a random vector $X$ valued in $\\mathbb{R}^d$ with $d\\geq 1$ by means of a regression rule with minimum least square error. In various applications, ranging from industrial quality control to public health through credit risk analysis for instance, training observations can be right censored, meaning that, rather than on independent copies of $(X,Y)$, statistical learning relies on a collection of $n\\geq 1$ independent realizations of the triplet $(X, \\; \\min\\{Y,\\; C\\},\\; \\delta)$, where $C$ is a nonnegative r.v. with unknown distribution, modeling censorship and $\\delta=\\mathbb{I}\\{Y\\leq C\\}$ indicates whether the duration is right censored or not. As ignoring censorship in the risk computation may clearly lead to a severe underestimation of the target duration and jeopardize prediction, we propose to consider a plug-in estimate of the true risk based on a Kaplan-Meier estimator of the conditional survival function of the censorship $C$ given $X$, referred to as Kaplan-Meier risk, in order to perform empirical risk minimization. It is established, under mild conditions, that the learning rate of minimizers of this biased/weighted empirical risk functional is of order $O_{\\mathbb{P}}(\\sqrt{\\log(n)/n})$ when ignoring model bias issues inherent to plug-in estimation, as can be attained in absence of censorship. Beyond theoretical results, numerical experiments are presented in order to illustrate the relevance of the approach developed.", "venue": "ArXiv", "authors": ["Guillaume  Ausset", "St\u00e9phan  Cl\u00e9men\u00e7on", "Fran\u00e7ois  Portier"], "year": 2019, "n_citations": 5}
{"id": 1691274, "s2_id": "36c01373afee5a496e5e89dab10629650bcd29a8", "title": "Graph learning under sparsity priors", "abstract": "Graph signals offer a very generic and natural representation for data that lives on networks or irregular structures. The actual data structure is however often unknown a priori but can sometimes be estimated from the knowledge of the application domain. If this is not possible, the data structure has to be inferred from the mere signal observations. This is exactly the problem that we address in this paper, under the assumption that the graph signals can be represented as a sparse linear combination of a few atoms of a structured graph dictionary. The dictionary is constructed on polynomials of the graph Laplacian, which can sparsely represent a general class of graph signals composed of localized patterns on the graph. We formulate a graph learning problem, whose solution provides an ideal fit between the signal observations and the sparse graph signal model. As the problem is non-convex, we propose to solve it by alternating between a signal sparse coding and a graph update step. We provide experimental results that outline the good graph recovery performance of our method, which generally compares favourably to other recent network inference algorithms.", "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Hermina Petric Maretic", "Dorina  Thanou", "Pascal  Frossard"], "year": 2017, "n_citations": 17}
{"id": 1704819, "s2_id": "f93e365cf46972e53bb1d02832e2558fb5b723b9", "title": "On Local Optima in Learning Bayesian Networks", "abstract": "This paper proposes and evaluates the k-greedy equivalence search algorithm (KES) for learning Bayesian networks (BNs) from complete data. The main characteristic of KES is that it allows a trade-off between greediness and randomness, thus exploring different good local optima when run repeatedly. When greediness is set at maximum, KES corresponds to the greedy equivalence search algorithm (GES). When greediness is kept at minimum, we prove that under mild conditions KES asymptotically returns any inclusion optimal BN with nonzero probability. Experimental results for both synthetic and real data are reported showing that KES finds a better local optimum than GES considerably often. Additionally, these results illustrate that the number of different local optima is usually huge.", "venue": "UAI", "authors": ["Jens Dalgaard Nielsen", "Tomas  Kocka", "Jos\u00e9 M. Pe\u00f1a"], "year": 2003, "n_citations": 58}
{"id": 1710577, "s2_id": "429f020e5af8d3c12d5a7a4f3198ddb32a8c2419", "title": "Noise Flow: Noise Modeling With Conditional Normalizing Flows", "abstract": "Modeling and synthesizing image noise is an important aspect in many computer vision applications. The long-standing additive white Gaussian and heteroscedastic (signal-dependent) noise models widely used in the literature provide only a coarse approximation of real sensor noise. This paper introduces Noise Flow, a powerful and accurate noise model based on recent normalizing flow architectures. Noise Flow combines well-established basic parametric noise models (e.g., signal-dependent noise) with the flexibility and expressiveness of normalizing flow networks. The result is a single, comprehensive, compact noise model containing fewer than 2500 parameters yet able to represent multiple cameras and gain factors. Noise Flow dramatically outperforms existing noise models, with 0.42 nats/pixel improvement over the camera-calibrated noise level functions, which translates to 52% improvement in the likelihood of sampled noise. Noise Flow represents the first serious attempt to go beyond simple parametric models to one that leverages the power of deep learning and data-driven noise distributions.", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "authors": ["Abdelrahman  Abdelhamed", "Marcus A. Brubaker", "Michael S. Brown"], "year": 2019, "n_citations": 50}
{"id": 1719931, "s2_id": "7a0ff86517c937d19787826acdf995d41d5be72f", "title": "The Complexity of Sparse Tensor PCA", "abstract": "We study the problem of sparse tensor principal component analysis: given a tensor Y = W + \u03bbx\u2297p with W \u2208 \u2297R having i.i.d. Gaussian entries, the goal is to recover the k-sparse unit vector x \u2208 R. The model captures both sparse PCA (in its Wigner form) and tensor PCA. For the highly sparse regime of k \u2264 \u221a n, we present a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm. For any 1 \u2264 t \u2264 k, our algorithms recovers the sparse vector for signal-tonoise ratio \u03bb \u2265 \u00d5( \u221a t \u00b7 (k/t)) in time \u00d5(n), capturing the state-of-the-art guarantees for the matrix settings (in both the polynomial-time and sub-exponential time regimes). Our results naturally extend to the case of r distinct k-sparse signals with disjoint supports, with guarantees that are independent of the number of spikes. Even in the restricted case of sparse PCA, known algorithms only recover the sparse vectors for \u03bb \u2265 \u00d5(k \u00b7r) while our algorithms require \u03bb \u2265 \u00d5(k). Finally, by analyzing the low-degree likelihood ratio, we complement these algorithmic results with rigorous evidence illustrating the trade-offs between signal-to-noise ratio and running time. This lower bound captures the known lower bounds for both sparse PCA and tensor PCA. In this general model, we observe a more intricate three-way trade-off between the number of samples n, the sparsity k, and the tensor power p.", "venue": "ArXiv", "authors": ["Davin  Choo", "Tommaso  d'Orsi"], "year": 2021, "n_citations": 0}
{"id": 1724503, "s2_id": "2f77a490245b7e414f5637872deb6bb5ab5b0a09", "title": "A deep learning-based ODE solver for chemical kinetics", "abstract": "Developing efficient and accurate algorithms for chemistry integration is a challenging task due to its strong stiffness and high dimensionality. The current work presents a deep learning-based numerical method called DeepCombustion0.0 to solve stiff ordinary differential equation systems. The homogeneous autoignition of DME/air mixture, including 54 species, is adopted as an example to illustrate the validity and accuracy of the algorithm. The training and testing datasets cover a wide range of temperature, pressure, and mixture conditions between 750 \u2013 1200 K, 30 \u2013 50 atm, and equivalence ratio = 0.7 \u2013 1.5. Both the firststage low-temperature ignition (LTI) and the second-stage high-temperature ignition (HTI) are considered. The methodology highlights the importance of the adaptive data sampling techniques, power transform preprocessing, and binary deep neural network (DNN) design. By using the adaptive random samplings and appropriate power transforms, smooth submanifolds in the state vector phase space are observed, on which two three-layer DNNs can be appropriately trained. The neural networks are end-to-end, which predict temporal gradients of the state vectors directly. The results show that temporal evolutions predicted by DNN agree well with traditional numerical methods in all state vector dimensions, including temperature, pressure, and species concentrations. Besides, the ignition delay time differences are within 1%. At the same time, the CPU time is reduced by more than 20 times and 200 times compared with the HMTS and VODE method, respectively. The current work demonstrates the enormous potential of applying the deep learning algorithm in chemical kinetics and combustion modeling.", "venue": "ArXiv", "authors": ["Tianhan  Zhang", "Yaoyu  Zhang", "E  Weinan", "Yiguang  Ju"], "year": 2020, "n_citations": 1}
{"id": 1724544, "s2_id": "540e856fb800215159378c4d81bc8c5a0f814460", "title": "Less-forgetful Learning for Domain Expansion in Deep Neural Networks", "abstract": "Expanding the domain that deep neural network has already learned without accessing old domain data is a challenging task because deep neural networks forget previously learned information when learning new data from a new domain. In this paper, we propose a less-forgetful learning method for the domain expansion scenario. While existing domain adaptation techniques solely focused on adapting to new domains, the proposed technique focuses on working well with both old and new domains without needing to know whether the input is from the old or new domain. First, we present two naive approaches which will be problematic, then we provide a new method using two proposed properties for less-forgetful learning. Finally, we prove the effectiveness of our method through experiments on image classification tasks. All datasets used in the paper, will be released on our website for someone's follow-up study.", "venue": "AAAI", "authors": ["Heechul  Jung", "Jeongwoo  Ju", "Minju  Jung", "Junmo  Kim"], "year": 2018, "n_citations": 37}
{"id": 1729321, "s2_id": "1103ba0119f47de33d78fa38cd387859c59388f7", "title": "Quantifying Uncertainty for Machine Learning Based Diagnostic", "abstract": "Virtual Diagnostic (VD) is a deep learning tool that can be used to predict a diagnostic output. VDs are especially useful in systems where measuring the output is invasive, limited, costly or runs the risk of damaging the output. Given a prediction, it is necessary to relay how reliable that prediction is. This is known as \u2018uncertainty quantification\u2019 of a prediction. IN this paper, we use ensemble methods and quantile regression neural networks to explore different ways of creating and analyzing prediction\u2019s uncertainty on experimental data from the Linac Coherent Light Source at SLAC. We aim to accurately and confidently predict the current profile or longitudinal phase space images of the electron beam. The ability to make informed decisions under uncertainty is crucial for reliable deployment of deep learning tools on safety-critical systems as particle accelerators.", "venue": "ArXiv", "authors": ["Owen  Convery", "Lewis  Smith", "Yarin  Gal", "Adi  Hanuka"], "year": 2021, "n_citations": 0}
{"id": 1735915, "s2_id": "daa72823db07f996990441e9ed311f3864e4fa13", "title": "A Temporal Neural Network Architecture for Online Learning", "abstract": "A long-standing proposition is that by emulating the operation of the brain's neocortex, a spiking neural network (SNN) can achieve similar desirable features: flexible learning, speed, and efficiency. Temporal neural networks (TNNs) are SNNs that communicate and process information encoded as relative spike times (in contrast to spike rates). A TNN architecture is proposed, and, as a proof-of-concept, TNN operation is demonstrated within the larger context of online supervised classification. First, through unsupervised learning, a TNN partitions input patterns into clusters based on similarity. The TNN then passes a cluster identifier to a simple online supervised decoder which finishes the classification task. The TNN learning process adjusts synaptic weights by using only signals local to each synapse, and clustering behavior emerges globally. The system architecture is described at an abstraction level analogous to the gate and register transfer levels in conventional digital design. Besides features of the overall architecture, several TNN components are new to this work. Although not addressed directly, the overall research objective is a direct hardware implementation of TNNs. Consequently, all the architecture elements are simple, and processing is done at very low precision. Importantly, low precision leads to very fast learning times. Simulation results using the time-honored MNIST dataset demonstrate learning times at least an order of magnitude faster than other online approaches while providing similar error rates.", "venue": "ArXiv", "authors": ["J. E. Smith"], "year": 2020, "n_citations": 2}
{"id": 1740813, "s2_id": "7ab20bbd532a5f8a2591569bbeb31f15aec27260", "title": "Generalized Low Rank Models", "abstract": "Principal components analysis (PCA) is a well-known technique for approximating a tabular data set by a low rank matrix. Here, we extend the idea of PCA to handle arbitrary data sets consisting of numerical, Boolean, categorical, ordinal, and other data types. This framework encompasses many well-known techniques in data analysis, such as nonnegative matrix factorization, matrix completion, sparse and robust PCA, k-means, k-SVD, and maximum margin matrix factorization. The method handles heterogeneous data sets, and leads to coherent schemes for compressing, denoising, and imputing missing entries across all data types simultaneously. It also admits a number of interesting interpretations of the low rank factors, which allow clustering of examples or of features. We propose several parallel algorithms for fitting generalized low rank models, and describe implementations and numerical results.", "venue": "Found. Trends Mach. Learn.", "authors": ["Madeleine  Udell", "Corinne  Horn", "Reza Bosagh Zadeh", "Stephen P. Boyd"], "year": 2016, "n_citations": 255}
{"id": 1766365, "s2_id": "235ef04fa936faad2e3c0bebc85b263f4d4c929a", "title": "DeepSymmetry : Using 3D convolutional networks for identification of tandem repeats and internal symmetries in protein structures", "abstract": "MOTIVATION\nThanks to the recent advances in structural biology, nowadays three-dimensional structures of various proteins are solved on a routine basis. A large portion of these structures contain structural repetitions or internal symmetries. To understand the evolution mechanisms of these proteins and how structural repetitions affect the protein function, we need to be able to detect such proteins very robustly. As deep learning is particularly suited to deal with spatially organized data, we applied it to the detection of proteins with structural repetitions.\n\n\nRESULTS\nWe present DeepSymmetry, a versatile method based on three-dimensional (3D) convolutional networks that detects structural repetitions in proteins and their density maps. Our method is designed to identify tandem repeat proteins, proteins with internal symmetries, symmetries in the raw density maps, their symmetry order, and also the corresponding symmetry axes. Detection of symmetry axes is based on learning six-dimensional Veronese mappings of 3D vectors, and the median angular error of axis determination is less than one degree. We demonstrate the capabilities of our method on benchmarks with tandem repeated proteins and also with symmetrical assemblies. For example, we have discovered about 7,800 putative tandem repeat proteins in the PDB.\n\n\nAVAILABILITY\nThe method is available at https://team.inria.fr/nano-d/software/deepsymmetry. It consists of a C\u2009++ executable that transforms molecular structures into volumetric density maps, and a Python code based on the TensorFlow framework for applying the DeepSymmetry model to these maps.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.", "venue": "Bioinform.", "authors": ["Guillaume  Pag\u00e8s", "Sergei  Grudinin"], "year": 2019, "n_citations": 4}
{"id": 1779779, "s2_id": "66ce87e6ac6c372327410c806657455e1426bba2", "title": "Making a Case for Federated Learning in the Internet of Vehicles and Intelligent Transportation Systems", "abstract": "With the incoming introduction of 5G networks and the advancement in technologies such as network function virtualization and software defined networking, new and emerging networking technologies and use cases are taking shape. One such technology is the Internet of Vehicles (IoV), which describes an interconnected system of vehicles and infrastructure. Coupled with recent developments in artificial intelligence and machine learning, IoV is transformed into an intelligent transportation system (ITS). There are, however, several operational considerations that hinder the adoption of ITSs, including scalability, high availability, and data privacy. To address these challenges, federated learning, a collaborative and distributed intelligence technique, is suggested. Through an ITS case study, the ability of a federated model deployed on roadside infrastructure throughout the network to recover from faults by leveraging group intelligence while reducing recovery time and restoring acceptable system performance is highlighted. With a multitude of use cases and benefits, federated learning is a key enabler for ITS and is poised to achieve widespread implementation in 5G and beyond networks and applications.", "venue": "IEEE Network", "authors": ["Dimitrios Michael Manias", "Abdallah  Shami"], "year": 2021, "n_citations": 4}
{"id": 1790679, "s2_id": "acc7416bb5f3975ab9fc2c50f0370e12d84c91bb", "title": "Deep Auxiliary Learning for Visual Localization and Odometry", "abstract": "Localization is an indispensable component of a robot's autonomy stack that enables it to determine where it is in the environment, essentially making it a precursor for any action execution or planning. Although convolutional neural networks have shown promising results for visual localization, they are still grossly outperformed by state-of-the-art local feature-based techniques. In this work, we propose VLocNet, a new convolutional neural network architecture for 6-DoF global pose regression and odometry estimation from consecutive monocular images. Our multitask model incorporates hard parameter sharing, thus being compact and enabling real-time inference, in addition to being end-to-end trainable. We propose a novel loss function that utilizes auxiliary learning to leverage relative pose information during training, thereby constraining the search space to obtain consistent pose estimates. We evaluate our proposed VLocNet on indoor as well as outdoor datasets and show that even our single task model exceeds the performance of state-of-the-art deep architectures for global localization, while achieving competitive performance for visual odometry estimation. Furthermore, we present extensive experimental evaluations utilizing our proposed Geometric Consistency Loss that show the effectiveness of multitask learning and demonstrate that our model is the first deep learning technique to be on par with, and in some cases outperforms state-of-the-art SIFT-based approaches.", "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA)", "authors": ["Abhinav  Valada", "Noha  Radwan", "Wolfram  Burgard"], "year": 2018, "n_citations": 125}
{"id": 1793198, "s2_id": "696799380997e0a324412db5eec757543c3df4c2", "title": "Generalized Zero-shot ICD Coding", "abstract": "The International Classification of Diseases (ICD) is a list of classification codes for the diagnoses. Automatic ICD coding is in high demand as the manual coding can be labor-intensive and error-prone. It is a multi-label text classification task with extremely long-tailed label distribution, making it difficult to perform fine-grained classification on both frequent and zero-shot codes at the same time. In this paper, we propose a latent feature generation framework for generalized zero-shot ICD coding, where we aim to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. Our framework generates pseudo features conditioned on the ICD code descriptions and exploits the ICD code hierarchical structure. To guarantee the semantic consistency between the generated features and real features, we reconstruct the keywords in the input documents that are related to the conditioned ICD codes. To the best of our knowledge, this works represents the first one that proposes an adversarial generative model for the generalized zero-shot learning on multi-label text classification. Extensive experiments demonstrate the effectiveness of our approach. On the public MIMIC-III dataset, our methods improve the F1 score from nearly 0 to 20.91% for the zero-shot codes, and increase the AUC score by 3% (absolute improvement) from previous state of the art. We also show that the framework improves the performance on few-shot codes.", "venue": "ArXiv", "authors": ["Congzheng  Song", "Shanghang  Zhang", "Najmeh  Sadoughi", "Pengtao  Xie", "Eric  Xing"], "year": 2019, "n_citations": 7}
{"id": 1804827, "s2_id": "a9957727b24550d6639f92f9841174bbeb5c95d7", "title": "Context Based Emotion Recognition Using EMOTIC Dataset", "abstract": "In our everyday lives and social interactions we often try to perceive the emotional states of people. There has been a lot of research in providing machines with a similar capacity of recognizing emotions. From a computer vision perspective, most of the previous efforts have been focusing in analyzing the facial expressions and, in some cases, also the body pose. Some of these methods work remarkably well in specific settings. However, their performance is limited in natural, unconstrained environments. Psychological studies show that the scene context, in addition to facial expression and body pose, provides important information to our perception of people's emotions. However, the processing of the context for automatic emotion recognition has not been explored in depth, partly due to the lack of proper data. In this paper we present EMOTIC, a dataset of images of people in a diverse set of natural situations, annotated with their apparent emotion. The EMOTIC dataset combines two different types of emotion representation: (1) a set of 26 discrete categories, and (2) the continuous dimensions Valence, Arousal, and Dominance. We also present a detailed statistical and algorithmic analysis of the dataset along with annotators\u2019 agreement analysis. Using the EMOTIC dataset we train different CNN models for emotion recognition, combining the information of the bounding box containing the person with the contextual information extracted from the scene. Our results show how scene context provides important information to automatically recognize emotional states and motivate further research in this direction.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": ["Ronak  Kosti", "Jose M. Alvarez", "Adria  Recasens", "Agata  Lapedriza"], "year": 2020, "n_citations": 25}
{"id": 1856696, "s2_id": "19e5a48bab9c6536db4df8014f0983545110eb72", "title": "A Seq-to-Seq Transformer Premised Temporal Convolutional Network for Chinese Word Segmentation", "abstract": "The prevalent approaches of Chinese word segmentation task almost rely on the Bi-LSTM neural network. However, the methods based the Bi-LSTM have some inherent drawbacks: hard to parallel computing, little efficient in applying the Dropout method to inhibit the Overfitting and little efficient in capturing the character information at the more distant site of a long sentence for the word segmentation task. In this work, we propose a sequence-to-sequence transformer model for Chinese word segmentation, which is premised a type of convolutional neural network named temporal convolutional network. The model uses the temporal convolutional network to construct an encoder, and uses one layer of fully-connected neural network to build a decoder, and applies the Dropout method to inhibit the Overfitting, and captures the character information at the distant site of a sentence by adding the layers of the encoder, and binds Conditional Random Fields model to train parameters, and uses the Viterbi algorithm to infer the final result of the Chinese word segmentation. The experiments on traditional Chinese corpora and simplified Chinese corpora show that the performance of Chinese word segmentation of the model is equivalent to the performance of the methods based the Bi-LSTM, and the model has a tremendous growth in parallel computing than the models based the Bi-LSTM.", "venue": "ArXiv", "authors": ["Wei  Jiang", "Yan  Tang"], "year": 2019, "n_citations": 3}
{"id": 1857956, "s2_id": "f365e170c08810d91e3fdc3f088a7cf14507a43a", "title": "Generalization and Memorization: The Bias Potential Model", "abstract": "Models for learning probability distributions such as generative models and density estimators behave quite differently from models for learning functions. One example is found in the memorization phenomenon, namely the ultimate convergence to the empirical distribution, that occurs in generative adversarial networks (GANs). For this reason, the issue of generalization is more subtle than that for supervised learning. For the bias potential model, we show that dimension-independent generalization accuracy is achievable if early stopping is adopted, despite that in the long term, the model either memorizes the samples or diverges.", "venue": "ArXiv", "authors": ["Hongkang  Yang", "E  Weinan"], "year": 2020, "n_citations": 2}
{"id": 1877882, "s2_id": "bfe3ccdaf40f04421c60df102ad8f5030d1fb65a", "title": "Growing Regression Forests by Classification: Applications to Object Pose Estimation", "abstract": "In this work, we propose a novel node splitting method for regression trees and incorporate it into the regression forest framework. Unlike traditional binary splitting, where the splitting rule is selected from a predefined set of binary splitting rules via trial-and-error, the proposed node splitting method first finds clusters of the training data which at least locally minimize the empirical loss without considering the input space. Then splitting rules which preserve the found clusters as much as possible are determined by casting the problem into a classification problem. Consequently, our new node splitting method enjoys more freedom in choosing the splitting rules, resulting in more efficient tree structures. In addition to the Euclidean target space, we present a variant which can naturally deal with a circular target space by the proper use of circular statistics. We apply the regression forest employing our node splitting to head pose estimation (Euclidean target space) and car direction estimation (circular target space) and demonstrate that the proposed method significantly outperforms state-of-the-art methods (38.5% and 22.5% error reduction respectively).", "venue": "ECCV", "authors": ["Kota  Hara", "Rama  Chellappa"], "year": 2014, "n_citations": 75}
{"id": 1900284, "s2_id": "a34315a8367693528ece708ef85f608ff850bbf1", "title": "Learning low bending and low distortion manifold embeddings", "abstract": "Autoencoders are a widespread tool in machine learning to transform high-dimensional data into a lower-dimensional representation which still exhibits the essential characteristics of the input. The encoder provides an embedding from the input data manifold into a latent space which may then be used for further processing. For instance, learning interpolation on the manifold may be simplified via the new manifold representation in latent space. The efficiency of such further processing heavily depends on the regularity and structure of the embedding. In this article, the embedding into latent space is regularized via a loss function that promotes an as isometric and as flat embedding as possible. The required training data comprises pairs of nearby points on the input manifold together with their local distance and their local Fr\u00e9chet average. This regularity loss functional even allows to train the encoder on its own. The loss functional is computed via a Monte Carlo integration which is shown to be consistent with a geometric loss functional defined directly on the embedding map. Numerical tests are performed using image data that encodes different data manifolds. The results show that smooth manifold embeddings in latent space are obtained. These embeddings are regular enough such that interpolation between not too distant points on the manifold is well approximated by linear interpolation in latent space.", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": ["Juliane  Braunsmann", "Marko  Rajkovi'c", "Martin  Rumpf", "Benedikt  Wirth"], "year": 2021, "n_citations": 0}
{"id": 1903565, "s2_id": "a7b670b207c79de49370d5e93d7429b410cd7b0f", "title": "Stochastic Gradient Variance Reduction by Solving a Filtering Problem", "abstract": "Deep neural networks (DNN) are typically optimized using stochastic gradient descent (SGD). However, the estimation of the gradient using stochastic samples tends to be noisy and unreliable, resulting in large gradient variance and bad convergence. In this paper, we propose Filter Gradient Decent (FGD), an efficient stochastic optimization algorithm that makes the consistent estimation of the local gradient by solving an adaptive filtering problem with different design of filters. Our method reduces variance in stochastic gradient descent by incorporating the historical states to enhance the current estimation. It is able to correct noisy gradient direction as well as to accelerate the convergence of learning. We demonstrate the effectiveness of the proposed Filter Gradient Descent on numerical optimization and training neural networks, where it achieves superior and robust performance compared with traditional momentum-based methods. To the best of our knowledge, we are the first to provide a practical solution that integrates filtering into gradient estimation by making the analogy between gradient estimation and filtering problem in signal processing. 1", "venue": "ArXiv", "authors": ["Xingyi  Yang"], "year": 2020, "n_citations": 0}
{"id": 1915776, "s2_id": "bcb2d1c9cdc321d192925cc97c563470b30b8251", "title": "Hierarchical Federated Learning ACROSS Heterogeneous Cellular Networks", "abstract": "We consider federated edge learning (FEEL), where mobile users (MUs) collaboratively learn a global model by sharing local updates on the model parameters rather than their datasets, with the help of a mobile base station (MBS). We optimize the resource allocation among MUs to reduce the communication latency in learning iterations. Observing that the performance in this centralized setting is limited due to the distance of the cell-edge users to the MBS, we introduce small cell base stations (SBSs) orchestrating FEEL among MUs within their cells, and periodically exchanging model updates with the MBS for global consensus. We show that this hierarchical federated learning (HFL) scheme significantly reduces the communication latency without sacrificing the accuracy.", "venue": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Mehdi Salehi Heydar Abad", "Emre  Ozfatura", "Deniz  G\u00fcnd\u00fcz", "\u00d6zg\u00fcr  Er\u00e7etin"], "year": 2020, "n_citations": 86}
{"id": 1916238, "s2_id": "24743abdec6942de2a478c799cd3c28aa2f22c44", "title": "Cross-modal variational inference for bijective signal-symbol translation", "abstract": "Extraction of symbolic information from signals is an active field of research enabling numerous applications especially in the Musical Information Retrieval domain. This complex task, that is also related to other topics such as pitch extraction or instrument recognition, is a demanding subject that gave birth to numerous approaches, mostly based on advanced signal processing-based algorithms. However, these techniques are often non-generic, allowing the extraction of definite physical properties of the signal (pitch, octave), but not allowing arbitrary vocabularies or more general annotations. On top of that, these techniques are one-sided, meaning that they can extract symbolic data from an audio signal, but cannot perform the reverse process and make symbol-to-signal generation. In this paper, we propose an bijective approach for signal/symbol translation by turning this problem into a density estimation task over signal and symbolic domains, considered both as related random variables. We estimate this joint distribution with two different variational auto-encoders, one for each domain, whose inner representations are forced to match with an additive constraint, allowing both models to learn and generate separately while allowing signal-to-symbol and symbol-to-signal inference. In this article, we test our models on pitch, octave and dynamics symbols, which comprise a fundamental step towards music transcription and label-constrained audio generation. In addition to its versatility, this system is rather light during training and generation while allowing several interesting creative uses that we outline at the end of the article.", "venue": "ArXiv", "authors": ["Axel  Chemla--Romeu-Santos", "Stavros  Ntalampiras", "Philippe  Esling", "Goffredo  Haus", "G'erard  Assayag"], "year": 2020, "n_citations": 4}
{"id": 1919457, "s2_id": "695bdf80b67e09b00eb38278aab5e7fd0eecfd86", "title": "Computer-Aided Diagnosis of Low Grade Endometrial Stromal Sarcoma (LGESS)", "abstract": "Low grade endometrial stromal sarcoma (LGESS) accounts for about 0.2% of all uterine cancer cases. Approximately 75% of LGESS patients are initially misdiagnosed with leiomyoma, which is a type of benign tumor, also known as fibroids. In this research, uterine tissue biopsy images of potential LGESS patients are preprocessed using segmentation and stain normalization algorithms. We then apply a variety of classic machine learning and advanced deep learning models to classify tissue images as either benign or cancerous. For the classic techniques considered, the highest classification accuracy we attain is about 0.85, while our best deep learning model achieves an accuracy of approximately 0.87. These results clearly indicate that properly trained learning algorithms can aid in the diagnosis of LGESS.", "venue": "Computers in biology and medicine", "authors": ["Xinxin  Yang", "Mark  Stamp"], "year": 2021, "n_citations": 0}
{"id": 1938856, "s2_id": "190999dcc6895c0218961a11ae3ef4d5d03ae5f0", "title": "CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture Recognition", "abstract": "Egocentric gestures are the most natural form of communication for humans to interact with wearable devices such as VR/AR helmets and glasses. A major issue in such scenarios for real-world applications is that may easily become necessary to add new gestures to the system e.g., a proper VR system should allow users to customize gestures incrementally. Traditional deep learning methods require storing all previous class samples in the system and training the model again from scratch by incorporating previous samples and new samples, which costs humongous memory and significantly increases computation over time. In this work, we demonstrate a lifelong 3D convolutional framework - c(C)la(a)ss increment(t)al net(Net)works (CatNet), which considers temporal information in videos and enables life-long learning for egocentric gesture video recognition by learning the feature representation of an exemplar set selected from previous class samples. Importantly, we propose a two-stream CatNet, which deploys RGB and depth modalities to train two separate networks. We evaluate Cat- Nets on a publicly available dataset - EgoGesture dataset, and show that CatNets can learn many classes incrementally over a long period of time. Results also demonstrate that the two-stream architecture achieves the best performance on both joint training and class incremental training compared to 3 other one-stream architectures. The codes and pre-trained models used in this work are provided at https://github.com/villawang/CatNet.", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": ["Zhengwei  Wang", "Qi  She", "Tejo  Chalasani", "Aljosa  Smolic"], "year": 2020, "n_citations": 5}
{"id": 1939299, "s2_id": "3250a024104a6ade3bfbfed1b807e2bb41945ec8", "title": "SDnDTI: Self-supervised deep learning-based denoising for diffusion tensor MRI", "abstract": "Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, Massachusetts, United States; Department of Radiology, Harvard Medical School, Boston, Massachusetts, United States; Department of Biomedical Engineering, Tsinghua University, Beijing, P. R. China; Harvard-MIT Division of Health Sciences and Technology, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States.", "venue": "ArXiv", "authors": ["Qiyuan  Tian", "Ziyu  Li", "Qiuyun  Fan", "Jonathan R. Polimeni", "Berkin  Bilgic", "David H. Salat", "Susie Y. Huang"], "year": 2021, "n_citations": 0}
{"id": 1940497, "s2_id": "e07e9ecf7d95b765a1e9e47470a1e4a287cec0f1", "title": "An Overview of Human Activity Recognition Using Wearable Sensors: Healthcare and Artificial Intelligence", "abstract": "With the rapid development of the internet of things (IoT) and artificial intelligence (AI) technologies, human activity recognition (HAR) has been applied in a variety of domains such as security and surveillance, human-robot interaction, and entertainment. Even though a number of surveys and review papers have been published, there is a lack of HAR overview paper focusing on healthcare applications that use wearable sensors. Therefore, we fill in the gap by presenting this overview paper. In particular, we present our emerging HAR projects for healthcare: identification of human activities for intensive care unit (ICU) patients and Duchenne muscular dystrophy (DMD) patients. Our HAR systems include hardware design to collect sensor data from ICU patients and DMD patients and accurate AI models to recognize patients\u2019 activities. This overview paper covers considerations and settings for building a HAR healthcare system, including sensor factors, AI model comparison, and system challenges. HUMAN ACTIVITY RECOGNITION has been actively researched in the past decade, thanks to the increasing number of deployed smart-devices such as smartphones and IoT devices. Based on the type of data being processed, a HAR system can be classified into vision-based and sensorbased. This paper targets wearable-sensor HAR systems, which are the most prevalent type of sensor-based HAR systems [1]. More importantly, wearable-sensor HAR systems do not suffer from severe privacy issues as in vision-based HAR systems, making wearable-sensor HAR systems suitable for healthcare applications. In a wearable-sensor HAR system, a user wears portable mobile devices that have builtin sensors. The activities of the user can then * These two authors contributed equally to this work. be classified by measuring and characterizing sensor signals when the user is conducting daily activities. A number of wearable-sensor HAR systems have been developed to deal with various healthcare problems. For example, Eliasz Kantoch designs a waist-worn system to track the users\u2019 amount of exercises in order to reduce the risk of morbidity [2]; Nagaraj Hedge et al. build a wrist-worn system to recognize activities of daily living for the treatment of health conditions [3]. However, building practical wearable-sensor HAR systems for healthcare is challenging in terms of sensor setup, data collection and processing, AI model selection, etc. Therefore, we propose this overview paper in the hope to shed light on designing wearable-sensor HAR systems for healthcare applications. Specifically, we share two of our HAR-based healthcare systems with Published by the IEEE Computer Society \u00a9 IEEE 1 ar X iv :2 10 3. 15 99 0v 1 [ cs .H C ] 2 9 M ar 2 02 1 Raw sensor data Feature extraction/selection Training set Validation set Test set Machine learning Model Model Training New sensor data (unseen) Feature extraction/selection", "venue": "ArXiv", "authors": ["Rex  Liu", "Albara Ah Ramli", "Huanle  Zhang", "Esha  Datta", "Xin  Liu"], "year": 2021, "n_citations": 2}
{"id": 1948789, "s2_id": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33", "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning", "abstract": "The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.", "venue": "AAAI", "authors": ["Matteo  Hessel", "Joseph  Modayil", "Hado van Hasselt", "Tom  Schaul", "Georg  Ostrovski", "Will  Dabney", "Dan  Horgan", "Bilal  Piot", "Mohammad Gheshlaghi Azar", "David  Silver"], "year": 2018, "n_citations": 1042}
{"id": 1968587, "s2_id": "075d00eeece71f122dcbb17588c091383948bf37", "title": "Terahertz-Band Joint Ultra-Massive MIMO Radar-Communications: Model-Based and Model-Free Hybrid Beamforming", "abstract": "Wireless communications and sensing at terahertz (THz) band are increasingly investigated as promising short-range technologies because of the availability of high operational bandwidth at THz. In order to address the extremely high attenuation at THz, ultra-massive multiple-input multiple-output (MIMO) antenna systems have been proposed for THz communications to compensate propagation losses. However, the cost and power associated with fully digital beamformers of these huge antenna arrays are prohibitive. In this paper, we develop wideband hybrid beamformers based on both model-based and model-free techniques for a new group-of-subarrays (GoSA) ultra-massive MIMO structure in low-THz band. Further, driven by the recent developments to save the spectrum, we propose beamformers for a joint ultra-massive MIMO radar-communications system, wherein the base station serves multi-antenna user equipment (RX), and tracks radar targets by generating multiple beams toward both RX and the targets. We formulate the GoSA beamformer design as an optimization problem to provide a trade-off between the unconstrained communications beamformers and the desired radar beamformers. To mitigate the beam split effect at THz band arising from frequency-independent analog beamformers, we propose a phase correction technique to align the beams of multiple subcarriers toward a single physical direction. Additionally, our design also exploits second-order channel statistics so that an infrequent channel feedback from the RX is achieved with less channel overhead. To further decrease the ultra-massive MIMO computational complexity and enhance robustness, we also implement deep learning solutions to the proposed model-based hybrid beamformers. Numerical experiments demonstrate that both techniques outperform the conventional approaches in terms of spectral efficiency and radar beampatterns, as well as exhibiting less hardware cost and computation time.", "venue": "IEEE Journal of Selected Topics in Signal Processing", "authors": ["Ahmet M. Elbir", "Kumar Vijay Mishra", "Symeon  Chatzinotas"], "year": 2021, "n_citations": 5}
{"id": 1972081, "s2_id": "2e04f58fa272a6eea79d23113f6a4743f8f53ac0", "title": "Fantasy Football Prediction", "abstract": "The ubiquity of professional sports and specifically the NFL have lead to an increase in popularity for Fantasy Football. Users have many tools at their disposal: statistics, predictions, rankings of experts and even recommendations of peers. There are issues with all of these, though. Especially since many people pay money to play, the prediction tools should be enhanced as they provide unbiased and easy-to-use assistance for users. This paper provides and discusses approaches to predict Fantasy Football scores of Quarterbacks with relatively limited data. In addition to that, it includes several suggestions on how the data could be enhanced to achieve better results. The dataset consists only of game data from the last six NFL seasons. I used two different methods to predict the Fantasy Football scores of NFL players: Support Vector Regression (SVR) and Neural Networks. The results of both are promising given the limited data that was used.", "venue": "ArXiv", "authors": ["Roman W. Lutz"], "year": 2015, "n_citations": 8}
{"id": 1976088, "s2_id": "cff8fc77bd7d860a1f9c910ef85e471b00c8b2fb", "title": "Exploiting Behavioral Consistence for Universal User Representation", "abstract": "User modeling is critical for developing personalized services in industry. A common way for user modeling is to learn user representations that can be distinguished by their interests or preferences. In this work, we focus on developing universal user representation model. The obtained universal representations are expected to contain rich information, and be applicable to various downstream applications without further modifications (e.g., user preference prediction and user profiling). Accordingly, we can be free from the heavy work of training task-specific models for every downstream task as in previous works. In specific, we propose Self-supervised User Modeling Network (SUMN) to encode behavior data into the universal representation. It includes two key components. The first one is a new learning objective, which guides the model to fully identify and preserve valuable user information under a self-supervised learning framework. The other one is a multi-hop aggregation layer, which benefits the model capacity in aggregating diverse behaviors. Extensive experiments on benchmark datasets show that our approach can outperform state-of-the-art unsupervised representation methods, and even compete with supervised ones.", "venue": "AAAI", "authors": ["Jie  Gu", "Feng  Wang", "Qinghui  Sun", "Zhiquan  Ye", "Xiaoxiao  Xu", "Jingmin  Chen", "Jun  Zhang"], "year": 2021, "n_citations": 5}
{"id": 1990076, "s2_id": "319f4a812d9e4f56ae2ba06cead7f1a78ab3a5a9", "title": "Deep Neural Network Approach to Forward-Inverse Problems", "abstract": "In this paper, we construct approximated solutions of Differential Equations (DEs) using the Deep Neural Network (DNN). Furthermore, we present an architecture that includes the process of finding model parameters through experimental data, the inverse problem. That is, we provide a unified framework of DNN architecture that approximates an analytic solution and its model parameters simultaneously. The architecture consists of a feed forward DNN with non-linear activation functions depending on DEs, automatic differentiation, reduction of order, and gradient based optimization method. We also prove theoretically that the proposed DNN solution converges to an analytic solution in a suitable function space for fundamental DEs. Finally, we perform numerical experiments to validate the robustness of our simplistic DNN architecture for 1D transport equation, 2D heat equation, 2D wave equation, and the Lotka-Volterra system.", "venue": "Networks Heterog. Media", "authors": ["Hyeontae  Jo", "Hwijae  Son", "Hyung Ju Hwang", "Eunheui  Kim"], "year": 2020, "n_citations": 15}
{"id": 1997927, "s2_id": "7c3ba65b82291c097fe64c9244feb7e3cc9c8460", "title": "Latent Normalizing Flows for Discrete Sequences", "abstract": "Normalizing flows are a powerful class of generative models for continuous random variables, showing both strong model flexibility and the potential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly applying normalizing flows to discrete sequences poses significant additional challenges. We propose a VAE-based generative model which jointly learns a normalizing flow-based distribution in the latent space and a stochastic mapping to an observed discrete space. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To capture this property, we propose several normalizing flow architectures to maximize model flexibility. Experiments consider common discrete sequence tasks of character-level language modeling and polyphonic music generation. Our results indicate that an autoregressive flow-based model can match the performance of a comparable autoregressive baseline, and a non-autoregressive flow-based model can improve generation speed with a penalty to performance.", "venue": "ICML", "authors": ["Zachary M. Ziegler", "Alexander M. Rush"], "year": 2019, "n_citations": 65}
{"id": 2021991, "s2_id": "c50a322b4108d527de05585cc4909bd92f3e688d", "title": "Long-tail learning via logit adjustment", "abstract": "Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels are associated with only a few samples. This poses a challenge for generalisation on such labels, and also makes naive learning biased towards dominant labels. In this paper, we present two simple modifications of standard softmax cross-entropy training to cope with these challenges. Our techniques revisit the classic idea of logit adjustment based on the label frequencies, either applied post-hoc to a trained model, or enforced in the loss during training. Such adjustment encourages a large relative margin between logits of rare versus dominant labels. These techniques unify and generalise several recent proposals in the literature, while possessing firmer statistical grounding and empirical performance.", "venue": "ICLR", "authors": ["Aditya Krishna Menon", "Sadeep  Jayasumana", "Ankit Singh Rawat", "Himanshu  Jain", "Andreas  Veit", "Sanjiv  Kumar"], "year": 2021, "n_citations": 65}
{"id": 2048579, "s2_id": "2606e6cfe57dcdc4d8ba75e5aaac7e4a29736def", "title": "Diversity-Aware Weighted Majority Vote Classifier for Imbalanced Data", "abstract": "In this paper, we propose a diversity-aware ensemble learning based algorithm, referred to as DAMVI, to deal with imbalanced binary classification tasks. Specifically, after learning base classifiers, the algorithm i) increases the weights of positive examples (minority class) which are \"hard\" to classify with uniformly weighted base classifiers; and ii) then learns weights over base classifiers by optimizing the PAC-Bayesian C-Bound that takes into account the accuracy and diversity between the classifiers. We show efficiency of the proposed approach with respect to state-of-art models on predictive maintenance tasks, credit card fraud detection, webpage classification and medical applications.", "venue": "2020 International Joint Conference on Neural Networks (IJCNN)", "authors": ["Anil  Goyal", "Jihed  Khiari"], "year": 2020, "n_citations": 0}
{"id": 2052178, "s2_id": "810e44d41dd10dd8f481cec07839553633a08747", "title": "On exploring practical potentials of quantum auto-encoder with advantages", "abstract": "Quantum auto-encoder (QAE) is a powerful tool to relieve the curse of dimensionality encountered in quantum physics, celebrated by the ability to extract low-dimensional patterns from quantum states living in the high-dimensional space. Despite its attractive properties, little is known about the practical applications of QAE with provable advantages. To address these issues, here we prove that QAE can be used to efficiently calculate the eigenvalues and prepare the corresponding eigenvectors of a high-dimensional quantum state with the low-rank property. With this regard, we devise three effective QAE-based learning protocols to solve the low-rank state fidelity estimation, the quantum Gibbs state preparation, and the quantum metrology tasks, respectively. Notably, all of these protocols are scalable and can be readily executed on near-term quantum machines. Moreover, we prove that the error bounds of the proposed QAE-based methods outperform those in previous literature. Numerical simulations collaborate with our theoretical analysis. Our work opens a new avenue of utilizing QAE to tackle various quantum physics and quantum information processing problems in a scalable way.", "venue": "ArXiv", "authors": ["Yuxuan  Du", "Dacheng  Tao"], "year": 2021, "n_citations": 2}
{"id": 2109653, "s2_id": "3e747b43ec64f35325848b4e028643dd25757693", "title": "Machine Learning for CSI Recreation Based on Prior Knowledge", "abstract": "Knowledge of channel state information (CSI) is fundamental to many functionalities within the mobile wireless communications systems. With the advance of machine learning (ML) and digital maps, i.e., digital twins, we have a big opportunity to learn the propagation environment and design novel methods to derive and report CSI. In this work, we propose to combine untrained neural networks (UNNs) and conditional generative adversarial networks (cGANs) for MIMO channel recreation based on prior knowledge. The UNNs learn the prior-CSI for some locations which are used to build the input to a cGAN. Based on the prior-CSIs, their locations and the location of the desired channel, the cGAN is trained to output the channel expected at the desired location. This combined approach can be used for low overhead CSI reporting as, after training, we only need to report the desired location. Our results show that our method is successful in modelling the wireless channel and robust to location quantization errors in line of sight conditions.", "venue": "ArXiv", "authors": ["Brenda Vilas Boas", "Wolfgang  Zirwas", "Martin  Haardt"], "year": 2021, "n_citations": 0}
{"id": 2122899, "s2_id": "41b3c74c1be988ead54ea124b7ea63255655a4c4", "title": "Autonomous Learning for Face Recognition in the Wild via Ambient Wireless Cues", "abstract": "Facial recognition is a key enabling component for emerging Internet of Things (IoT) services such as smart homes or responsive offices. Through the use of deep neural networks, facial recognition has achieved excellent performance. However, this is only possibly when trained with hundreds of images of each user in different viewing and lighting conditions. Clearly, this level of effort in enrolment and labelling is impossible for wide-spread deployment and adoption. Inspired by the fact that most people carry smart wireless devices with them, e.g. smartphones, we propose to use this wireless identifier as a supervisory label. This allows us to curate a dataset of facial images that are unique to a certain domain e.g. a set of people in a particular office. This custom corpus can then be used to finetune existing pre-trained models e.g. FaceNet. However, due to the vagaries of wireless propagation in buildings, the supervisory labels are noisy and weak. We propose a novel technique, AutoTune, which learns and refines the association between a face and wireless identifier over time, by increasing the inter-cluster separation and minimizing the intra-cluster distance. Through extensive experiments with multiple users on two sites, we demonstrate the ability of AutoTune to design an environment-specific, continually evolving facial recognition system with entirely no user effort.", "venue": "WWW", "authors": ["Chris Xiaoxuan Lu", "Xuan  Kan", "Bowen  Du", "Changhao  Chen", "Hongkai  Wen", "Andrew  Markham", "Agathoniki  Trigoni", "John A. Stankovic"], "year": 2019, "n_citations": 5}
{"id": 2134322, "s2_id": "67c78d3655759c3a548fe8cab07a0bee4e34afd7", "title": "DeepScores-A Dataset for Segmentation, Detection and Classification of Tiny Objects", "abstract": "We present the DeepScores dataset with the goal of advancing the state-of-the-art in small object recognition by placing the question of object recognition in the context of scene understanding. DeepScores contains high quality images of musical scores, partitioned into 300, 000 sheets of written music that contain symbols of different shapes and sizes. With close to a hundred million small objects, this makes our dataset not only unique, but also the largest public dataset. DeepScores comes with ground truth for object classification, detection and semantic segmentation. DeepScores thus poses a relevant challenge for computer vision in general, and optical music recognition (OMR) research in particular. We present a detailed statistical analysis of the dataset, comparing it with other computer vision datasets like PASCAL VOC, SUN, SVHN, ImageNet, MS-COCO, as well as with other OMR datasets. Finally, we provide baseline performances for object classification, intuition for the inherent difficulty that DeepScores poses to state-of-the-art object detectors like YOLO or R-CNN, and give pointers to future research based on this dataset.", "venue": "ICPR 2018", "authors": ["Lukas  Tuggener", "Ismail  Elezi", "J\u00fcrgen  Schmidhuber", "Marcello  Pelillo", "Thilo  Stadelmann"], "year": 2018, "n_citations": 0}
{"id": 2138163, "s2_id": "7db2b07c4be80b06a13ee10934074aa7c4c6c94a", "title": "MPF6D: Masked Pyramid Fusion 6D Pose Estimation", "abstract": "Object pose estimation has multiple important applications, such as robotic grasping and augmented reality. We present a new method to estimate the 6D pose of objects that improves upon the accuracy of current proposals and can still be used in real-time. Our method uses RGB-D data as input to segment objects and estimate their pose. It uses a neural network with multiple heads, one head estimates the object classification and generates the mask, the second estimates the values of the translation vector and the last head estimates the values of the quaternion that represents the rotation of the object. These heads leverage a pyramid architecture used during feature extraction and feature fusion. Our method can be used in real-time with its low inference time of 0.12 seconds and has high accuracy. With this combination of fast inference and good accuracy it is possible to use our method in robotic pick and place tasks and/or augmented reality applications.", "venue": "ArXiv", "authors": ["Nuno  Pereira", "Lu'is A. Alexandre"], "year": 2021, "n_citations": 0}
{"id": 2138853, "s2_id": "ab6cc09a72a7d15c8b21be9022c8bec694c79d93", "title": "Performance Evaluation of Classification Models for Household Income, Consumption and Expenditure Data Set", "abstract": "Page No.: 134-140 Volume: 20, Issue 5, 2021 ISSN: 1682-3915 Asian Journal of Information Technology Copy Right: Medwell Publications Abstract: Food security is more prominent on the policy agenda today than it has been in the past, thanks to recent food shortages at both the regional and global levels as well as renewed promises from major donor countries to combat chronic hunger. One field where machine learning can be used is in the classification of household food insecurity. In this study, we establish a robust methodology to categorize whether or not a household is being food secure and food insecure by machine learning algorithms. In this study, we have used ten machine learning algorithms to classify the food security status of the Household. Gradient Boosting (GB), Random Forest (RF), Extra Tree (ET), Bagging, K-Nearest Neighbor (KNN), Decision Tree (DT), Support Vector Machine (SVM), Logistic Regression (LR), Ada Boost (AB) and Naive Bayes were the classification algorithms used throughout this study (NB). Then, we perform classification tasks from developing data set for household food security status by gathering data from HICE survey data and validating it by Domain Experts. The performance of all classifiers has better results for all performance metrics. The performance of the Random Forest and Gradient Boosting models are outstanding with a testing accuracy of 0.9997 and the other classifier such as Bagging, Decision tree, Ada Boost, Extra tree, K-nearest neighbor, Logistic Regression, SVM and Naive Bayes are scored 0.9996, 0.09996, 0.9994, 0.95675, 0.9415, 0.8915, 0.7853 and 0.7595, respectively.", "venue": "ArXiv", "authors": ["Mersha  Nigus", "Dorsewamy"], "year": 2021, "n_citations": 0}
{"id": 2147666, "s2_id": "3516a7d5b95637346fac4dff8c3e529642cb7717", "title": "T-SCI: A Two-Stage Conformal Inference Algorithm with Guaranteed Coverage for Cox-MLP", "abstract": "It is challenging to deal with censored data, where we only have access to the incomplete information of survival time instead of its exact value. Fortunately, under linear predictor assumption, people can obtain guaranteed coverage for the confidence band of survival time using methods like Cox Regression. However, when relaxing the linear assumption with neural networks (e.g., Cox-MLP (Katzman et al., 2018; Kvamme et al., 2019)), we lose the guaranteed coverage. To recover the guaranteed coverage without linear assumption, we propose two algorithms based on conformal inference under strong ignorability assumption. In the first algorithm WCCI, we revisit weighted conformal inference and introduce a new nonconformity score based on partial likelihood. We then propose a two-stage algorithm T-SCI, where we run WCCI in the first stage and apply quantile conformal inference to calibrate the results in the second stage. Theoretical analysis shows that T-SCI returns guaranteed coverage under milder assumptions than WCCI. We conduct extensive experiments on synthetic data and real data using different methods, which validate our analysis.", "venue": "ICML", "authors": ["Jiaye  Teng", "Zeren  Tan", "Yang  Yuan"], "year": 2021, "n_citations": 0}
{"id": 2152477, "s2_id": "96c90ab340b2823a0040cd64b25d1d32af807994", "title": "Calibration of Encoder Decoder Models for Neural Machine Translation", "abstract": "We study the calibration of several state of the art neural machine translation(NMT) systems built on attention-based encoder-decoder models. For structured outputs like in NMT, calibration is important not just for reliable confidence with predictions, but also for proper functioning of beam-search inference. We show that most modern NMT models are surprisingly miscalibrated even when conditioned on the true previous tokens. Our investigation leads to two main reasons -- severe miscalibration of EOS (end of sequence marker) and suppression of attention uncertainty. We design recalibration methods based on these signals and demonstrate improved accuracy, better sequence-level calibration, and more intuitive results from beam-search.", "venue": "ArXiv", "authors": ["Aviral  Kumar", "Sunita  Sarawagi"], "year": 2019, "n_citations": 38}
{"id": 2157502, "s2_id": "52d2f27da12d2ac4202d2d1db2c7a04d623cadf0", "title": "Automatic assessment of Alzheimer's disease diagnosis based on deep learning techniques", "abstract": "Early detection is crucial to prevent the progression of Alzheimer's disease (AD). Thus, specialists can begin preventive treatment as soon as possible. They demand fast and precise assessment in the diagnosis of AD in the earliest and hardest to detect stages. The main objective of this work is to develop a system that automatically detects the presence of the disease in sagittal magnetic resonance images (MRI), which are not generally used. Sagittal MRIs from ADNI and OASIS data sets were employed. Experiments were conducted using Transfer Learning (TL) techniques in order to achieve more accurate results. There are two main conclusions to be drawn from this work: first, the damages related to AD and its stages can be distinguished in sagittal MRI and, second, the results obtained using DL models with sagittal MRIs are similar to the state-of-the-art, which uses the horizontal-plane MRI. Although sagittal-plane MRIs are not commonly used, this work proved that they were, at least, as effective as MRI from other planes at identifying AD in early stages. This could pave the way for further research. Finally, one should bear in mind that in certain fields, obtaining the examples for a data set can be very expensive. This study proved that DL models could be built in these fields, whereas TL is an essential tool for completing the task with fewer examples.", "venue": "Comput. Biol. Medicine", "authors": ["Alejandro  Puente-Castro", "Enrique  Fern\u00e1ndez-Blanco", "Alejandro  Pazos", "Cristian R. Munteanu"], "year": 2020, "n_citations": 16}
{"id": 2163997, "s2_id": "2886253fb82537a6dff134a838008d222a77a316", "title": "Welfare and Distributional Impacts of Fair Classification", "abstract": "Current methodologies in machine learning analyze the effects of various statistical parity notions of fairness primarily in light of their impacts on predictive accuracy and vendor utility loss. In this paper, we propose a new framework for interpreting the effects of fairness criteria by converting the constrained loss minimization problem into a social welfare maximization problem. This translation moves a classifier and its output into utility space where individuals, groups, and society at-large experience different welfare changes due to classification assignments. Under this characterization, predictions and fairness constraints are seen as shaping societal welfare and distribution and revealing individuals' implied welfare weights in society--weights that may then be interpreted through a fairness lens. The social welfare formulation of the fairness problem brings to the fore concerns of distributive justice that have always had a central albeit more implicit role in standard algorithmic fairness approaches.", "venue": "ArXiv", "authors": ["Lily  Hu", "Yiling  Chen"], "year": 2018, "n_citations": 22}
{"id": 2165378, "s2_id": "9ee4c1717c663c281006f31c0a2f8831c0512294", "title": "Counterfactual Explanations for Machine Learning: Challenges Revisited", "abstract": "Copyright held by the owner/author(s). CHI\u201921,, May 8\u20139, 2021, Virtual ACM 978-1-4503-6819-3/20/04. https://doi.org/10.1145/3334480.XXXXXXX Abstract Counterfactual explanations (CFEs) are an emerging technique under the umbrella of interpretability of machine learning (ML) models. They provide \u201cwhat if\u201d feedback of the form \u201cif an input datapoint were x\u2032 instead of x, then an ML model\u2019s output would be y\u2032 instead of y.\u201d Counterfactual explainability for ML models has yet to see widespread adoption in industry. In this short paper, we posit reasons for this slow uptake. Leveraging recent work outlining desirable properties of CFEs and our experience running the ML wing of a model monitoring startup, we identify outstanding obstacles hindering CFE deployment in industry.", "venue": "ArXiv", "authors": ["Sahil  Verma", "John  Dickerson", "Keegan  Hines"], "year": 2021, "n_citations": 0}
{"id": 2204759, "s2_id": "72dd7f652072bb7de313e1f7d6451c3daf51e395", "title": "A Theoretical Investigation of Graph Degree as an Unsupervised Normality Measure", "abstract": "For a graph representation of a dataset, a straightforward normality measure for a sample can be its graph degree. Considering a weighted graph, this corresponds to sum of rows in a similarity matrix. The measure is intuitive given the abnormal samples are usually rare and they are dissimilar to the rest of the data. In order to have an in-depth theoretical understanding, in this manuscript, we investigate the graph degree in spectral graph clustering based and kernel based point of views and draw connections to a recent kernel method for the two sample problem. We show that our analyses guide us to choose fully-connected graphs whose edge weights are calculated via universal kernels. We show that a simple graph degree based unsupervised anomaly detection method with the above properties, achieves leading accuracy compared to other unsupervised anomaly detection methods on average over 10 widely used datasets. We also provide an extensive analysis on the effect of the kernel parameter on the method's accuracy.", "venue": "ArXiv", "authors": ["\u00c7aglar  Aytekin", "Francesco  Cricri", "Lixin  Fan", "Emre  Aksu"], "year": 2018, "n_citations": 0}
{"id": 2217353, "s2_id": "05734b8c7a1f692c5e9b68173467ba556c801f06", "title": "One TTS Alignment To Rule Them All", "abstract": "Speech-to-text alignment is a critical component of neural textto-speech (TTS) models. Autoregressive TTS models typically use an attention mechanism to learn these alignments on-line. However, these alignments tend to be brittle and often fail to generalize to long utterances and out-of-domain text, leading to missing or repeating words. Most non-autoregressive endto-end TTS models rely on durations extracted from external sources. In this paper we leverage the alignment mechanism proposed in RAD-TTS as a generic alignment learning framework, easily applicable to a variety of neural TTS models. The framework combines forward-sum algorithm, the Viterbi algorithm, and a simple and efficient static prior. In our experiments, the alignment learning framework improves all tested TTS architectures, both autoregressive (Flowtron, Tacotron 2) and non-autoregressive (FastPitch, FastSpeech 2, RAD-TTS). Specifically, it improves alignment convergence speed of existing attention-based mechanisms, simplifies the training pipeline, and makes the models more robust to errors on long utterances. Most importantly, the framework improves the perceived speech synthesis quality, as judged by human evaluators.", "venue": "ArXiv", "authors": ["Rohan  Badlani", "Adrian  Lancucki", "Kevin J. Shih", "Rafael  Valle", "Wei  Ping", "Bryan  Catanzaro"], "year": 2021, "n_citations": 3}
{"id": 2223382, "s2_id": "6fb5aaa44c3cb117a9864c7d727129d9f0a74a08", "title": "Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange", "abstract": "In this paper, we present a novel method for measurably adjusting the semantics of text while preserving its sentiment and fluency, a task we call semantic text exchange. This is useful for text data augmentation and the semantic correction of text generated by chatbots and virtual assistants. We introduce a pipeline called SMERTI that combines entity replacement, similarity masking, and text infilling. We measure our pipeline\u2019s success by its Semantic Text Exchange Score (STES): the ability to preserve the original text\u2019s sentiment and fluency while adjusting semantic content. We propose to use masking (replacement) rate threshold as an adjustable parameter to control the amount of semantic change in the text. Our experiments demonstrate that SMERTI can outperform baseline models on Yelp reviews, Amazon reviews, and news headlines.", "venue": "EMNLP", "authors": ["Steven Y. Feng", "Aaron W. Li", "Jesse  Hoey"], "year": 2019, "n_citations": 13}
{"id": 2260776, "s2_id": "f83f199abe1c9460b1bfff3c7d5914e95832ecbc", "title": "Technical Background for \"A Precision Medicine Approach to Develop and Internally Validate Optimal Exercise and Weight Loss Treatments for Overweight and Obese Adults with Knee Osteoarthritis\"", "abstract": "We provide additional statistical background for the methodology developed in the clinical analysis of knee osteoarthritis in \"A Precision Medicine Approach to Develop and Internally Validate Optimal Exercise and Weight Loss Treatments for Overweight and Obese Adults with Knee Osteoarthritis\" (Jiang et al. 2020). Jiang et al. 2020 proposed a pipeline to learn optimal treatment rules with precision medicine models and compared them with zero-order models with a Z-test. The model performance was based on value functions, a scalar that predicts the future reward of each decision rule. The jackknife (i.e., leave-one-out cross validation) method was applied to estimate the value function and its variance of several outcomes in IDEA. IDEA is a randomized clinical trial studying three interventions (exercise (E), dietary weight loss (D), and D+E) on overweight and obese participants with knee osteoarthritis. In this report, we expand the discussion and justification with additional statistical background. We elaborate more on the background of precision medicine, the derivation of the jackknife estimator of value function and its estimated variance, the consistency property of jackknife estimator, as well as additional simulation results that reflect more of the performance of jackknife estimators. We recommend reading Jiang et al. 2020 for clinical application and interpretation of the optimal ITR of knee osteoarthritis as well as the overall understanding of the pipeline and recommend using this article to understand the underlying statistical derivation and methodology.", "venue": "ArXiv", "authors": ["Xiaotong  Jiang", "Amanda E. Nelson", "Rebecca J. Cleveland", "Daniel P. Beavers", "Todd A. Schwartz", "Liubov  Arbeeva", "Carolina  Alvarez", "Leigh F. Callahan", "Stephen  Messier", "Richard  Loeser", "Michael R. Kosorok"], "year": 2020, "n_citations": 0}
{"id": 2264188, "s2_id": "58819982c9b0fd53c33901d8939b51a423b44dc5", "title": "Neural Hierarchical Factorization Machines for User's Event Sequence Analysis", "abstract": "Many prediction tasks of real-world applications need to model multi-order feature interactions in user's event sequence for better detection performance. However, existing popular solutions usually suffer two key issues: 1) only focusing on feature interactions and failing to capture the sequence influence; 2) only focusing on sequence information, but ignoring internal feature relations of each event, thus failing to extract a better event representation. In this paper, we consider a two-level structure for capturing the hierarchical information over user's event sequence: 1) learning effective feature interactions based event representation; 2) modeling the sequence representation of user's historical events. Experimental results on both industrial and public datasets clearly demonstrate that our model achieves significantly better performance compared with state-of-the-art baselines.", "venue": "SIGIR", "authors": ["Dongbo  Xi", "Fuzhen  Zhuang", "Bowen  Song", "Yongchun  Zhu", "Shuai  Chen", "Dan  Hong", "Tao  Chen", "Xi  Gu", "Qing  He"], "year": 2020, "n_citations": 6}
{"id": 2284325, "s2_id": "4b88329392a75287942d85f42012f47f356a2714", "title": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations", "abstract": "Recognizing analogies, synonyms, antonyms, and associations appear to be four distinct tasks, requiring distinct NLP algorithms. In the past, the four tasks have been treated independently, using a wide variety of algorithms. These four semantic classes, however, are a tiny sample of the full range of semantic phenomena, and we cannot afford to create ad hoc algorithms for each semantic phenomenon; we need to seek a unified approach. We propose to subsume a broad range of phenomena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonym questions, ESL synonym-antonym questions, and similar-associated-both questions from cognitive psychology.", "venue": "COLING", "authors": ["Peter D. Turney"], "year": 2008, "n_citations": 212}
{"id": 2290771, "s2_id": "2519eff9c308f627caec42cb730673a289627408", "title": "What Really is Deep Learning Doing?", "abstract": "Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective of mechanical learning and learning machine (see [1], [2]). From this particular angle, we can see deep learning much better and answer with confidence: What deep learning is really doing? why it works well, how it works, and how much data is necessary for learning. We also will discuss advantages and disadvantages of deep learning at the end of this work.", "venue": "ArXiv", "authors": ["Chuyu  Xiong"], "year": 2017, "n_citations": 4}
{"id": 2322458, "s2_id": "0f89596e6669fecb4b6f35ccafe791d3621a581f", "title": "Learning to Noise: Application-Agnostic Data Sharing with Local Differential Privacy", "abstract": "In recent years, the collection and sharing of individuals' private data has become commonplace in many industries. Local differential privacy (LDP) is a rigorous approach which uses a randomized algorithm to preserve privacy even from the database administrator, unlike the more standard central differential privacy. For LDP, when applying noise directly to high-dimensional data, the level of noise required all but entirely destroys data utility. In this paper we introduce a novel, application-agnostic privatization mechanism that leverages representation learning to overcome the prohibitive noise requirements of direct methods, while maintaining the strict guarantees of LDP. We further demonstrate that this privatization mechanism can be used to train machine learning algorithms across a range of applications, including private data collection, private novel-class classification, and the augmentation of clean datasets with additional privatized features. We achieve significant gains in performance on downstream classification tasks relative to benchmarks that noise the data directly, which are state-of-the-art in the context of application-agnostic LDP mechanisms for high-dimensional data.", "venue": "ArXiv", "authors": ["Alex  Mansbridge", "Gregory  Barbour", "Davide  Piras", "Christopher  Frye", "Ilya  Feige", "David  Barber"], "year": 2020, "n_citations": 0}
{"id": 2323821, "s2_id": "fc6198f6cbd342dae590126b8aa69fb5d139f08d", "title": "Inference-Time Personalized Federated Learning", "abstract": "In Federated learning (FL), multiple clients collaborate to learn a model through a central server but keep the data decentralized. Personalized federated learning (PFL) further extends FL to handle data heterogeneity between clients by learning personalized models. In both FL and PFL, all clients participate in the training process and their labeled data is used for training. However, in reality, novel clients may wish to join a prediction service after it has been deployed, obtaining predictions for their own unlabeled data. Here, we defined a new learning setup, Inference-Time PFL (IT-PFL), where a model trained on a set of clients, needs to be later evaluated on novel unlabeled clients at inference time. We propose a novel approach to this problem IT-PFLHN, based on a hypernetwork module and an encoder module. Specifically, we train an encoder network that learns a representation for a client given its unlabeled data. That client representation is fed to a hypernetwork that generates a personalized model for that client. Evaluated on four benchmark datasets, we find that IT-PFL-HN generalizes better than current FL and PFL methods, especially when the novel client has a large domain shift. We also analyzed the generalization error for the novel client, showing how it can be bounded using results from multi-task learning and domain adaptation. Finally, since novel clients do not contribute their data to training, they can potentially have better control over their data privacy; indeed, we showed analytically and experimentally how novel clients can apply differential privacy to their data.", "venue": "ArXiv", "authors": ["Ohad  Amosy", "Gal  Eyal", "Gal  Chechik"], "year": 2021, "n_citations": 0}
{"id": 2335431, "s2_id": "340e3ede45f12714d2522c94c5b08b1b33f16655", "title": "Predictive Inequity in Object Detection", "abstract": "In this work, we investigate whether state-of-the-art object detection systems have equitable predictive performance on pedestrians with different skin tones. This work is motivated by many recent examples of ML and vision systems displaying higher error rates for certain demographic groups than others. We annotate an existing large scale dataset which contains pedestrians, BDD100K, with Fitzpatrick skin tones in ranges [1-3] or [4-6]. We then provide an in-depth comparative analysis of performance between these two skin tone groupings, finding that neither time of day nor occlusion explain this behavior, suggesting this disparity is not merely the result of pedestrians in the 4-6 range appearing in more difficult scenes for detection. We investigate to what extent time of day, occlusion, and reweighting the supervised loss during training affect this predictive bias.", "venue": "ArXiv", "authors": ["Benjamin  Wilson", "Judy  Hoffman", "Jamie  Morgenstern"], "year": 2019, "n_citations": 91}
{"id": 2340538, "s2_id": "68afe02939e682c9384717331a63d44715590c67", "title": "Support Vector Machine Active Learning Algorithms with Query-by-Committee Versus Closest-to-Hyperplane Selection", "abstract": "This paper investigates and evaluates support vector machine active learning algorithms for use with imbalanced datasets, which commonly arise in many applications such as information extraction applications. Algorithms based on closestto- hyperplane selection and query-by-committee selection are combined with methods for addressing imbalance such as positive amplification based on prevalence statistics from initial random samples. Three algorithms (ClosestPA, QBagPA, and QBoostPA) are presented and carefully evaluated on datasets for text classification and relation extraction. The ClosestPA algorithm is shown to consistently outperform the other two in a variety of ways and insights are provided as to why this is the case.", "venue": "2018 IEEE 12th International Conference on Semantic Computing (ICSC)", "authors": ["Michael  Bloodgood"], "year": 2018, "n_citations": 18}
{"id": 2364521, "s2_id": "6942aa5539b0ca8534f1acdf195ac967db01dd30", "title": "Discount Factor as a Regularizer in Reinforcement Learning", "abstract": "Specifying a Reinforcement Learning (RL) task involves choosing a suitable planning horizon, which is typically modeled by a discount factor. It is known that applying RL algorithms with a lower discount factor can act as a regularizer, improving performance in the limited data regime. Yet the exact nature of this regularizer has not been investigated. In this work, we fill in this gap. For several Temporal-Difference (TD) learning methods, we show an explicit equivalence between using a reduced discount factor and adding an explicit regularization term to the algorithm\u2019s loss. Motivated by the equivalence, we empirically study this technique compared to standard L2 regularization by extensive experiments in discrete and continuous domains, using tabular and functional representations. Our experiments suggest the regularization effectiveness is strongly related to properties of the available data, such as size, distribution, and mixing rate.", "venue": "ICML", "authors": ["Ron  Amit", "Ron  Meir", "Kamil  Ciosek"], "year": 2020, "n_citations": 14}
{"id": 2370817, "s2_id": "682cedc3445e7182ced8f3c4981a4033f1267bc4", "title": "Oracle Teacher: Towards Better Knowledge Distillation", "abstract": "Knowledge distillation (KD), best known as an effective method for model compression, aims at transferring the knowledge of a bigger network (teacher) to a much smaller network (student). Conventional KD methods usually employ the teacher model trained in a supervised manner, where output labels are treated only as targets. Extending this supervised scheme further, we introduce a new type of teacher model for KD, namely Oracle Teacher, that utilizes the embeddings of both the source inputs and the output labels to extract a more accurate knowledge to be transferred to the student. The proposed model follows the encoder-decoder attention structure of the Transformer network, which allows the model to attend to related information from the output labels. Extensive experiments are conducted on three different sequence learning tasks: speech recognition, scene text recognition, and machine translation. From the experimental results, we empirically show that the proposed model improves the students across these tasks while achieving a considerable speed-up in the teacher model\u2019s training time.", "venue": "ArXiv", "authors": ["Ji Won Yoon", "Hyung Yong Kim", "Hyeonseung  Lee", "Sunghwan  Ahn", "Nam Soo Kim"], "year": 2021, "n_citations": 0}
{"id": 2374420, "s2_id": "de3a72d78b6f554c6686d5e04c3ea8180bc91176", "title": "KNN Learning Techniques for Proportional Myocontrol in Prosthetics", "abstract": "This work has been conducted in the context of pattern-recognition-based control for electromyographic prostheses. It presents a k-nearest neighbour (kNN) classification technique for gesture recognition, extended by a proportionality scheme. The methods proposed are practically implemented and validated. Datasets are captured by means of a state-of-theart 8-channel electromyography (EMG) armband positioned on the forearm. Based on this data, the influence of kNN\u2019s parameters is analyzed in pilot experiments. Moreover, the effect of proportionality scaling and rest thresholding schemes is investigated. A randomized, double-blind user study is conducted to compare the implemented method with the stateof-research algorithm Ridge Regression with Random Fourier Features (RR-RFF) for different levels of gesture exertion. The results from these experiments show a statistically significant improvement in favour of the kNN-based algorithm. I. MOTIVATION AND RELATED WORK The kNN learning scheme has been applied for myoelectric control of prosthetic devices several times [1]\u2013[4]. So far, kNN was utilized for sole classification as an intention detection method based on EMG signals. In preliminary experiments, kNN showed promising results in gesture detection referring to success rate (SR) as well as generalizability. It is considered as robust (i. a., against electrode shift [5] and sampling frequency variation [6]). Further benefits are the algorithm\u2019s incrementality and low level of implementation complexity. As kNN is an instance-based machine learning algorithm, training of an explicit model is not necessary. Since proportionality is a key feature in myocontrol, regression algorithms gain more popularity [7], [8]; moreover several attempts were made to combine classification concepts with proportional scaling in EMG-based intention detection: LDA [9], [10], neural networks [11], [12] and common spatial patterns [13] have been used to control the velocity based on the signal intensity. To the authors\u2019 knowledge, kNN was not adapted as a proportional scheme so far. In this study, we developed such a scheme investigating several modalities to include proportionality. II. CONCEPTUAL APPROACH It is assumed that the intensity of an exerted gesture is approximately proportional to the amplitude of the signal (mean of all channels of the 8-channel EMG signal). We intend to use this property for proportional scaling of gestures 1 T. Sziburis is with the High Precision Alignment Technologies Section, CERN, 1211 Geneva 23, Switzerland, and was with the German Aerospace Center (DLR), Robotics and Mechatronics Center (RMC), M\u00fcnchner Str. 20, 82234 We\u00dfling, Germany (tim.sziburis@cern.ch) 2 M. Nowak is with the German Aerospace Center (DLR), Robotics and Mechatronics Center (RMC), M\u00fcnchner Str. 20, 82234 We\u00dfling, Germany 3 D. Brunelli is with the Department of Industrial Engineering, DII, University of Trento, 38123 Trento, Italy classified by kNN. The rectified sensor reading is divided into a normalized signal and the signal strength (normalization factor), while the former is used for gesture prediction, and the latter for scaling under the assumption of linear correlation of signal strength and proportional intent for the particular gesture. The rest gesture is treated independently. The mean magnitude of the rest samples gathered during training (t0) is taken as baseline for rest. If the threshold t = g \u00b7 t0 (gain g) is exceeded, a gesture is not classified as rest anymore. An increase in t reduces unwanted activations. However, a higher level of t requires the user to exert higher forces to activate a gesture and therefore also leads to a lower resolution of proportionality. There is a tradeoff between suppressing unwanted activations and providing a high level of resolution (maintaining the maximum). We therefore introduce a divisor d which scales the proportionality function offset m0 = t d ; but not the threshold for rest t itself. Different configurations are tested in pilot tests. III. CONTROL METHOD ADAPTATION The training process comprises: 1) capturing training data, 2) calculating class magnitude means for proportionality scaling, 3) normalization of this data, 4) block-wise crossvalidation for obtaining the optimal k in terms of accuracy. The prediction process is structured as follows: 1) rest thresholding, 2) normalization of new sample, 3) calculating k nearest neighbours of the new sample, 4) applying distance weighting on the k selected neighbouring samples, 5) executing kNN classification by majority voting, and 6) signal magnitude analysis and scaling the prediction proportionally. For the preparation of the user study, we conducted a twostep method adaptation (analyzing seven actions: rest, power grasp, point, wrist flexion/extension, pronation/supination). First, we performed offline cross-validation to determine the most suited kNN parameters (k, distance metric and weighting factor). This was followed by a single-subject pilot to evaluate different ways of introducing proportionality.", "venue": "Biosystems & Biorobotics", "authors": ["Tim  Sziburis", "Markus  Nowak", "Davide  Brunelli"], "year": 2021, "n_citations": 1}
{"id": 2385471, "s2_id": "f47714b81c4e905460aa0b6ceb9c5257f0352b49", "title": "Texture Modeling with Convolutional Spike-and-Slab RBMs and Deep Extensions", "abstract": "We apply the spike-and-slab Restricted Boltzmann Machine (ssRBM) to texture modeling. The ssRBM with tiled-convolution weight sharing (TssRBM) achieves or surpasses the state-of-the-art on texture synthesis and inpainting by parametric models. We also develop a novel RBM model with a spike-and-slab visible layer and binary variables in the hidden layer. This model is designed to be stacked on top of the TssRBM. We show the resulting deep belief network (DBN) is a powerful generative model that improves on single-layer models and is capable of modeling not only single high-resolution and challenging textures but also multiple textures.", "venue": "AISTATS", "authors": ["Heng  Luo", "Pierre Luc Carrier", "Aaron C. Courville", "Yoshua  Bengio"], "year": 2013, "n_citations": 29}
{"id": 2387739, "s2_id": "42c62c03b68cf7d8eeef9337640564bcacb0174f", "title": "Towards Interpreting Recurrent Neural Networks through Probabilistic Abstraction", "abstract": "Neural networks are becoming a popular tool for solving many realworld problems such as object recognition and machine translation, thanks to its exceptional performance as an end-to-end solution. However, neural networks are complex black-box models, which hinders humans from interpreting and consequently trusting them in making critical decisions. Towards interpreting neural networks, several approaches have been proposed to extract simple deterministic models from neural networks. The results are not encouraging (e.g., low accuracy and limited scalability), fundamentally due to the limited expressiveness of such simple models.In this work, we propose an approach to extract probabilistic automata for interpreting an important class of neural networks, i.e., recurrent neural networks. Our work distinguishes itself from existing approaches in two important ways. One is that probability is used to compensate for the loss of expressiveness. This is inspired by the observation that human reasoning is often \u2018probabilistic\u2019. The other is that we adaptively identify the right level of abstraction so that a simple model is extracted in a request-specific way. We conduct experiments on several real-world datasets using state-of-the-art architectures including GRU and LSTM. The result shows that our approach significantly improves existing approaches in terms of accuracy or scalability. Lastly, we demonstrate the usefulness of the extracted models through detecting adversarial texts.", "venue": "2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)", "authors": ["Guoliang  Dong", "Jingyi  Wang", "Jun  Sun", "Yang  Zhang", "Xinyu  Wang", "Ting  Dai", "Jin Song Dong", "Xingen  Wang"], "year": 2020, "n_citations": 2}
{"id": 2391160, "s2_id": "5ed4617d39833a8dd8e931282ca2fcee136db634", "title": "Unsupervised Generative Modeling Using Matrix Product States", "abstract": "Generative modeling, which learns joint probability distribution from data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning analogous to the density matrix renormalization group method, which allows dynamically adjusting dimensions of the tensors and offers an efficient direct sampling approach for generative tasks. We apply our method to generative modeling of several standard datasets including the Bars and Stripes, random binary patterns and the MNIST handwritten digits to illustrate the abilities, features and drawbacks of our model over popular generative models such as Hopfield model, Boltzmann machines and generative adversarial networks. Our work sheds light on many interesting directions of future exploration on the development of quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to be realized on quantum devices.", "venue": "Physical Review X", "authors": ["Zhao-Yu  Han", "Jun  Wang", "Heng  Fan", "Lei  Wang", "Pan  Zhang"], "year": 2018, "n_citations": 135}
{"id": 2394355, "s2_id": "2737f9ce3b96b05c75cd3b86d1d5150d73568fc0", "title": "Employing an Adjusted Stability Measure for Multi-Criteria Model Fitting on Data Sets with Similar Features", "abstract": "Fitting models with high predictive accuracy that include all relevant but no irrelevant or redundant features is a challenging task on data sets with similar (e.g. highly correlated) features. We propose the approach of tuning the hyperparameters of a predictive model in a multicriteria fashion with respect to predictive accuracy and feature selection stability. We evaluate this approach based on both simulated and real data sets and we compare it to the standard approach of single-criteria tuning of the hyperparameters as well as to the state-of-the-art technique \u201cstability selection\u201d. We conclude that our approach achieves the same or better predictive performance compared to the two established approaches. Considering the stability during tuning does not decrease the predictive accuracy of the resulting models. Our approach succeeds at selecting the relevant features while avoiding irrelevant or redundant features. The single-criteria approach fails at avoiding irrelevant or redundant features and the stability selection approach fails at selecting enough relevant features for achieving acceptable predictive accuracy. For our approach, for data sets with many similar features, the feature selection stability must be evaluated with an adjusted stability measure, that is, a measure that considers similarities between features. For data sets with only few similar features, an unadjusted stability measure suffices and is faster to compute.", "venue": "ArXiv", "authors": ["Andrea  Bommert", "J\u00f6rg  Rahnenf\u00fchrer", "Michel  Lang"], "year": 2021, "n_citations": 0}
{"id": 2408566, "s2_id": "496f9df842ff1995933236fee3dc54a9023e4822", "title": "DAN: Dual-View Representation Learning for Adapting Stance Classifiers to New Domains", "abstract": "We address the issue of having a limited number of annotations for stance classification in a new domain, by adapting out-of-domain classifiers with domain adaptation. Existing approaches often align different domains in a single, global feature space (or view), which may fail to fully capture the richness of the languages used for expressing stances, leading to reduced adaptability on stance data. In this paper, we identify two major types of stance expressions that are linguistically distinct, and we propose a tailored dual-view adaptation network (DAN) to adapt these expressions across domains. The proposed model first learns a separate view for domain transfer in each expression channel and then selects the best adapted parts of both views for optimal transfer. We find that the learned view features can be more easily aligned and more stance-discriminative in either or both views, leading to more transferable overall features after combining the views. Results from extensive experiments show that our method can enhance the state-of-the-art single-view methods in matching stance data across different domains, and that it consistently improves those methods on various adaptation tasks.", "venue": "ECAI", "authors": ["Chang  Xu", "Cecile  Paris", "Surya  Nepal", "Ross  Sparks", "Chong  Long", "Yafang  Wang"], "year": 2020, "n_citations": 1}
{"id": 2423020, "s2_id": "68812b51d259ca4bcbf2ed3d3d18b6929e9cece4", "title": "Adversarial Robustness of Supervised Sparse Coding", "abstract": "Several recent results provide theoretical insights into the phenomena of adversarial examples. Existing results, however, are often limited due to a gap between the simplicity of the models studied and the complexity of those deployed in practice. In this work, we strike a better balance by considering a model that involves learning a representation while at the same time giving a precise generalization bound and a robustness certificate. We focus on the hypothesis class obtained by combining a sparsity-promoting encoder coupled with a linear classifier, and show an interesting interplay between the expressivity and stability of the (supervised) representation map and a notion of margin in the feature space. We bound the robust risk (to $\\ell_2$-bounded perturbations) of hypotheses parameterized by dictionaries that achieve a mild encoder gap on training data. Furthermore, we provide a robustness certificate for end-to-end classification. We demonstrate the applicability of our analysis by computing certified accuracy on real data, and compare with other alternatives for certified robustness.", "venue": "NeurIPS", "authors": ["Jeremias  Sulam", "Ramchandran  Muthumukar", "Raman  Arora"], "year": 2020, "n_citations": 7}
{"id": 2436420, "s2_id": "ede80f3dc75584f685708e6befff131931dcfb8b", "title": "Reducing the Long Tail Losses in Scientific Emulations with Active Learning", "abstract": "Deep-learning-based models are increasingly used to emulate scientific simulations to accelerate scientific research. However, accurate, supervised deep learning models require huge amount of labelled data, and that often becomes the bottleneck in employing neural networks. In this work, we leveraged an active learning approach called core-set selection to actively select data, per a pre-defined budget, to be labelled for training. To further improve the model performance and reduce the training costs, we also warm started the training using a shrink-and-perturb trick. We tested on two case studies in different fields, namely galaxy halo occupation distribution modelling in astrophysics and x-ray emission spectroscopy in plasma physics, and the results are promising: we achieved competitive overall performance compared to using a random sampling baseline, and more importantly, successfully reduced the larger absolute losses, i.e. the long tail in the loss distribution, at virtually no overhead costs.", "venue": "ArXiv", "authors": ["Yi Heng Lim", "Muhammad Firmansyah Kasim"], "year": 2021, "n_citations": 0}
{"id": 2444826, "s2_id": "4af072e5a420b38546a6908505f7f59031a2062d", "title": "Improved Covariance Matrix Estimator using Shrinkage Transformation and Random Matrix Theory", "abstract": "One of the major challenges in multivariate analysis is the estimation of population covariance matrix from sample covariance matrix (SCM). Most recent covariance matrix estimators use either shrinkage transformations or asymptotic results from Random Matrix Theory (RMT). Shrinkage techniques help in pulling extreme correlation values towards certain target values whereas tools from RMT help in removing noisy eigenvalues of SCM. Both of these techniques use different approaches to achieve a similar goal which is to remove noisy correlations and add structure to SCM to overcome the bias-variance trade-off. In this paper, we first critically evaluate the pros and cons of these two techniques and then propose an improved estimator which exploits the advantages of both by taking an optimally weighted convex combination of covariance matrices estimated by an improved shrinkage transformation and a RMT based filter. It is a generalized estimator which can adapt to changing sampling noise conditions in various datasets by performing hyperparameter optimization. We show the effectiveness of this estimator on the problem of designing a financial portfolio with minimum risk. We have chosen this problem because the complex properties of stock market data provide extreme conditions to test the robustness of a covariance estimator. Using data from four of the world's largest stock exchanges, we show that our proposed estimator outperforms existing estimators in minimizing the out-of-sample risk of the portfolio and hence predicts population statistics more precisely. Since covariance analysis is a crucial statistical tool, this estimator can be used in a wide range of machine learning, signal processing and high dimensional pattern recognition applications.", "venue": "ArXiv", "authors": ["Samruddhi  Deshmukh", "Amartansh  Dubey"], "year": 2019, "n_citations": 2}
{"id": 2447822, "s2_id": "6761232310c84a68ba08e526ac42f112c55f749f", "title": "Robust Distributed Optimization With Randomly Corrupted Gradients", "abstract": "In this paper, we propose a first-order distributed optimization algorithm that is provably robust to Byzantine failures\u2014arbitrary and potentially adversarial behavior, where all the participating agents are prone to failure. We model each agent\u2019s state over time as a two state Markov chain that indicates Byzantine or trustworthy behaviours at different time instants. We set no restrictions on the maximum number of Byzantine agents at any given time. We design our method based on three layers of defense: 1) Temporal gradient averaging, 2) robust aggregation, and 3) gradient normalization. We study two settings for stochastic optimization, namely Sample Average Approximation and Stochastic Approximation, and prove that for strongly convex and smooth non-convex cost functions, our algorithm achieves order-optimal statistical error and convergence rates.", "venue": "ArXiv", "authors": ["Berkay  Turan", "Cesar A. Uribe", "Hoi-To  Wai", "Mahnoosh  Alizadeh"], "year": 2021, "n_citations": 0}
{"id": 2449044, "s2_id": "d83f70bcc32b69f0cb2acc1232afa0df423f3e5e", "title": "Deep Learning for Hindi Text Classification: A Comparison", "abstract": "Natural Language Processing (NLP) and especially natural language text analysis have seen great advances in recent times. Usage of deep learning in text processing has revolutionized the techniques for text processing and achieved remarkable results. Different deep learning architectures like CNN, LSTM, and very recent Transformer have been used to achieve state of the art results variety on NLP tasks. In this work, we survey a host of deep learning architectures for text classification tasks. The work is specifically concerned with the classification of Hindi text. The research in the classification of morphologically rich and low resource Hindi language written in Devanagari script has been limited due to the absence of large labeled corpus. In this work, we used translated versions of English data-sets to evaluate models based on CNN, LSTM and Attention. Multilingual pre-trained sentence embeddings based on BERT and LASER are also compared to evaluate their effectiveness for the Hindi language. The paper also serves as a tutorial for popular text classification techniques.", "venue": "IHCI", "authors": ["Ramchandra  Joshi", "Purvi  Goel", "Raviraj  Joshi"], "year": 2019, "n_citations": 14}
{"id": 2451933, "s2_id": "0be73843210fc27a2d5895ec036828b878c607da", "title": "Brain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech Recognition", "abstract": "In this paper, we propose a deep learning-based algorithm to improve the performance of automatic speech recognition (ASR) systems for aphasia, apraxia, and dysarthria speech by utilizing electroencephalography (EEG) features recorded synchronously with aphasia, apraxia, and dysarthria speech. We demonstrate a significant decoding performance improvement by more than 50% during test time for isolated speech recognition task and we also provide preliminary results indicating performance improvement for the more challenging continuous speech recognition task by utilizing EEG features. The results presented in this paper show the first step towards demonstrating the possibility of utilizing non-invasive neural signals to design a real-time robust speech prosthetic for stroke survivors recovering from aphasia, apraxia, and dysarthria. Our aphasia, apraxia, and dysarthria speech-EEG data set will be released to the public to help further advance this interesting and crucial research.", "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)", "authors": ["Gautam  Krishna", "Mason  Carnahan", "Shilpa  Shamapant", "Yashitha  Surendranath", "Saumya  Jain", "Arundhati  Ghosh", "Co  Tran", "Jose del R Millan", "Ahmed H Tewfik"], "year": 2021, "n_citations": 0}
{"id": 2471883, "s2_id": "ec063bc7e9d3265a292e521cf827c9c9b005e38a", "title": "CSI Clustering with Variational Autoencoding", "abstract": "The model order of a wireless channel plays an important role for a variety of applications in communications engineering, e.g., it represents the number of resolvable incident wavefronts with non-negligible power incident from a transmitter to a receiver. Areas such as direction of arrival estimation leverage the model order to analyze the multipath components of channel state information. In this work, we propose to use a variational autoencoder to group unlabeled channel state information with respect to the model order in the variational autoencoder latent space in an unsupervised manner. We validate our approach with simulated 3GPP channel data. Our results suggest that, in order to learn an appropriate clustering, it is crucial to use a more flexible likelihood model for the variational autoencoder decoder than it is usually the case in standard applications.", "venue": "ArXiv", "authors": ["Michael  Baur", "Michael  W\u00fcrth", "Vlad-Costin  Andrei", "Michael  Koller", "Wolfgang  Utschick"], "year": 2021, "n_citations": 0}
{"id": 2474115, "s2_id": "8e5a30011f16715d3ca58a81cf0b2f96a2f26534", "title": "Architectural Implications of Graph Neural Networks", "abstract": "Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This letter tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.", "venue": "IEEE Computer Architecture Letters", "authors": ["Zhihui  Zhang", "Jingwen  Leng", "Lingxiao  Ma", "Youshan  Miao", "Chao  Li", "Minyi  Guo"], "year": 2020, "n_citations": 9}
{"id": 2479267, "s2_id": "6c2f88b24c2d183de00831a0b0c6ec909a0f61c2", "title": "Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System", "abstract": "The last-generation video conferencing software allows users to utilize a virtual background to conceal their personal environment due to privacy concerns, especially in official meetings with other employers. On the other hand, users maybe want to fool people in the meeting by considering the virtual background to conceal where they are. In this case, developing tools to understand the virtual background utilize for fooling people in meeting plays an important role. Besides, such detectors must prove robust against different kinds of attacks since a malicious user can fool the detector by applying a set of adversarial editing steps on the video to conceal any revealing footprint. In this paper, we study the feasibility of an efficient tool to detect whether a videoconferencing user background is real. In particular, we provide the first tool which computes pixel cooccurrences matrices and uses them to search for inconsistencies among spectral and spatial bands. Our experiments confirm that cross co-occurrences matrices improve the robustness of the detector against different kinds of attacks. This work\u2019s performance is especially noteworthy with regard to color SPAM features [1]. Moreover, the performance especially is significant with regard to robustness versus post-processing, like geometric transformations, filtering, contrast enhancement, and JPEG compression with different quality factors.", "venue": "ArXiv", "authors": ["Mauro  Conti", "Simone  Milani", "Ehsan  Nowroozi", "Gabriele  Orazi"], "year": 2021, "n_citations": 1}
{"id": 2487160, "s2_id": "c4a80f0fd7e0db37f47bbb6edd7fe68413edd0cb", "title": "Variational Transport: A Convergent Particle-BasedAlgorithm for Distributional Optimization", "abstract": "We consider the optimization problem of minimizing a functional defined over a family of probability distributions, where the objective functional is assumed to possess a variational form. Such a distributional optimization problem arises widely in machine learning and statistics, with MonteCarlo sampling, variational inference, policy optimization, and generative adversarial network as examples. For this problem, we propose a novel particle-based algorithm, dubbed as variational transport, which approximately performs Wasserstein gradient descent over the manifold of probability distributions via iteratively pushing a set of particles. Specifically, we prove that moving along the geodesic in the direction of functional gradient with respect to the second-order Wasserstein distance is equivalent to applying a pushforward mapping to a probability distribution, which can be approximated accurately by pushing a set of particles. Specifically, in each iteration of variational transport, we first solve the variational problem associated with the objective functional using the particles, whose solution yields the Wasserstein gradient direction. Then we update the current distribution by pushing each particle along the direction specified by such a solution. By characterizing both the statistical error incurred in estimating the Wasserstein gradient and the progress of the optimization algorithm, we prove that when the objective functional satisfies a functional version of the PolyakLojasiewicz (PL) (Polyak, 1963) and smoothness conditions, variational transport converges linearly to the global minimum of the objective functional up to a certain statistical error, which decays to zero sublinearly as the number of particles goes to infinity.", "venue": "ArXiv", "authors": ["Zhuoran  Yang", "Yufeng  Zhang", "Yongxin  Chen", "Zhaoran  Wang"], "year": 2020, "n_citations": 2}
{"id": 2492357, "s2_id": "4589378e0ee598606f848858004b1e036ee5ccba", "title": "Energy Storage Arbitrage in Real-Time Markets via Reinforcement Learning", "abstract": "In this paper, we derive a temporal arbitrage policy for storage via reinforcement learning. Real-time price arbitrage is an important source of revenue for storage units, but designing good strategies have proven to be difficult because of the highly uncertain nature of the prices. Instead of current model predictive or dynamic programming approaches, we use reinforcement learning to design an optimal arbitrage policy. This policy is learned through repeated charge and discharge actions performed by the storage unit through updating a value matrix. We design a reward function that does not only reflect the instant profit of charge/discharge decisions but also incorporate the history information. Simulation results demonstrate that our designed reward function leads to significant performance improvement compared with existing algorithms.", "venue": "2018 IEEE Power & Energy Society General Meeting (PESGM)", "authors": ["Hao  Wang", "Baosen  Zhang"], "year": 2018, "n_citations": 32}
{"id": 2494583, "s2_id": "729c5bd6f35d12ddc1312ec9dec60f091998a8bc", "title": "A Deep Learning Approach to Tongue Detection for Pediatric Population", "abstract": "Children with severe disabilities and complex communication needs face limitations in the usage of access technology (AT) devices. Conventional ATs (e.g., mechanical switches) can be insufficient for nonverbal children and those with limited voluntary motion control. Automatic techniques for the detection of tongue gestures represent a promising pathway. Previous studies have shown the robustness of tongue detection algorithms on adult participants, but further research is needed to use these methods with children. In this study, a network architecture for tongue-out gesture recognition was implemented and evaluated on videos recorded in a naturalistic setting when children were playing a video-game. A cascade object detector algorithm was used to detect the participants' faces, and an automated classification scheme for tongue gesture detection was developed using a convolutional neural network (CNN). In evaluation experiments conducted, the network was trained using adults and children's images. The network classification accuracy was evaluated using leave-one-subject-out cross-validation. Preliminary classification results obtained from the analysis of videos of five typically developing children showed an accuracy of up to 99% in predicting tongue-out gestures. Moreover, we demonstrated that using only children data for training the classifier yielded better performance than adult's one supporting the need for pediatric tongue gesture datasets.", "venue": "ArXiv", "authors": ["Javad Rahimipour Anaraki", "Silvia  Orlandi", "Tom  Chau"], "year": 2020, "n_citations": 1}
{"id": 2510829, "s2_id": "b783115f0ba40a6a83eee084799f765665285c23", "title": "Combinatorial 3D Shape Generation via Sequential Assembly", "abstract": "3D shape generation has drawn attention in computer vision and machine learning since it opens an inspiring way to designing or creating new objects. Existing methods, however, do not reflect an important aspect of human generation processes in real life -- we often create a 3D shape by sequentially assembling geometric primitives into a combinatorial configuration. In this work, we propose a new 3D shape generation algorithm that aims to create such a combinatorial configuration from a set of volumetric primitives. To tackle the exponential growth of feasible combinations in terms of the number of primitives, we adopt sequential model-based optimization. Our method sequentially assembles primitives by exploiting and exploring adequate regions that are constrained by the current primitive placements. The evaluation function conveys global structure guidance for the assembling process to follow. Experimental results demonstrate that our method successfully generates combinatorial objects and simulates more realistic generation processes. We also introduce a new dataset for combinatorial 3D shape generation.", "venue": "ArXiv", "authors": ["Jungtaek  Kim", "Hyunsoo  Chung", "Minsu  Cho", "Jaesik  Park"], "year": 2020, "n_citations": 2}
{"id": 2521743, "s2_id": "ea4f46178806f7c64734d49616b7485d7d31bbe6", "title": "Distributed dictionary learning", "abstract": "The paper studies distributed Dictionary Learning (DL) problems where the learning task is distributed over a multi-agent network with time-varying (nonsymmetric) connectivity. This formulation is relevant, for instance, in Big Data scenarios where massive amounts of data are collected/stored in different spatial locations and it is unfeasible to aggregate and/or process all data in a fusion center, due to resource limitations, communication overhead or privacy considerations. We develop a general distributed algorithmic framework for the (nonconvex) DL problem and establish its asymptotic convergence. The new method hinges on Successive Convex Approximation (SCA) techniques coupled with i) a gradient tracking mechanism instrumental to locally estimate the missing global information; and ii) a consensus step, as a mechanism to distribute the computations among the agents. To the best of our knowledge, this is the first distributed algorithm with provable convergence for the DL problem and, more in general, bi-convex optimization problems over (time-varying) directed graphs.", "venue": "2016 50th Asilomar Conference on Signals, Systems and Computers", "authors": ["Amir  Daneshmand", "Gesualdo  Scutari", "Francisco  Facchinei"], "year": 2016, "n_citations": 8}
{"id": 2529077, "s2_id": "2bf44f9cbda46943771b77a10e4b093da1fb8a49", "title": "A Sequential Self Teaching Approach for Improving Generalization in Sound Event Recognition", "abstract": "An important problem in machine auditory perception is to recognize and detect sound events. In this paper, we propose a sequential self-teaching approach to learning sounds. Our main proposition is that it is harder to learn sounds in adverse situations such as from weakly labeled and/or noisy labeled data, and in these situations a single stage of learning is not sufficient. Our proposal is a sequential stage-wise learning process that improves generalization capabilities of a given modeling system. We justify this method via technical results and on Audioset, the largest sound events dataset, our sequential learning approach can lead to up to 9% improvement in performance. A comprehensive evaluation also shows that the method leads to improved transferability of knowledge from previously trained models, thereby leading to improved generalization capabilities on transfer learning tasks.", "venue": "ICML", "authors": ["Anurag  Kumar", "Vamsi Krishna Ithapu"], "year": 2020, "n_citations": 16}
{"id": 2531583, "s2_id": "153012061f7e90f5daa708ed068cea82181136a8", "title": "Quantum Multiple Kernel Learning", "abstract": "Kernel methods play an important role in machine learning applications due to their conceptual simplicity and superior performance on numerous machine learning tasks. Expressivity of a machine learning model, referring to the ability of the model to approximate complex functions, has a significant influence on its performance in these tasks. One approach to enhancing the expressivity of kernel machines is to combine multiple individual kernels to arrive at a more expressive combined kernel. This approach is referred to as multiple kernel learning (MKL). In this work, we propose an MKL method we refer to as quantum MKL, which combines multiple quantum kernels. Our method leverages the power of deterministic quantum computing with one qubit (DQC1) to estimate the combined kernel for a set of classically intractable individual quantum kernels. The combined kernel estimation is achieved without explicitly computing each individual kernel, while still allowing for the tuning of individual kernels in order to achieve better expressivity. Our simulations on two binary classification problems---one performed on a synthetic dataset and the other on a German credit dataset---demonstrate the superiority of the quantum MKL method over single quantum kernel machines.", "venue": "ArXiv", "authors": ["Seyed Shakib Vedaie", "Moslem  Noori", "Jaspreet S. Oberoi", "Barry C. Sanders", "Ehsan  Zahedinejad"], "year": 2020, "n_citations": 0}
{"id": 2565514, "s2_id": "03fdf61c7d7aeeca06f572399ca455a2c4e69bf6", "title": "Discovering Imperfectly Observable Adversarial Actions using Anomaly Detection", "abstract": "Anomaly detection is a method for discovering unusual and suspicious behavior. In many real-world scenarios, the examined events can be directly linked to the actions of an adversary, such as attacks on computer networks or frauds in financial operations. While the defender wants to discover such malicious behavior, the attacker seeks to accomplish their goal (e.g., exfiltrating data) while avoiding the detection. To this end, anomaly detectors have been used in a game-theoretic framework that captures these goals of a two-player competition. We extend the existing models to more realistic settings by (1) allowing both players to have continuous action spaces and by assuming that (2) the defender cannot perfectly observe the action of the attacker. We propose two algorithms for solving such games -- a direct extension of existing algorithms based on discretizing the feature space and linear programming and the second algorithm based on constrained learning. Experiments show that both algorithms are applicable for cases with low feature space dimensions but the learning-based method produces less exploitable strategies and it is scalable to higher dimensions. Moreover, we use real-world data to compare our approaches with existing classifiers in a data-exfiltration scenario via the DNS channel. The results show that our models are significantly less exploitable by an informed attacker.", "venue": "AAMAS", "authors": ["Olga  Petrova", "Karel  Durkota", "Galina  Alperovich", "Karel  Horak", "Michal  Najman", "Branislav  Bosansky", "Viliam  Lisy"], "year": 2020, "n_citations": 1}
{"id": 2569511, "s2_id": "efd4901ded8c13e998e75e4ddda127390f39fcc2", "title": "Optimal Gradient-based Algorithms for Non-concave Bandit Optimization", "abstract": "Bandit problems with linear or concave reward have been extensively studied, but relatively few works have studied bandits with non-concave reward. This work considers a large family of bandit problems where the unknown underlying reward function is non-concave, including the low-rank generalized linear bandit problems and two-layer neural network with polynomial activation bandit problem. For the low-rank generalized linear bandit problem, we provide a minimax-optimal algorithm in the dimension, refuting both conjectures in [LMT21, JWWN19]. Our algorithms are based on a unified zeroth-order optimization paradigm that applies in great generality and attains optimal rates in several structured polynomial settings (in the dimension). We further demonstrate the applicability of our algorithms in RL in the generative model setting, resulting in improved sample complexity over prior approaches. Finally, we show that the standard optimistic algorithms (e.g., UCB) are sub-optimal by dimension factors. In the neural net setting (with polynomial activation functions) with noiseless reward, we provide a bandit algorithm with sample complexity equal to the intrinsic algebraic dimension. Again, we show that optimistic approaches have worse sample complexity, polynomial in the extrinsic dimension (which could be exponentially worse in the polynomial degree).", "venue": "ArXiv", "authors": ["Baihe  Huang", "Kaixuan  Huang", "Sham M. Kakade", "Jason D. Lee", "Qi  Lei", "Runzhe  Wang", "Jiaqi  Yang"], "year": 2021, "n_citations": 1}
{"id": 2577707, "s2_id": "0d4f766064ccdee3ac970b936a63a6a7f136b6b8", "title": "Riemannian stochastic quasi-Newton algorithm with variance reduction and its convergence analysis", "abstract": "Stochastic variance reduction algorithms have recently become popular for minimizing the average of a large, but finite number of loss functions. The present paper proposes a Riemannian stochastic quasi-Newton algorithm with variance reduction (R-SQN-VR). The key challenges of averaging, adding, and subtracting multiple gradients are addressed with notions of retraction and vector transport. We present convergence analyses of R-SQN-VR on both non-convex and retraction-convex functions under retraction and vector transport operators. The proposed algorithm is evaluated on the Karcher mean computation on the symmetric positive-definite manifold and the low-rank matrix completion on the Grassmann manifold. In all cases, the proposed algorithm outperforms the state-of-the-art Riemannian batch and stochastic gradient algorithms.", "venue": "AISTATS", "authors": ["Hiroyuki  Kasai", "Hiroyuki  Sato", "Bamdev  Mishra"], "year": 2018, "n_citations": 13}
{"id": 2578558, "s2_id": "2ed604e502ce47f2b30e5dac26968bf86b519792", "title": "Paying Attention to Function Words", "abstract": "All natural languages exhibit a distinction between content words (like nouns and adjectives) and function words (like determiners, auxiliaries, prepositions). Yet surprisingly little has been said about the emergence of this universal architectural feature of natural languages. Why have human languages evolved to exhibit this division of labor between content and function words? How could such a distinction have emerged in the first place? This paper takes steps towards answering these questions by showing how the distinction can emerge through reinforcement learning in agents playing a signaling game across contexts which contain multiple objects that possess multiple perceptually salient gradable properties.", "venue": "ArXiv", "authors": ["Shane  Steinert-Threlkeld"], "year": 2019, "n_citations": 1}
{"id": 2585502, "s2_id": "b86c6626b758a0f53823ad0f7c578d692afafef5", "title": "Handling adversarial concept drift in streaming data", "abstract": "Abstract Classifiers operating in a dynamic, real world environment, are vulnerable to adversarial activity, which causes the data distribution to change over time. These changes are traditionally referred to as concept drift, and several approaches have been developed in literature to deal with the problem of drift detection and handling. However, most concept drift handling techniques approach it as a domain independent task, to make them applicable to a wide gamut of reactive systems. These techniques are developed from an adversarial agnostic perspective, where they naively assume that adversarial activity is like any other change to the data, which can be fixed by retraining the models. However, this is not the case when a malicious agent is trying to evade the deployed classification system. In such an environment, the properties of concept drift are unique, as the drift is intended to degrade the system and at the same time designed to avoid detection by traditional concept drift detection techniques. This special category of drift is termed as adversarial drift, and this paper analyzes its characteristics and impact in a streaming environment. A novel framework for dealing with adversarial concept drift is proposed, called the Predict-Detect streaming framework. This framework uses adversarial forethought and incorporates the context of classification into the drift detection task, to provide leverage in dynamic-adversarial domains. Experimental evaluation of the framework, on generated adversarial drifting data streams, demonstrates that this framework is able to provide early and reliable unsupervised indication of drift, and is able to recover from drifts swiftly. While traditional drift detectors can be evaded by intelligent adversaries, the proposed framework is especially designed to capture adversaries by misdirecting them into revealing themselves. In addition, the framework is designed to work on imbalanced and sparsely labeled data streams, as a limited-memory, incremental algorithm. The generic design and domain independent nature of the framework makes it applicable as a blueprint for developers wanting to implement reactive security to their classification based systems.", "venue": "Expert Syst. Appl.", "authors": ["Tegjyot Singh Sethi", "Mehmed M. Kantardzic"], "year": 2018, "n_citations": 31}
{"id": 2596577, "s2_id": "06721153cbe68be457d589ba6dd46b5fb335030e", "title": "Drug-Drug Interaction Prediction Based on Knowledge Graph Embeddings and Convolutional-LSTM Network", "abstract": "Interference between pharmacological substances can cause serious medical injuries. Correctly predicting so-called drug-drug interactions (DDI) does not only reduce these cases but can also result in a reduction of drug development cost. Presently, most drug-related knowledge is the result of clinical evaluations and post-marketing surveillance; resulting in a limited amount of information. Existing data-driven prediction approaches for DDIs typically rely on a single source of information, while using information from multiple sources would help improve predictions. Machine learning (ML) techniques are used, but the techniques are often unable to deal with skewness in the data. Hence, we propose a new ML approach for predicting DDIs based on multiple data sources. For this task, we use 12,000 drug features from DrugBank, PharmGKB, and KEGG drugs, which are integrated using Knowledge Graphs (KGs). To train our prediction model, we first embed the nodes in the graph using various embedding approaches. We found that the best performing combination was a ComplEx embedding method creating using PyTorch-BigGraph (PBG) with a Convolutional-LSTM network and classic machine learning-based prediction models. The model averaging ensemble method of three best classifiers yields up to 0.94, 0.92, 0.80 for AUPR, F1 F1-score, and MCC, respectively during 5-fold cross-validation tests.", "venue": "BCB", "authors": ["Md. Rezaul Karim", "Michael  Cochez", "Joao Bosco Jares", "Mamtaz  Uddin", "Oya  Beyan", "Stefan  Decker"], "year": 2019, "n_citations": 29}
{"id": 2601729, "s2_id": "f13a1cbc325671059a84bdc754b4c4dbd29f7e2f", "title": "VarArray: Array-Geometry-Agnostic Continuous Speech Separation", "abstract": "Continuous speech separation using a microphone array was shown to be promising in dealing with the speech overlap problem in natural conversation transcription. This paper proposes VarArray, an arraygeometry-agnostic speech separation neural network model. The proposed model is applicable to any number of microphones without retraining while leveraging the nonlinear correlation between the input channels. The proposed method adapts different elements that were proposed before separately, including transform-averageconcatenate, conformer speech separation, and inter-channel phase differences, and combines them in an efficient and cohesive way. Large-scale evaluation was performed with two real meeting transcription tasks by using a fully developed transcription system requiring no prior knowledge such as reference segmentations, which allowed us to measure the impact that the continuous speech separation system could have in realistic settings. The proposed model outperformed a previous approach to array-geometry-agnostic modeling for all of the geometry configurations considered, achieving asclite-based speaker-agnostic word error rates of 17.5% and 20.4% for the AMI development and evaluation sets, respectively, in the end-to-end setting using no ground-truth segmentations.", "venue": "ArXiv", "authors": ["Takuya  Yoshioka", "Xiaofei  Wang", "Dongmei  Wang", "Min  Tang", "Zirun  Zhu", "Zhuo  Chen", "Naoyuki  Kanda"], "year": 2021, "n_citations": 1}
{"id": 2601934, "s2_id": "40768023af4f6689743ce484ff34714c88f2e129", "title": "A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization", "abstract": "We consider the zeroth-order optimization problem in the huge-scale setting, where the dimension of the problem is so large that performing even basic vector operations on the decision variables is infeasible. In this paper, we propose a novel algorithm, coined ZO-BCD, that exhibits favorable overall query complexity and has a much smaller per-iteration computational complexity. In addition, we discuss how the memory footprint of ZO-BCD can be reduced even further by the clever use of circulant measurement matrices. As an application of our new method, we propose the idea of crafting adversarial attacks on neural network based classifiers in a wavelet domain, which can result in problem dimensions of over one million. In particular, we show that crafting adversarial examples to audio classifiers in a wavelet domain can achieve the state-of-the-art attack success rate of 97.9% with significantly less distortion.", "venue": "ICML", "authors": ["HanQin  Cai", "Yuchen  Lou", "Daniel  McKenzie", "Wotao  Yin"], "year": 2021, "n_citations": 4}
{"id": 2605761, "s2_id": "977bfee08c9fdcbfd19210caf014109a9eee1ea0", "title": "TransICD: Transformer Based Code-wise Attention Model for Explainable ICD Coding", "abstract": "International Classification of Disease (ICD) coding procedure which refers to tagging medical notes with diagnosis codes has been shown to be effective and crucial to the billing system in medical sector. Currently, ICD codes are assigned to a clinical note manually which is likely to cause many errors. Moreover, training skilled coders also requires time and human resources. Therefore, automating the ICD code determination process is an important task. With the advancement of artificial intelligence theory and computational hardware, machine learning approach has emerged as a suitable solution to automate this process. In this project, we apply a transformer-based architecture to capture the interdependence among the tokens of a document and then use a code-wise attention mechanism to learn code-specific representations of the entire document. Finally, they are fed to separate dense layers for corresponding code prediction. Furthermore, to handle the imbalance in the code frequency of clinical datasets, we employ a label distribution aware margin (LDAM) loss function. The experimental results on the MIMIC-III dataset show that our proposed model outperforms other baselines by a significant margin. In particular, our best setting achieves a micro-AUC score of 0.923 compared to 0.868 of bidirectional recurrent neural networks. We also show that by using the code-wise attention mechanism, the model can provide more insights about its prediction, and thus it can support clinicians to make reliable decisions. Our code is available online.", "venue": "AIME", "authors": ["Biplob  Biswas", "Thai-Hoang  Pham", "Ping  Zhang"], "year": 2021, "n_citations": 2}
{"id": 2606346, "s2_id": "06a0573c91643f06ba388b019322543d1237f461", "title": "The Integrity of Machine Learning Algorithms against Software Defect Prediction", "abstract": "The increased computerization in recent years has resulted in the production of a variety of different software, however measures need to be taken to ensure that the produced software isn't defective. Many researchers have worked in this area and have developed different Machine Learning-based approaches that predict whether the software is defective or not. This issue can't be resolved simply by using different conventional classifiers because the dataset is highly imbalanced i.e the number of defective samples detected is extremely less as compared to the number of non-defective samples. Therefore, to address this issue, certain sophisticated methods are required. The different methods developed by the researchers can be broadly classified into Resampling based methods, Cost-sensitive learning-based methods, and Ensemble Learning. Among these methods. This report analyses the performance of the Online Sequential Extreme Learning Machine (OS-ELM) proposed by Liang this http URL. against several classifiers such as Logistic Regression, Support Vector Machine, Random Forest, and Naive Bayes after oversampling the data. OS-ELM trains faster than conventional deep neural networks and it always converges to the globally optimal solution. A comparison is performed on the original dataset as well as the over-sampled data set. The oversampling technique used is Cluster-based Over-Sampling with Noise Filtering. This technique is better than several state-of-the-art techniques for oversampling. The analysis is carried out on 3 projects KC1, PC4 and PC3 carried out by the NASA group. The metrics used for measurement are recall and balanced accuracy. The results are higher for OS-ELM as compared to other classifiers in both scenarios.", "venue": "ArXiv", "authors": ["Param  Khakhar", "Rahul Kumar Dubey"], "year": 2020, "n_citations": 0}
{"id": 2626761, "s2_id": "037aa837d95b5f0edef494a2392b1788dc840a47", "title": "A Multi-Perspective Architecture for Semantic Code Search", "abstract": "The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multi-perspective cross-lingual neural framework for code\u2013text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space.", "venue": "ACL", "authors": ["Rajarshi  Haldar", "Lingfei  Wu", "Jinjun  Xiong", "Julia  Hockenmaier"], "year": 2020, "n_citations": 7}
{"id": 2659869, "s2_id": "3bf249f716a384065443abc6172f4bdef88738d9", "title": "A Hybrid Instance-based Transfer Learning Method", "abstract": "In recent years, supervised machine learning models have demonstrated tremendous success in a variety of application domains. Despite the promising results, these successful models are data hungry and their performance relies heavily on the size of training data. However, in many healthcare applications it is difficult to collect sufficiently large training datasets. Transfer learning can help overcome this issue by transferring the knowledge from readily available datasets (source) to a new dataset (target). In this work, we propose a hybrid instance-based transfer learning method that outperforms a set of baselines including state-of-the-art instance-based transfer learning approaches. Our method uses a probabilistic weighting strategy to fuse information from the source domain to the model learned in the target domain. Our method is generic, applicable to multiple source domains, and robust with respect to negative transfer. We demonstrate the effectiveness of our approach through extensive experiments for two different applications.", "venue": "ArXiv", "authors": ["Azin  Asgarian", "Parinaz  Sobhani", "Ji Chao Zhang", "Madalin  Mihailescu", "Ariel  Sibilia", "Ahmed Bilal Ashraf", "Babak  Taati"], "year": 2018, "n_citations": 9}
{"id": 2660291, "s2_id": "84413b968c3f8c612ffeb0b027997ee49654a586", "title": "Provable repair of deep neural networks", "abstract": "Deep Neural Networks (DNNs) have grown in popularity over the past decade and are now being used in safety-critical domains such as aircraft collision avoidance. This has motivated a large number of techniques for finding unsafe behavior in DNNs. In contrast, this paper tackles the problem of correcting a DNN once unsafe behavior is found. We introduce the provable repair problem, which is the problem of repairing a network N to construct a new network N\u2032 that satisfies a given specification. If the safety specification is over a finite set of points, our Provable Point Repair algorithm can find a provably minimal repair satisfying the specification, regardless of the activation functions used. For safety specifications addressing convex polytopes containing infinitely many points, our Provable Polytope Repair algorithm can find a provably minimal repair satisfying the specification for DNNs using piecewise-linear activation functions. The key insight behind both of these algorithms is the introduction of a Decoupled DNN architecture, which allows us to reduce provable repair to a linear programming problem. Our experimental results demonstrate the efficiency and effectiveness of our Provable Repair algorithms on a variety of challenging tasks.", "venue": "PLDI", "authors": ["Matthew  Sotoudeh", "Aditya V. Thakur"], "year": 2021, "n_citations": 4}
{"id": 2661376, "s2_id": "648dce875272ba601b36a164a10648decdb3044d", "title": "Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing", "abstract": "A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.", "venue": "ICML", "authors": ["Sanghamitra  Dutta", "Dennis  Wei", "Hazar  Yueksel", "Pin-Yu  Chen", "Sijia  Liu", "Kush R. Varshney"], "year": 2020, "n_citations": 28}
{"id": 2670063, "s2_id": "803d666bbf038e7e3bb723c8a652ad8c7bde4e75", "title": "A Graph-Based Machine Learning Approach for Bot Detection", "abstract": "Bot detection using machine learning (ML), with network flow-level features, has been extensively studied in the literature. However, existing flow-based approaches typically incur a high computational overhead and do not completely capture the network communication patterns, which can expose additional aspects of malicious hosts. Recently, bot detection systems which leverage communication graph analysis using ML have gained attention to overcome these limitations. A graph-based approach is rather intuitive, as graphs are true representations of network communications. In this paper, we propose a two-phased, graph-based bot detection system which leverages both unsupervised and supervised ML. The first phase prunes presumable benign hosts, while the second phase achieves bot detection with high precision. Our system detects multiple types of bots and is robust to zero-day attacks. It also accommodates different network topologies and is suitable for large-scale data.", "venue": "2019 IFIP/IEEE Symposium on Integrated Network and Service Management (IM)", "authors": ["Abbas Abou Daya", "Mohammad Ali Salahuddin", "Noura  Limam", "Raouf  Boutaba"], "year": 2019, "n_citations": 25}
{"id": 2673149, "s2_id": "c3e810c7818a26290866fe1dcd851495890be775", "title": "Byzantine-Robust and Privacy-Preserving Framework for FedML", "abstract": "Federated learning has emerged as a popular paradigm for collaboratively training a model from data distributed among a set of clients. This learning setting presents, among others, two unique challenges: how to protect privacy of the clients' data during training, and how to ensure integrity of the trained model. We propose a two-pronged solution that aims to address both challenges under a single framework. First, we propose to create secure enclaves using a trusted execution environment (TEE) within the server. Each client can then encrypt their gradients and send them to verifiable enclaves. The gradients are decrypted within the enclave without the fear of privacy breaches. However, robustness check computations in a TEE are computationally prohibitive. Hence, in the second step, we perform a novel gradient encoding that enables TEEs to encode the gradients and then offloading Byzantine check computations to accelerators such as GPUs. Our proposed approach provides theoretical bounds on information leakage and offers a significant speed-up over the baseline in empirical evaluation.", "venue": "ArXiv", "authors": ["Hanieh  Hashemi", "Yongqin  Wang", "Chuan  Guo", "Murali  Annavaram"], "year": 2021, "n_citations": 5}
{"id": 2673889, "s2_id": "5994ea254d8e60e9b240cf0c3824b3e5f40df85a", "title": "Machine Learning for Indoor Localization Using Mobile Phone-Based Sensors", "abstract": "In this paper we investigate the problem of localizing a mobile device based on readings from its embedded sensors utilizing machine learning methodologies. We consider a realworld environment, collect a large dataset of 3110 datapoints, and examine the performance of a substantial number of machine learning algorithms in localizing a mobile device. We have found algorithms that give a mean error as accurate as 0.76 meters, outperforming other indoor localization systems reported in the literature. We also propose a hybrid instance-based approach that results in a speed increase by a factor of ten with no loss of accuracy in a live deployment over standard instance-based methods, allowing for fast and accurate localization. Further, we determine how smaller datasets collected with less density affect accuracy of localization, important for use in real-world environments. Finally, we demonstrate that these approaches are appropriate for real-world deployment by evaluating their performance in an online, in-motion experiment.", "venue": "ArXiv", "authors": ["David  Mascharka", "Eric D. Manley"], "year": 2015, "n_citations": 12}
{"id": 2701491, "s2_id": "7daf15f8fad8c8ffb0d510a064e0fc5eed58ac8f", "title": "Distributed Preemption Decisions: Probabilistic Graphical Model, Algorithm and Near-Optimality", "abstract": "Cooperative decision making is a vision of future network management and control. Distributed connection preemption is an important example where nodes can make intelligent decisions on allocating resources and controlling traffic flows for multi-class service networks. A challenge is that nodal decisions are spatially dependent as traffic flows trespass multiple nodes in a network. Hence the performance-complexity trade-off becomes important, i.e., how accurate decisions are versus how much information is exchanged among nodes. Connection preemption is known to be NP-complete. Centralized preemption is optimal but computationally intractable. Decentralized preemption is computationally efficient but may result in a poor performance. This work investigates distributed preemption where nodes decide whether and which flows to preempt using only local information exchange with neighbors. We develop, based on the probabilistic graphical models, a near-optimal distributed algorithm. The algorithm is used by each node to make collectively near-optimal preemption decisions. We study trade-offs between near-optimal performance and complexity that corresponds to the amount of information-exchange of the distributed algorithm. The algorithm is validated by both analysis and simulation.", "venue": "ArXiv", "authors": ["Sung-eok  Jeon", "Chuanyi  Ji"], "year": 2009, "n_citations": 0}
{"id": 2724605, "s2_id": "ab8157352f8f5e7ff9b49cac9386e457ed78a189", "title": "Semi-supervised Pathology Segmentation with Disentangled Representations", "abstract": "Automated pathology segmentation remains a valuable diagnostic tool in clinical practice. However, collecting training data is challenging. Semi-supervised approaches by combining labelled and unlabelled data can offer a solution to data scarcity. An approach to semi-supervised learning relies on reconstruction objectives (as self-supervision objectives) that learns in a joint fashion suitable representations for the task. Here, we propose Anatomy-Pathology Disentanglement Network (APD-Net), a pathology segmentation model that attempts to learn jointly for the first time: disentanglement of anatomy, modality, and pathology. The model is trained in a semi-supervised fashion with new reconstruction losses directly aiming to improve pathology segmentation with limited annotations. In addition, a joint optimization strategy is proposed to fully take advantage of the available annotations. We evaluate our methods with two private cardiac infarction segmentation datasets with LGE-MRI scans. APD-Net can perform pathology segmentation with few annotations, maintain performance with different amounts of supervision, and outperform related deep learning methods.", "venue": "DART/DCL@MICCAI", "authors": ["Haochuan  Jiang", "Agisilaos  Chartsias", "Xinheng  Zhang", "Giorgos  Papanastasiou", "Scott  Semple", "Mark  Dweck", "David  Semple", "Rohan  Dharmakumar", "Sotirios A. Tsaftaris"], "year": 2020, "n_citations": 1}
{"id": 2732562, "s2_id": "e702da6dd47efa254743aacc90aa690a35e614cc", "title": "Sparse Tensor Additive Regression", "abstract": "Tensors are becoming prevalent in modern applications such as medical imaging and digital marketing. In this paper, we propose a sparse tensor additive regression (STAR) that models a scalar response as a flexible nonparametric function of tensor covariates. The proposed model effectively exploits the sparse and low-rank structures in the tensor additive regression. We formulate the parameter estimation as a non-convex optimization problem, and propose an efficient penalized alternating minimization algorithm. We establish a non-asymptotic error bound for the estimator obtained from each iteration of the proposed algorithm, which reveals an interplay between the optimization error and the statistical rate of convergence. We demonstrate the efficacy of STAR through extensive comparative simulation studies, and an application to the click-through-rate prediction in online advertising.", "venue": "ArXiv", "authors": ["Botao  Hao", "Boxiang  Wang", "Pengyuan  Wang", "Jingfei  Zhang", "Jian  Yang", "Will Wei Sun"], "year": 2019, "n_citations": 9}
{"id": 2764691, "s2_id": "796d3aeb78efdfd197824444cc8f252ff90a609f", "title": "Escaping the Big Data Paradigm with Compact Transformers", "abstract": "With the rise of Transformers as the standard for language processing, and their advancements in computer vision, along with their unprecedented size and amounts of training data, many have come to believe that they are not suitable for small sets of data. This trend leads to great concerns, including but not limited to: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we dispel the myth that transformers are \u201ddata hungry\u201d and therefore can only be applied to large sets of data. We show for the first time that with the right size and tokenization, transformers can perform head-to-head with state-of-the-art CNNs on small datasets. Our model eliminates the requirement for class token and positional embeddings through a novel sequence pooling strategy and the use of convolutions. We show that compared to CNNs, our compact transformers have fewer parameters and MACs, while obtaining similar accuracies. Our method is flexible in terms of model size, and can have as little as 0.28M parameters and achieve reasonable results. It can reach an accuracy of 95.29 % when training from scratch on CIFAR-10, which is comparable with modern CNN based approaches, and a significant improvement over previous Transformer based models. Our simple and compact design democratizes transformers by making them accessible to those equipped with basic computing resources and/or dealing with important small datasets. Our method works on larger datasets, such as ImageNet (80.28% accuracy with 29% parameters of ViT), and NLP tasks as well.", "venue": "ArXiv", "authors": ["Ali  Hassani", "Steven  Walton", "Nikhil  Shah", "Abulikemu  Abuduweili", "Jiachen  Li", "Humphrey  Shi"], "year": 2021, "n_citations": 23}
{"id": 2764936, "s2_id": "2f793af60d17591e3d26ce120d8bea05caa6541e", "title": "OGNet: Towards a Global Oil and Gas Infrastructure Database using Deep Learning on Remotely Sensed Imagery", "abstract": "At least a quarter of the warming that the Earth is experiencing today is due to anthropogenic methane emissions. There are multiple satellites in orbit and planned for launch in the next few years which can detect and quantify these emissions; however, to attribute methane emissions to their sources on the ground, a comprehensive database of the locations and characteristics of emission sources worldwide is essential. In this work, we develop deep learning algorithms that leverage freely available high-resolution aerial imagery to automatically detect oil and gas infrastructure, one of the largest contributors to global methane emissions. We use the best algorithm, which we call OGNet, together with expert review to identify the locations of oil refineries and petroleum terminals in the U.S. We show that OGNet detects many facilities which are not present in four standard public datasets of oil and gas infrastructure. All detected facilities are associated with characteristics known to contribute to methane emissions, including the infrastructure type and the number of storage tanks. The data curated and produced in this study is freely available at this http URL .", "venue": "ArXiv", "authors": ["Hao  Sheng", "Jeremy  Irvin", "Sasankh  Munukutla", "Shawn  Zhang", "Christopher  Cross", "Kyle  Story", "Rose  Rustowicz", "Cooper  Elsworth", "Zutao  Yang", "Mark  Omara", "Ritesh  Gautam", "Robert B. Jackson", "Andrew Y. Ng"], "year": 2020, "n_citations": 2}
{"id": 2769840, "s2_id": "2f40314c1b0f20e7ae91903ab582b9976ab3f39e", "title": "Multi-level Stress Assessment from ECG in a Virtual Reality Environment using Multimodal Fusion", "abstract": "ECG is an attractive option to assess stress in serious Virtual Reality (VR) applications due to its non-invasive nature. However, the existing Machine Learning (ML) models perform poorly. Moreover, existing studies only perform a binary stress assessment, while to develop a more engaging biofeedback-based application, multi-level assessment is necessary. Existing studies annotate and classify a single experience (e.g. watching a VR video) to a single stress level, which again prevents design of dynamic experiences where real-time in-game stress assessment can be utilized. In this paper, we report our findings on a new study on VR stress assessment, where three stress levels are assessed. ECG data was collected from 9 users experiencing a VR roller coaster. The VR experience was then manually labeled in 10-seconds segments to three stress levels by three raters. We then propose a novel multimodal deep fusion model utilizing spectrogram and 1D ECG that can provide a stress prediction from just a 1-second window. Experimental results demonstrate that the proposed model outperforms the classical HRV-based ML models (9% increase in accuracy) and baseline deep learning models (2.5% increase in accuracy). We also report results on the benchmark WESAD dataset to show the supremacy of the model.", "venue": "ArXiv", "authors": ["Zeeshan  Ahmad", "Suha  Rabbani", "Muhammad Rehman Zafar", "Syem  Ishaque", "Sridhar  Krishnan", "Naimul  Khan"], "year": 2021, "n_citations": 0}
{"id": 2781993, "s2_id": "2a0ec0b70e334dc2cbefa7793426daa1e10e9be3", "title": "Imitation Learning with Recurrent Neural Networks", "abstract": "We present a novel view that unifies two frameworks that aim to solve sequential prediction problems: learning to search (L2S) and recurrent neural networks (RNN). We point out equivalences between elements of the two frameworks. By complementing what is missing from one framework comparing to the other, we introduce a more advanced imitation learning framework that, on one hand, augments L2S s notion of search space and, on the other hand, enhances RNNs training procedure to be more robust to compounding errors arising from training on highly correlated examples.", "venue": "ArXiv", "authors": ["Khanh  Nguyen"], "year": 2016, "n_citations": 4}
{"id": 2782895, "s2_id": "39ee7f84a10d8bf74a9aec0ad829e1da483bf5ac", "title": "Generating Adversarial Examples with an Optimized Quality", "abstract": "Deep learning models are widely used in a range of application areas, such as computer vision, computer security, etc. However, deep learning models are vulnerable to Adversarial Examples (AEs),carefully crafted samples to deceive those models. Recent studies have introduced new adversarial attack methods, but, to the best of our knowledge, none provided guaranteed quality for the crafted examples as part of their creation, beyond simple quality measures such as Misclassification Rate (MR). In this paper, we incorporateImage Quality Assessment (IQA) metrics into the design and generation process of AEs. We propose an evolutionary-based single- and multi-objective optimization approaches that generate AEs with high misclassification rate and explicitly improve the quality, thus indistinguishability, of the samples, while perturbing only a limited number of pixels. In particular, several IQA metrics, including edge analysis, Fourier analysis, and feature descriptors, are leveraged into the process of generating AEs. Unique characteristics of the evolutionary-based algorithm enable us to simultaneously optimize the misclassification rate and the IQA metrics of the AEs. In order to evaluate the performance of the proposed method, we conduct intensive experiments on different well-known benchmark datasets(MNIST, CIFAR, GTSRB, and Open Image Dataset V5), while considering various objective optimization configurations. The results obtained from our experiments, when compared with the exist-ing attack methods, validate our initial hypothesis that the use ofIQA metrics within generation process of AEs can substantially improve their quality, while maintaining high misclassification rate.Finally, transferability and human perception studies are provided, demonstrating acceptable performance.", "venue": "ArXiv", "authors": ["Aminollah  Khormali", "DaeHun  Nyang", "David  Mohaisen"], "year": 2020, "n_citations": 0}
{"id": 2791452, "s2_id": "a645028c9d6bf1175ee0c246e59fe18360a38670", "title": "GraphReach: Position-Aware Graph Neural Network using Reachability Estimations", "abstract": "Majority of the existing graph neural networks (GNN) learn node embeddings that encode their local neighborhoods but not their positions. Consequently, two nodes that are vastly distant but located in similar local neighborhoods map to similar embeddings in those networks. This limitation prevents accurate performance in predictive tasks that rely on position information. In this paper, we develop GRAPHREACH, a position-aware inductive GNN that captures the global positions of nodes through reachability estimations with respect to a set of anchor nodes. The anchors are strategically selected so that reachability estimations across all the nodes are maximized. We show that this combinatorial anchor selection problem is NP-hard and, consequently, develop a greedy (1\u22121/e) approximation heuristic. Empirical evaluation against state-of-the-art GNN architectures reveal that GRAPHREACH provides up to 40% relative improvement in accuracy. In addition, it is more robust to adversarial attacks.", "venue": "IJCAI", "authors": ["Sunil  Nishad", "Shubhangi  Agarwal", "Arnab  Bhattacharya", "Sayan  Ranu"], "year": 2021, "n_citations": 3}
{"id": 2794989, "s2_id": "c3fe3504df6cbefe5bcc3a8e05768f41a6268360", "title": "Stochastic Variance Reduced Riemannian Eigensolver", "abstract": "We study the stochastic Riemannian gradient algorithm for matrix eigen-decomposition. The state-of-the-art stochastic Riemannian algorithm requires the learning rate to decay to zero and thus suffers from slow convergence and sub-optimal solutions. In this paper, we address this issue by deploying the variance reduction (VR) technique of stochastic gradient descent (SGD). The technique was originally developed to solve convex problems in the Euclidean space. We generalize it to Riemannian manifolds and realize it to solve the non-convex eigen-decomposition problem. We are the first to propose and analyze the generalization of SVRG to Riemannian manifolds. Specifically, we propose the general variance reduction form, SVRRG, in the framework of the stochastic Riemannian gradient optimization. It's then specialized to the problem with eigensolvers and induces the SVRRG-EIGS algorithm. We provide a novel and elegant theoretical analysis on this algorithm. The theory shows that a fixed learning rate can be used in the Riemannian setting with an exponential global convergence rate guaranteed. The theoretical results make a significant improvement over existing studies, with the effectiveness empirically verified.", "venue": "ArXiv", "authors": ["Zhiqiang  Xu", "Yiping  Ke"], "year": 2016, "n_citations": 6}
{"id": 2807434, "s2_id": "7647a06965d868a4f6451bef0818994100a142e8", "title": "Empower Sequence Labeling with Task-Aware Neural Language Model", "abstract": "Linguistic sequence labeling is a general modeling approach that encompasses a variety of problems, such as part-of-speech tagging and named entity recognition. Recent advances in neural networks (NNs) make it possible to build reliable models without handcrafted features. However, in many cases, it is hard to obtain sufficient annotations to train these models. In this study, we develop a novel neural framework to extract abundant knowledge hidden in raw texts to empower the sequence labeling task. Besides word-level knowledge contained in pre-trained word embeddings, character-aware neural language models are incorporated to extract character-level knowledge. Transfer learning techniques are further adopted to mediate different components and guide the language model towards the key knowledge. Comparing to previous methods, these task-specific knowledge allows us to adopt a more concise model and conduct more efficient training. Different from most transfer learning methods, the proposed framework does not rely on any additional supervision. It extracts knowledge from self-contained order information of training sequences. Extensive experiments on benchmark datasets demonstrate the effectiveness of leveraging character-level knowledge and the efficiency of co-training. For example, on the CoNLL03 NER task, model training completes in about 6 hours on a single GPU, reaching F1 score of 91.71$\\pm$0.10 without using any extra annotation.", "venue": "AAAI", "authors": ["Liyuan  Liu", "Jingbo  Shang", "Frank F. Xu", "Xiang  Ren", "Huan  Gui", "Jian  Peng", "Jiawei  Han"], "year": 2018, "n_citations": 227}
{"id": 2809003, "s2_id": "459b03612760372039a00bd3248dac00cdc23e22", "title": "Accelerated Almost-Sure Convergence Rates for Nonconvex Stochastic Gradient Descent using Stochastic Learning Rates", "abstract": "Large-scale optimization problems require algorithms both effective and efficient. One such popular and proven algorithm is Stochastic Gradient Descent which uses first-order gradient information to solve these problems. This paper studies almostsure convergence rates of the Stochastic Gradient Descent method when instead of deterministic, its learning rate becomes stochastic. In particular, its learning rate is equipped with a multiplicative stochasticity, producing a stochastic learning rate scheme. Theoretical results show accelerated almost-sure convergence rates of Stochastic Gradient Descent in a nonconvex setting when using an appropriate stochastic learning rate, compared to a deterministic-learning-rate scheme. The theoretical results are verified empirically.", "venue": "ArXiv", "authors": ["Theodoros  Mamalis", "Dusan  Stipanovic", "Petros  Voulgaris"], "year": 2021, "n_citations": 1}
{"id": 2832520, "s2_id": "4bbb23119c815e643e453b72f2ef10f46da58eb4", "title": "Modeling Event Propagation via Graph Biased Temporal Point Process", "abstract": "Temporal point process is widely used for sequential data modeling. In this article, we focus on the problem of modeling sequential event propagation in graph, such as retweeting by social network users and news transmitting between websites. Given a collection of event propagation sequences, the conventional point process model considers only the event history, i.e., embed event history into a vector, not the latent graph structure. We propose a graph biased temporal point process (GBTPP) leveraging the structural information from graph representation learning, where the direct influence between nodes and indirect influence from event history is modeled. Moreover, the learned node embedding vector is also integrated into the embedded event history as side information. Experiments on a synthetic data set and two real-world data sets show the efficacy of our model compared with conventional methods and state-of-the-art ones.", "venue": "IEEE transactions on neural networks and learning systems", "authors": ["Weichang  Wu", "Huanxi  Liu", "Xiaohu  Zhang", "Yu  Liu", "Hongyuan  Zha"], "year": 2020, "n_citations": 13}
{"id": 2847648, "s2_id": "0f329e4b2d7bd066a0961ce607d8a6cbe36aecea", "title": "Music Generation with Temporal Structure Augmentation", "abstract": "In this paper we introduce a novel feature augmentation approach for generating structured musical compositions comprising melodies and harmonies. The proposed method augments a connectionist generation model with count-down to song conclusion and meter markers as extra input features to study whether neural networks can learn to produce more aesthetically pleasing and structured musical output as a consequence of augmenting the input data with structural features. An RNN architecture with LSTM cells is trained on the Nottingham folk music dataset in a supervised sequence learning setup, following a Music Language Modelling approach, and then applied to generation of harmonies and melodies. Our experiments show an improved prediction performance for both types of annotation. The generated music was also subjectively evaluated using an on-line Turing style listening test which confirms a substantial improvement in the aesthetic quality and in the perceived structure of the music generated using the temporal structure.", "venue": "ArXiv", "authors": ["Shakeel  Raja"], "year": 2020, "n_citations": 1}
{"id": 2882821, "s2_id": "39957461f7e9aa5a43398e652952132aa91adaf0", "title": "Constrained Instance and Class Reweighting for Robust Learning under Label Noise", "abstract": "Deep neural networks have shown impressive performance in supervised learning, enabled by their ability to fit well to the provided training data. However, their performance is largely dependent on the quality of the training data and often degrades in the presence of noise. We propose a principled approach for tackling label noise with the aim of assigning importance weights to individual instances and class labels. Our method works by formulating a class of constrained optimization problems that yield simple closed form updates for these importance weights. The proposed optimization problems are solved per mini-batch which obviates the need of storing and updating the weights over the full dataset. Our optimization framework also provides a theoretical perspective on existing label smoothing heuristics for addressing label noise (such as label bootstrapping). We evaluate our method on several benchmark datasets and observe considerable performance gains in the presence of label noise.", "venue": "ArXiv", "authors": ["Abhishek  Kumar", "Ehsan  Amid"], "year": 2021, "n_citations": 1}
{"id": 2895248, "s2_id": "d1efc73871819d4be944e23cecf5c71498bca6ea", "title": "Clustering without Over-Representation", "abstract": "In this paper we consider clustering problems in which each point is endowed with a color. The goal is to cluster the points to minimize the classical clustering cost but with the additional constraint that no color is over-represented in any cluster. This problem is motivated by practical clustering settings, e.g., in clustering news articles where the color of an article is its source, it is preferable that no single news source dominates any cluster. For the most general version of this problem, we obtain an algorithm that has provable guarantees of performance; our algorithm is based on finding a fractional solution using a linear program and rounding the solution subsequently. For the special case of the problem where no color has an absolute majority in any cluster, we obtain a simpler combinatorial algorithm also with provable guarantees. Experiments on real-world data shows that our algorithms are effective in finding good clustering without over-representation.", "venue": "KDD", "authors": ["Sara  Ahmadian", "Alessandro  Epasto", "Ravi  Kumar", "Mohammad  Mahdian"], "year": 2019, "n_citations": 40}
{"id": 2900098, "s2_id": "adc1d9d0e0086238c8000c960fe49761a0de46c2", "title": "Discrete Graph Structure Learning for Forecasting Multiple Time Series", "abstract": "Time series forecasting is an extensively studied subject in statistics, economics, and computer science. Exploration of the correlation and causation among the variables in a multivariate time series shows promise in enhancing the performance of a time series model. When using deep neural networks as forecasting models, we hypothesize that exploiting the pairwise information among multiple (multivariate) time series also improves their forecast. If an explicit graph structure is known, graph neural networks (GNNs) have been demonstrated as powerful tools to exploit the structure. In this work, we propose learning the structure simultaneously with the GNN if the graph is unknown. We cast the problem as learning a probabilistic graph model through optimizing the mean performance over the graph distribution. The distribution is parameterized by a neural network so that discrete graphs can be sampled differentiably through reparameterization. Empirical evaluations show that our method is simpler, more efficient, and better performing than a recently proposed bilevel learning approach for graph structure learning, as well as a broad array of forecasting models, either deep or non-deep learning based, and graph or non-graph based.", "venue": "ICLR", "authors": ["Chao  Shang", "Jie  Chen", "Jinbo  Bi"], "year": 2021, "n_citations": 7}
{"id": 2908462, "s2_id": "d054ee22d4349df88d1d09e7dcf781230bc27596", "title": "Evaluating the Effect of Longitudinal Dose and INR Data on Maintenance Warfarin Dose Predictions", "abstract": "Warfarin, a commonly prescribed drug to prevent blood clots, has a highly variable individual response. Determining a maintenance warfarin dose that achieves a therapeutic blood clotting time, as measured by the international normalized ratio (INR), is crucial in preventing complications. Machine learning algorithms are increasingly being used for warfarin dosing; usually, an initial dose is predicted with clinical and genotype factors, and revised after a few days based on previous doses and current INR. Since sequential dose and INR better capture individual differences in warfarin response, we hypothesized that longitudinal dose response data will improve maintenance dose predictions. To test this hypothesis, we analyzed a dataset from the COAG warfarin dosing clinical trial, which includes clinical data, warfarin dosing history and INR measurements over the study period, and maintenance dose when therapeutic INR was achieved. Various regression models were trained to predict the maintenance warfarin dose using clinical factors, warfarin dosing history and INR data. Dose revision algorithms that used a single dose and INR measurement achieved comparable performance as the baseline revision algorithm. In contrast, dose revision algorithms that used longitudinal dose and INR data predicted doses that were statistically significantly much closer to the true maintenance dose. With the best performing model, gradient boosting (GB), the proportion of ideal estimated dose, i.e., within \u00b120% of the true dose, increased from baseline (54.92%) to that with GB with single (63.11%) and longitudinal (75.41%) INR. More accurate maintenance dose predictions with longitudinal dose response data can potentially achieve therapeutic INR faster, reduce drug-related complications and improve patient outcomes with warfarin therapy.", "venue": "2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)", "authors": ["Anish  Karpurapu", "Adam  Krekorian", "Ye  Tian", "Leslie M. Collins", "Ravi  Karra", "Aaron  Franklin", "Boyla O. Mainsah"], "year": 2021, "n_citations": 0}
{"id": 2947836, "s2_id": "4f76f373bc257c5ca09ec5c443b54c25a8cb6ad4", "title": "Combining heterogeneous classifiers for relational databases", "abstract": "Practical usage of machine learning is gaining strategic importance in enterprises looking for business intelligence. However, most enterprise data is distributed in multiple relational databases with expert-designed schema. Using traditional single-table machine learning techniques over such data not only incur a computational penalty for converting to a flat form (mega-join), even the human-specified semantic information present in the relations is lost. In this paper, we present a practical, two-phase hierarchical meta-classification algorithm for relational databases with a semantic divide and conquer approach. We propose a recursive, prediction aggregation technique over heterogeneous classifiers applied on individual database tables. The proposed algorithm was evaluated on three diverse datasets, namely TPCH, PKDD and UCI benchmarks and showed considerable reduction in classification time without any loss of prediction accuracy.", "venue": "Pattern Recognit.", "authors": ["Geetha  Manjunath", "M. Narasimha Murty", "Dinkar  Sitaram"], "year": 2013, "n_citations": 11}
{"id": 2947962, "s2_id": "59a0a18a204d1e89ca29aff54cbc732b4d5bb5b5", "title": "Discrete-Continuous ADMM for Transductive Inference in Higher-Order MRFs", "abstract": "This paper introduces a novel algorithm for transductive inference in higher-order MRFs, where the unary energies are parameterized by a variable classifier. The considered task is posed as a joint optimization problem in the continuous classifier parameters and the discrete label variables. In contrast to prior approaches such as convex relaxations, we propose an advantageous decoupling of the objective function into discrete and continuous subproblems and a novel, efficient optimization method related to ADMM. This approach preserves integrality of the discrete label variables and guarantees global convergence to a critical point. We demonstrate the advantages of our approach in several experiments including video object segmentation on the DAVIS data set and interactive image segmentation.", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "authors": ["Emanuel  Laude", "Jan-Hendrik  Lange", "Jonas  Sch\u00fcpfer", "Csaba  Domokos", "Laura  Leal-Taix\u00e9", "Frank R. Schmidt", "Bjoern  Andres", "Daniel  Cremers"], "year": 2018, "n_citations": 5}
{"id": 2972624, "s2_id": "b84668a1e6dd0d9fd9ab410768e1f8135a56f785", "title": "Graph-based Normalizing Flow for Human Motion Generation and Reconstruction", "abstract": "Data-driven approaches for modeling human skeletal motion have found various applications in interactive media and social robotics. Challenges remain in these fields for generating high-fidelity samples and robustly reconstructing motion from imperfect input data, due to e.g. missed marker detection. In this paper, we propose a probabilistic generative model to synthesize and reconstruct long horizon motion sequences conditioned on past information and control signals, such as the path along which an individual is moving. Our method adapts the existing work MoGlow by introducing a new graph-based model. The model leverages the spatial-temporal graph convolutional network (ST-GCN) to effectively capture the spatial structure and temporal correlation of skeletal motion data at multiple scales. We evaluate the models on a mixture of motion capture datasets of human locomotion with foot-step and bone-length analysis. The results demonstrate the advantages of our model in reconstructing missing markers and achieving comparable results on generating realistic future poses. When the inputs are imperfect, our model shows improvements on robustness of generation.", "venue": "2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN)", "authors": ["Wenjie  Yin", "Hang  Yin", "Danica  Kragic", "M\u00e5rten  Bj\u00f6rkman"], "year": 2021, "n_citations": 0}
{"id": 2975640, "s2_id": "d8f9dc49ac9115bfc1a0e2731ce552970141a888", "title": "Learning Compact Convolutional Neural Networks with Nested Dropout", "abstract": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.", "venue": "ICLR", "authors": ["Chelsea  Finn", "Lisa Anne Hendricks", "Trevor  Darrell"], "year": 2015, "n_citations": 2}
{"id": 2988239, "s2_id": "9d51b79ed7e418910cdc16d139c270434ac25e0a", "title": "Hamilton-Jacobi-Bellman Equations for Q-Learning in Continuous Time", "abstract": "In this paper, we introduce Hamilton-Jacobi-Bellman (HJB) equations for Q-functions in continuous time optimal control problems with Lipschitz continuous controls. The standard Q-function used in reinforcement learning is shown to be the unique viscosity solution of the HJB equation. A necessary and sufficient condition for optimality is provided using the viscosity solution framework. By using the HJB equation, we develop a Q-learning method for continuous-time dynamical systems. A DQN-like algorithm is also proposed for high-dimensional state and control spaces. The performance of the proposed Q-learning algorithm is demonstrated using 1-, 10- and 20-dimensional dynamical systems.", "venue": "L4DC", "authors": ["Jeongho  Kim", "Insoon  Yang"], "year": 2020, "n_citations": 3}
{"id": 3009386, "s2_id": "d879286020ce9ecfa6cfed8426f5e7e9b441fce1", "title": "Scalable Planning with Deep Neural Network Learned Transition Models", "abstract": "In many real-world planning problems with factored, mixed discrete and continuous state and action spaces such as Reservoir Control, Heating Ventilation, and Air Conditioning, and Navigation domains, it is difficult to obtain a model of the complex nonlinear dynamics that govern state evolution. However, the ubiquity of modern sensors allows us to collect large quantities of data from each of these complex systems and build accurate, nonlinear deep neural network models of their state transitions. But there remains one major problem for the task of control -- how can we plan with deep network learned transition models without resorting to Monte Carlo Tree Search and other black-box transition model techniques that ignore model structure and do not easily extend to mixed discrete and continuous domains? In this paper, we introduce two types of nonlinear planning methods that can leverage deep neural network learned transition models: Hybrid Deep MILP Planner (HD-MILP-Plan) and Tensorflow Planner (TF-Plan). In HD-MILP-Plan, we make the critical observation that the Rectified Linear Unit transfer function for deep networks not only allows faster convergence of model learning, but also permits a direct compilation of the deep network transition model to a Mixed-Integer Linear Program encoding. Further, we identify deep network specific optimizations for HD-MILP-Plan that improve performance over a base encoding and show that we can plan optimally with respect to the learned deep networks. In TF-Plan, we take advantage of the efficiency of auto-differentiation tools and GPU-based computation where we encode a subclass of purely continuous planning problems as Recurrent Neural Networks and directly optimize the actions through backpropagation. We compare both planners and show that TF-Plan is able to approximate the optimal plans found by HD-MILP-Plan in less computation time...", "venue": "J. Artif. Intell. Res.", "authors": ["Ga  Wu", "Buser  Say", "Scott  Sanner"], "year": 2020, "n_citations": 11}
{"id": 3039049, "s2_id": "5998cc7cd9c2b3e8e269c6328148ea38b11c2975", "title": "Video Contrastive Learning with Global Context", "abstract": "Contrastive learning has revolutionized the self-supervised image representation learning field and recently been adapted to the video domain. One of the greatest advantages of contrastive learning is that it allows us to flexibly define powerful loss objectives as long as we can find a reasonable way to formulate positive and negative samples to contrast. However, existing approaches rely heavily on the short-range spatiotemporal salience to form clip-level contrastive signals, thus limit themselves from using global context. In this paper, we propose a new video-level contrastive learning method based on segments to formulate positive pairs. Our formulation is able to capture the global context in a video, thus robust to temporal content change. We also incorporate a temporal order regularization term to enforce the inherent sequential structure of videos. Extensive experiments show that our video-level contrastive learning framework (VCLR) is able to outperform previous state-of-the-arts on five video datasets for downstream action classification, action localization, and video retrieval.", "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "authors": ["Haofei  Kuang", "Yi  Zhu", "Zhi  Zhang", "Xinyu  Li", "Joseph  Tighe", "Soren  Schwertfeger", "Cyrill  Stachniss", "Mu  Li"], "year": 2021, "n_citations": 1}
{"id": 3039864, "s2_id": "7c8a0eaa9342f12b35898ad5c916ee034646523e", "title": "Active discovery of network roles for predicting the classes of network nodes", "abstract": "Nodes in real world networks often have class labels, or underlying attributes, that are related to the way in which they connect to other nodes. Sometimes this relationship is simple, for instance nodes of the same class are may be more likely to be connected. In other cases, however, this is not true, and the way that nodes link in a network exhibits a different, more complex relationship to their attributes. Here, we consider networks in which we know how the nodes are connected, but we do not know the class labels of the nodes or how class labels relate to the network links. We wish to identify the best subset of nodes to label in order to learn this relationship between node attributes and network links. We can then use this discovered relationship to accurately predict the class labels of the rest of the network nodes. \nWe present a model that identifies groups of nodes with similar link patterns, which we call network roles, using a generative blockmodel. The model then predicts labels by learning the mapping from network roles to class labels using a maximum margin classifier. We choose a subset of nodes to label according to an iterative margin-based active learning strategy. By integrating the discovery of network roles with the classifier optimisation, the active learning process can adapt the network roles to better represent the network for node classification. We demonstrate the model by exploring a selection of real world networks, including a marine food web and a network of English words. We show that, in contrast to other network classifiers, this model achieves good classification accuracy for a range of networks with different relationships between class labels and network links.", "venue": "J. Complex Networks", "authors": ["Leto  Peel"], "year": 2015, "n_citations": 13}
{"id": 3041228, "s2_id": "80d043f4b791ec83f07697ae2f34607eec48511d", "title": "Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps", "abstract": "Many-to-one maps are ubiquitous in machine learning, from the image recognition model that assigns a multitude of distinct images to the concept of \u201ccat\u201d to the time series forecasting model which assigns a range of distinct time-series to a single scalar regression value. While the primary use of such models is naturally to associate correct output to each input, in many problems it is also useful to be able to explore, understand, and sample from a model\u2019s fibers, which are the set of input values x such that f(x) = y, for fixed y in the output space. In this paper we show that popular generative architectures are ill-suited to such tasks. Motivated by this we introduce a novel generative architecture, a Bundle Network, based on the concept of a fiber bundle from (differential) topology. BundleNets exploit the idea of a local trivialization wherein a space can be locally decomposed into a product space that cleanly encodes the many-to-one nature of the map. By enforcing this decomposition in BundleNets and by utilizing state-of-the-art invertible components, investigating a network\u2019s fibers becomes natural.", "venue": "ArXiv", "authors": ["Nico  Courts", "Henry  Kvinge"], "year": 2021, "n_citations": 0}
{"id": 3049539, "s2_id": "a686497e9cc6d2abdc4e4c0a9e9b91872bad02ba", "title": "Detection of gravitational waves using topological data analysis and convolutional neural network: An improved approach", "abstract": "The gravitational wave detection problem is challenging because the noise is typically overwhelming. Convolutional neural networks (CNNs) have been successfully applied, but require a large training set and the accuracy suffers significantly in the case of low SNR. We propose an improved method that employs a feature extraction step using persistent homology. The resulting method is more resilient to noise, more capable of detecting signals with varied signatures and requires less training. This is a powerful improvement as the detection problem can be computationally intense and is concerned with a relatively large class of wave signatures.", "venue": "ArXiv", "authors": ["Christopher  Bresten", "Jae-Hun  Jung"], "year": 2019, "n_citations": 3}
{"id": 3099396, "s2_id": "6792310247db7afb37c98232922ce55f9a1123d2", "title": "On the Inductive Bias of Word-Character-Level Multi-Task Learning for Speech Recognition", "abstract": "End-to-end automatic speech recognition (ASR) commonly transcribes audio signals into sequences of characters while its performance is evaluated by measuring the word-error rate (WER). This suggests that predicting sequences of words directly may be helpful instead. However, training with word-level supervision can be more difficult due to the sparsity of examples per label class. In this paper we analyze an end-to-end ASR model that combines a word-and-character representation in a multi-task learning (MTL) framework. We show that it improves on the WER and study how the word-level model can benefit from character-level supervision by analyzing the learned inductive preference bias of each model component empirically. We find that by adding character-level supervision, the MTL model interpolates between recognizing more frequent words (preferred by the word-level model) and shorter words (preferred by the character-level model).", "venue": "ArXiv", "authors": ["Jan  Kremer", "Lasse  Borgholt", "Lars  Maal\u00f8e"], "year": 2018, "n_citations": 5}
{"id": 3105137, "s2_id": "36048164e05bb36ed47d66cd6005a8bcd004bdb8", "title": "Dual-to-kernel learning with ideals", "abstract": "In this paper, we propose a theory which unifies kernel learning and symbolic algebraic methods. We show that both worlds are inherently dual to each other, and we use this duality to combine the structure-awareness of algebraic methods with the efficiency and generality of kernels. The main idea lies in relating polynomial rings to feature space, and ideals to manifolds, then exploiting this generative-discriminative duality on kernel matrices. We illustrate this by proposing two algorithms, IPCA and AVICA, for simultaneous manifold and feature learning, and test their accuracy on synthetic and real world data.", "venue": "ArXiv", "authors": ["Franz J. Kir\u00e1ly", "Martin  Kreuzer", "Louis  Theran"], "year": 2014, "n_citations": 7}
{"id": 3122734, "s2_id": "4d732e720f07e26c170619f6365920866f417981", "title": "Hamiltonian Monte Carlo with Asymmetrical Momentum Distributions", "abstract": "Existing rigorous convergence guarantees for the Hamiltonian Monte Carlo (HMC) algorithm use Gaussian auxiliary momentum variables, which are crucially symmetrically distributed. We present a novel convergence analysis for HMC utilizing new analytic and probabilistic arguments. The convergence is rigorously established under significantly weaker conditions, which among others allow for general auxiliary distributions. In our framework, we show that plain HMC with asymmetrical momentum distributions breaks a key self-adjointness requirement. We propose a modified version that we call the Alternating Direction HMC (AD-HMC). Sufficient conditions are established under which AD-HMC exhibits geometric convergence in Wasserstein distance. Numerical experiments suggest that AD-HMC can show improved performance over HMC with Gaussian auxiliaries.", "venue": "ArXiv", "authors": ["Soumyadip  Ghosh", "Yingdong  Lu", "Tomasz  Nowicki"], "year": 2021, "n_citations": 1}
{"id": 3164253, "s2_id": "9765bc468a70f915f5e8b6e5d577baac0e268b92", "title": "Restricted Boltzmann Machines for Robust and Fast Latent Truth Discovery", "abstract": "We address the problem of latent truth discovery, LTD for short, where the goal is to discover the underlying true values of entity attributes in the presence of noisy, conflicting or incomplete information. Despite a multitude of algorithms to address the LTD problem that can be found in literature, only little is known about their overall performance with respect to effectiveness (in terms of truth discovery capabilities), efficiency and robustness. A practical LTD approach should satisfy all these characteristics so that it can be applied to heterogeneous datasets of varying quality and degrees of cleanliness. \nWe propose a novel algorithm for LTD that satisfies the above requirements. The proposed model is based on Restricted Boltzmann Machines, thus coined LTD-RBM. In extensive experiments on various heterogeneous and publicly available datasets, LTD-RBM is superior to state-of-the-art LTD techniques in terms of an overall consideration of effectiveness, efficiency and robustness.", "venue": "ArXiv", "authors": ["Klaus  Broelemann", "Thomas  Gottron", "Gjergji  Kasneci"], "year": 2018, "n_citations": 6}
{"id": 3166179, "s2_id": "2e5e9355fdd35d6ba9115525e74156c34a759931", "title": "On Polynomial time Constructions of Minimum Height Decision Tree", "abstract": "In this paper we study a polynomial time algorithms that for an input $A\\subseteq {B_m}$ outputs a decision tree for $A$ of minimum depth. This problem has many applications that include, to name a few, computer vision, group testing, exact learning from membership queries and game theory. \nArkin et al. and Moshkov gave a polynomial time $(\\ln |A|)$- approximation algorithm (for the depth). The result of Dinur and Steurer for set cover implies that this problem cannot be approximated with ratio $(1-o(1))\\cdot \\ln |A|$, unless P=NP. Moskov the combinatorial measure of extended teaching dimension of $A$, $ETD(A)$. He showed that $ETD(A)$ is a lower bound for the depth of the decision tree for $A$ and then gave an {\\it exponential time} $ETD(A)/\\log(ETD(A))$-approximation algorithm. \nIn this paper we further study the $ETD(A)$ measure and a new combinatorial measure, $DEN(A)$, that we call the density of the set $A$. We show that $DEN(A)\\le ETD(A)+1$. We then give two results. The first result is that the lower bound $ETD(A)$ of Moshkov for the depth of the decision tree for $A$ is greater than the bounds that are obtained by the classical technique used in the literature. The second result is a polynomial time $(\\ln 2) DEN(A)$-approximation (and therefore $(\\ln 2) ETD(A)$-approximation) algorithm for the depth of the decision tree of $A$. We also show that a better approximation ratio implies P=NP. \nWe then apply the above results to learning the class of disjunctions of predicates from membership queries. We show that the $ETD$ of this class is bounded from above by the degree $d$ of its Hasse diagram. We then show that Moshkov algorithm can be run in polynomial time and is $(d/\\log d)$-approximation algorithm. This gives optimal algorithms when the degree is constant. For example, learning axis parallel rays over constant dimension space.", "venue": "ISAAC", "authors": ["Nader H. Bshouty", "Waseem  Makhoul"], "year": 2018, "n_citations": 1}
{"id": 3169975, "s2_id": "db0e82f5bbbcfece78c918125dc30f3e3c9c9d17", "title": "Word level Script Identification from Bangla and Devanagri Handwritten Texts mixed with Roman Script", "abstract": "India is a multi-lingual country where Roman script is often used alongside different Indic scripts in a text document. To develop a script specific handwritten Optical Character Recognition (OCR) system, it is therefore necessary to identify the scripts of handwritten text correctly. In this paper, we present a system, which automatically separates the scripts of handwritten words from a document, written in Bangla or Devanagri mixed with Roman scripts. In this script separation technique, we first, extract the text lines and words from document pages using a script independent Neighboring Component Analysis technique (1). Then we have designed a Multi Layer Perceptron (MLP) based classifier for script separation, trained with 8 different word- level holistic features. Two equal sized datasets, one with Bangla and Roman scripts and the other with Devanagri and Roman scripts, are prepared for the system evaluation. On respective independent text samples, word-level script identification accuracies of 99.29% and 98.43% are achieved.", "venue": "ArXiv", "authors": ["Ram  Sarkar", "Nibaran  Das", "Subhadip  Basu", "Mahantapas  Kundu", "Mita  Nasipuri", "Dipak Kumar Basu"], "year": 2010, "n_citations": 36}
{"id": 3179128, "s2_id": "3618178f673f4c63d1380ef14276b9fba3c3329e", "title": "Bayesian and Dempster-Shafer models for combining multiple sources of evidence in a fraud detection system", "abstract": "Combining evidence from different sources can be achieved with Bayesian or Dempster-Shafer methods. The first requires an estimate of the priors and likelihoods while the second only needs an estimate of the posterior probabilities and enables reasoning with uncertain information due to imprecision of the sources and with the degree of conflict between them. This paper describes the two methods and how they can be applied to the estimation of a global score in the context of fraud detection.", "venue": "ArXiv", "authors": ["Fabrice  Daniel"], "year": 2021, "n_citations": 0}
{"id": 3185119, "s2_id": "2533d16533a148635beffcb8a27ce009d25baff5", "title": "Layer-wise Learning of Kernel Dependence Networks", "abstract": "We propose a greedy strategy to train a deep network for multi-class classification, where each layer is defined as a composition of a linear projection and a nonlinear mapping. This nonlinear mapping is defined as the feature map of a Gaussian kernel, and the linear projection is learned by maximizing the dependence between the layer output and the labels, using the Hilbert Schmidt Independence Criterion (HSIC) as the dependence measure. Since each layer is trained greedily in sequence, all learning is local, and neither backpropagation nor even gradient descent is needed. The depth and width of the network are determined via natural guidelines, and the procedure regularizes its weights in the linear layer. As the key theoretical result, the function class represented by the network is proved to be sufficiently rich to learn any dataset labeling using a finite number of layers, in the sense of reaching minimum mean-squared error or cross-entropy, as long as no two data points with different labels coincide. Experiments demonstrate good generalization performance of the greedy approach across multiple benchmarks while showing a significant computational advantage against a multilayer perceptron of the same complexity trained globally by backpropagation.", "venue": "ArXiv", "authors": ["Chieh  Wu", "Aria  Masoomi", "Arthur  Gretton", "Jennifer  Dy"], "year": 2020, "n_citations": 0}
{"id": 3196123, "s2_id": "22c2380cce89849a7ff85f9845f25593f0773954", "title": "Adaptive Epidemic Forecasting and Community Risk Evaluation of COVID-19", "abstract": "Pandemic control measures like lock-down, restrictions on restaurants and gatherings, social-distancing have shown to be effective in curtailing the spread of COVID-19. However, their sustained enforcement has negative economic effects. To craft strategies and policies that reduce the hardship on the people and the economy while being effective against the pandemic, authorities need to understand the disease dynamics at the right geo-spatial granularity. Considering factors like the hospitals\u2019 ability to handle the fluctuating demands, evaluating various reopening scenarios, and accurate forecasting of cases are vital to decision making. Towards this end, we present a flexible end-to-end solution that seamlessly integrates public health data with tertiary client data to accurately estimate the risk of reopening a community. At its core lies a state-of-the-art prediction model that auto-captures changing trends in transmission and mobility. Benchmarking against various published baselines confirm the superiority of our forecasting algorithm. Combined with the ability to extend to multiple client-specific requirements and perform deductive reasoning through counter-factual analysis, this solution provides actionable insights to multiple client domains ranging from government to educational institutions, hospitals, and commercial establishments.", "venue": "ArXiv", "authors": ["Vishrawas  Gopalakrishnan", "Sayali  Navalekar", "Pan  Ding", "Ryan  Hooley", "Jacob  Miller", "Raman  Srinivasan", "Ajay  Deshpande", "Xuan  Liu", "Simone  Bianco", "James H. Kaufman"], "year": 2021, "n_citations": 1}
{"id": 3207899, "s2_id": "600ae41fd5fc5c8df3be12e6551d96d549d2394c", "title": "Efficiently measuring a quantum device using machine learning", "abstract": "Scalable quantum technologies such as quantum computers will require very large numbers of quantum devices to be characterised and tuned. As the number of devices on chip increases, this task becomes ever more time-consuming, and will be intractable on a large scale without efficient automation. We present measurements on a quantum dot device performed by a machine learning algorithm in real time. The algorithm selects the most informative measurements to perform next by combining information theory with a probabilistic deep-generative model that can generate full-resolution reconstructions from scattered partial measurements. We demonstrate, for two different current map configurations that the algorithm outperforms standard grid scan techniques, reducing the number of measurements required by up to 4 times and the measurement time by 3.7 times. Our contribution goes beyond the use of machine learning for data search and analysis, and instead demonstrates the use of algorithms to automate measurements. This works lays the foundation for learning-based automated measurement of quantum devices.", "venue": "npj Quantum Information", "authors": ["D. T. Lennon", "H.  Moon", "L. C. Camenzind", "Liuqi  Yu", "D. M. Zumb\u00fchl", "G. A. D. Briggs", "M. A. Osborne", "E. A. Laird", "N.  Ares"], "year": 2019, "n_citations": 36}
{"id": 3222665, "s2_id": "cec54a745e4a6b21a8779a20d20746188c003364", "title": "Multilingual Offensive Language Identification with Cross-lingual Embeddings", "abstract": "Offensive content is pervasive in social media and a reason for concern to companies and government organizations. Several studies have been recently published investigating methods to detect the various forms of such content (e.g. hate speech, cyberbulling, and cyberaggression). The clear majority of these studies deal with English partially because most annotated datasets available contain English data. In this paper, we take advantage of English data available by applying cross-lingual contextual word embeddings and transfer learning to make predictions in languages with less resources. We project predictions on comparable data in Bengali, Hindi, and Spanish and we report results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and 0.7513 F1 macro for Spanish. Finally, we show that our approach compares favorably to the best systems submitted to recent shared tasks on these three languages, confirming the robustness of cross-lingual contextual embeddings and transfer learning for this task.", "venue": "EMNLP", "authors": ["Tharindu  Ranasinghe", "Marcos  Zampieri"], "year": 2020, "n_citations": 37}
{"id": 3227770, "s2_id": "07b187dead8e9d4575e374b36dac77a76bf03f85", "title": "RxNN: A Framework for Evaluating Deep Neural Networks on Resistive Crossbars", "abstract": "Resistive crossbars designed with nonvolatile memory devices have emerged as promising building blocks for deep neural network (DNN) hardware, due to their ability to compactly and efficiently realize vector\u2013matrix multiplication (VMM), the dominant computational kernel in DNNs. However, a key challenge with resistive crossbars is that they suffer from a range of device and circuit level nonidealities, such as driver resistance, sensing resistance, sneak paths, interconnect parasitics, nonlinearities in the peripheral circuits, stochastic write operations, and process variations. These nonidealities can lead to errors in VMMs, eventually degrading the DNN\u2019s accuracy. It is therefore critical to study the impact of crossbar nonidealities on the accuracy of large-scale DNNs (with millions of neurons and billions of synaptic connections). However, this is challenging because the existing device and circuit models are too slow to use in application-level evaluations. We present RxNN, a fast and accurate simulation framework to evaluate large-scale DNNs on resistive crossbar systems. RxNN splits and maps the computations involved in each DNN layer into crossbar operations, and evaluates them using a fast crossbar model (FCM) that accurately captures the errors arising due to crossbar nonidealities while being four-to-five orders of magnitude faster than circuit simulation. FCM models a crossbar-based VMM operation using three stages\u2014nonlinear models for the input and output peripheral circuits (digital-to-analog and analog-to-digital converters), and an equivalent nonideal conductance matrix for the core crossbar array. We implement RxNN by extending the Caffe machine learning framework and use it to evaluate a suite of six large-scale DNNs developed for the ImageNet Challenge (ILSVRC). Our experiments reveal that resistive crossbar nonidealities can lead to significant accuracy degradations (9.6%\u201332%) for these large-scale DNNs. To the best of our knowledge, this article is the first quantitative evaluation of the accuracy of large-scale DNNs on resistive crossbar-based hardware. We also demonstrate that RxNN enables fast model-in-the-loop retraining of DNNs to partially mitigate the accuracy degradation.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Shubham  Jain", "Abhronil  Sengupta", "Kaushik  Roy", "Anand  Raghunathan"], "year": 2021, "n_citations": 29}
{"id": 3266610, "s2_id": "c3966ade2d5345b4503a9acd17cf28efe34cdb09", "title": "Coarse-Grained Smoothness for RL in Metric Spaces", "abstract": "Principled decision-making in continuous state\u2013 action spaces is impossible without some assumptions. A common approach is to assume Lipschitz continuity of the Q-function. We show that, unfortunately, this property fails to hold in many typical domains. We propose a new coarse-grained smoothness definition that generalizes the notion of Lipschitz continuity, is more widely applicable, and allows us to compute significantly tighter bounds on Q-functions, leading to improved learning. We provide a theoretical analysis of our new smoothness definition, and discuss its implications and impact on control and exploration in continuous domains.", "venue": "ArXiv", "authors": ["Omer  Gottesman", "Kavosh  Asadi", "Cameron  Allen", "Sam  Lobel", "George  Konidaris", "Michael  Littman"], "year": 2021, "n_citations": 0}
{"id": 3280429, "s2_id": "bc69ba1b1379b76ec43f272c63deddd05a2d71c2", "title": "\u201cWhat is on your mind?\u201d Automated Scoring of Mindreading in Childhood and Early Adolescence", "abstract": "In this paper we present the first work on the automated scoring of mindreading ability in middle childhood and early adolescence. We create MIND-CA, a new corpus of 11,311 question-answer pairs in English from 1,066 children aged from 7 to 14. We perform machine learning experiments and carry out extensive quantitative and qualitative evaluation. We obtain promising results, demonstrating the applicability of state-of-the-art NLP solutions to a new domain and task.", "venue": "COLING", "authors": ["Venelin  Kovatchev", "Phillip  Smith", "Mark  Lee", "Imogen Grumley Traynor", "Irene Luque Aguilera", "Rory T. Devine"], "year": 2020, "n_citations": 1}
{"id": 3283416, "s2_id": "9cc0416da6e6d198d0e3dcec2117adbd16f85f98", "title": "Multi-level Binarized LSTM in EEG Classification for Wearable Devices", "abstract": "Long Short-Term Memory (LSTM) is widely used in various sequential applications. Complex LSTMs could be hardly deployed on wearable and resourced-limited devices due to the huge amount of computations and memory requirements. Binary LSTMs are introduced to cope with this problem, however, they lead to significant accuracy loss in some applications such as EEG classification which is essential to be deployed in wearable devices. In this paper, we propose an efficient multi-level binarized LSTM which has significantly reduced computations whereas ensuring an accuracy pretty close to full precision LSTM. By deploying 5-level binarized weights and inputs, our method reduces area and delay of MAC operation about $31\\times and 27\\times$ in 65nm technology, respectively with less than 0.01% accuracy loss. In contrast to many compute-intensive deep-learning approaches, the proposed algorithm is lightweight, and therefore, brings performance efficiency with accurate LSTM-based EEG classification to realtime wearable devices.", "venue": "2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)", "authors": ["Najmeh  Nazari", "Seyed Ahmad Mirsalari", "Sima  Sinaei", "Mostafa E. Salehi", "Masoud  Daneshtalab"], "year": 2020, "n_citations": 7}
{"id": 3286014, "s2_id": "c8cb1490c9d36286d27fb489355a81c9e470669e", "title": "Domain Adaptation for Rare Classes Augmented with Synthetic Samples", "abstract": "To alleviate lower classification performance on rare classes in imbalanced datasets, a possible solution is to augment the underrepresented classes with synthetic samples. Domain adaptation can be incorporated in a classifier to decrease the domain discrepancy between real and synthetic samples. While domain adaptation is generally applied on completely synthetic source domains and real target domains, we explore how domain adaptation can be applied when only a single rare class is augmented with simulated samples. As a testbed, we use a camera trap animal dataset with a rare deer class, which is augmented with synthetic deer samples. We adapt existing domain adaptation methods to two new methods for the single rare class setting: DeerDANN, based on the Domain-Adversarial Neural Network (DANN), and DeerCORAL, based on deep correlation alignment (Deep CORAL) architectures. Experiments show that DeerDANN has the highest improvement in deer classification accuracy of 24.0% versus 22.4% improvement of DeerCORAL when compared to the baseline. Further, both methods require fewer than 10k synthetic samples, as used by the baseline, to achieve these higher accuracies. DeerCORAL requires the least number of synthetic samples (2k deer), followed by DeerDANN (8k deer).", "venue": "ArXiv", "authors": ["Tuhin  Das", "Robert-Jan  Bruintjes", "Attila  Lengyel", "Jan van Gemert", "Sara  Beery"], "year": 2021, "n_citations": 0}
{"id": 3308401, "s2_id": "d1866c21df0aaabc2deb8252501b0927666e6adc", "title": "Bag of Tricks for Retail Product Image Classification", "abstract": "Retail Product Image Classification is an important Computer Vision and Machine Learning problem for building real world systems like self-checkout stores and automated retail execution evaluation. In this work, we present various tricks to increase accuracy of Deep Learning models on different types of retail product image classification datasets. These tricks enable us to increase the accuracy of fine tuned convnets for retail product image classification by a large margin. As the most prominent trick, we introduce a new neural network layer called Local-Concepts-Accumulation (LCA) layer which gives consistent gains across multiple datasets. Two other tricks we find to increase accuracy on retail product identification are using an instagram-pretrained Convnet and using Maximum Entropy as an auxiliary loss for classification.", "venue": "ICIAR", "authors": ["Muktabh Mayank Srivastava"], "year": 2020, "n_citations": 4}
{"id": 3315251, "s2_id": "99b9595ac6a13bf986db40a301e3f11442eec36b", "title": "Block Contextual MDPs for Continual Learning", "abstract": "In reinforcement learning (RL), when defining a Markov Decision Process (MDP), the environment dynamics is implicitly assumed to be stationary. This assumption of stationarity, while simplifying, can be unrealistic in many scenarios. In the continual reinforcement learning scenario, the sequence of tasks is another source of nonstationarity. In this work, we propose to examine this continual reinforcement learning setting through the block contextual MDP (BC-MDP) framework, which enables us to relax the assumption of stationarity. This framework challenges RL algorithms to handle both nonstationarity and rich observation settings and, by additionally leveraging smoothness properties, enables us to study generalization bounds for this setting. Finally, we take inspiration from adaptive control to propose a novel algorithm that addresses the challenges introduced by this more realistic BC-MDP setting, allows for zero-shot adaptation at evaluation time, and achieves strong performance on several nonstationary environments.", "venue": "ArXiv", "authors": ["Shagun  Sodhani", "Franziska  Meier", "Joelle  Pineau", "Amy  Zhang"], "year": 2021, "n_citations": 0}
{"id": 3316682, "s2_id": "a0e8e5d6520054e867189c6df0eeb93775ce921a", "title": "SPACE: Structured Compression and Sharing of Representational Space for Continual Learning", "abstract": "Humans learn incrementally from sequential experiences throughout their lives, which has proven hard to emulate in artificial neural networks. Incrementally learning tasks causes neural networks to overwrite relevant information learned about older tasks, resulting in \u2018Catastrophic Forgetting\u2019. Efforts to overcome this phenomenon often utilize resources poorly, for instance, by growing the network architecture or needing to save parametric importance scores, or violate data privacy between tasks. To tackle this, we propose SPACE, an algorithm that enables a network to learn continually and efficiently by partitioning the learnt space into a Core space, that serves as the condensed knowledge base over previously learned tasks, and a Residual space, which is akin to a scratch space for learning the current task. After learning each task, the Residual is analyzed for redundancy, both within itself and with the learnt Core space. A minimal number of extra dimensions required to explain the current task are added to the Core space and the remaining Residual is freed up for learning the next task. We evaluate our algorithm on P-MNIST, CIFAR and a sequence of 8 different datasets, and achieve comparable accuracy to the state-of-the-art methods while overcoming catastrophic forgetting. Additionally, our algorithm is well suited for practical use. The partitioning algorithm analyzes all layers in one shot, ensuring scalability to deeper networks. Moreover, the analysis of dimensions translates to filter-level sparsity, and the structured nature of the resulting architecture gives us up to 5x improvement in energy efficiency during task inference over the current state-of-the-art.", "venue": "IEEE Access", "authors": ["Gobinda  Saha", "Isha  Garg", "Aayush  Ankit", "Kaushik  Roy"], "year": 2021, "n_citations": 4}
{"id": 3327585, "s2_id": "d9824bb29ba79e7fba6f6853f970b244c7c917d2", "title": "Qualitative detection of oil adulteration with machine learning approaches", "abstract": "The study focused on the machine learning analysis approaches to identify the adulteration of 9 kinds of edible oil qualitatively and answered the following three questions: Is the oil sample adulterant? How does it constitute? What is the main ingredient of the adulteration oil? After extracting the high-performance liquid chromatography (HPLC) data on triglyceride from 370 oil samples, we applied the adaptive boosting with multi-class Hamming loss (AdaBoost.MH) to distinguish the oil adulteration in contrast with the support vector machine (SVM). Further, we regarded the adulterant oil and the pure oil samples as ones with multiple labels and with only one label, respectively. Then multi-label AdaBoost.MH and multi-label learning vector quantization (ML-LVQ) model were built to determine the ingredients and their relative ratio in the adulteration oil. The experimental results on six measures show that ML-LVQ achieves better performance than multi-label AdaBoost.MH.", "venue": "ArXiv", "authors": ["Xiao-Bo  Jin", "Qiang  Lu", "Feng  Wang", "Quan-gong  Huo"], "year": 2013, "n_citations": 1}
{"id": 3328046, "s2_id": "efdf11e111f6475b30341474ca08488ecb36e126", "title": "Feature trajectory dynamic time warping for clustering of speech segments", "abstract": "Dynamic time warping (DTW) can be used to compute the similarity between two sequences of generally differing length. We propose a modification to DTW that performs individual and independent pairwise alignment of feature trajectories. The modified technique, termed feature trajectory dynamic time warping (FTDTW), is applied as a similarity measure in the agglomerative hierarchical clustering of speech segments. Experiments using MFCC and PLP parametrisations extracted from TIMIT and from the Spoken Arabic Digit Dataset (SADD) show consistent and statistically significant improvements in the quality of the resulting clusters in terms of F-measure and normalised mutual information (NMI).", "venue": "EURASIP J. Audio Speech Music. Process.", "authors": ["Lerato  Lerato", "Thomas  Niesler"], "year": 2019, "n_citations": 3}
{"id": 3348483, "s2_id": "b88c0990cb25ac3d7786132e8121b01a0efef933", "title": "Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity", "abstract": "Sensitive statistics are often collected across sets of users, with repeated collection of reports done over time. For example, trends in users' private preferences or software usage may be monitored via such reports. We study the collection of such statistics in the local differential privacy (LDP) model, and describe an algorithm whose privacy cost is polylogarithmic in the number of changes to a user's value. \n \nMore fundamentally---by building on anonymity of the users' reports---we also demonstrate how the privacy cost of our LDP algorithm can actually be much lower when viewed in the central model of differential privacy. We show, via a new and general privacy amplification technique, that any permutation-invariant algorithm satisfying e-local differential privacy will satisfy [MATH HERE]-central differential privacy. By this, we explain how the high noise and [MATH HERE] overhead of LDP protocols is a consequence of them being significantly more private in the central model. As a practical corollary, our results imply that several LDP-based industrial deployments may have much lower privacy cost than their advertised e would indicate---at least if reports are anonymized.", "venue": "SODA", "authors": ["\u00dalfar  Erlingsson", "Vitaly  Feldman", "Ilya  Mironov", "Ananth  Raghunathan", "Kunal  Talwar", "Abhradeep  Thakurta"], "year": 2019, "n_citations": 163}
{"id": 3357396, "s2_id": "3e50dd3c1d5fd04d97e27aaa462e7a8059b88aeb", "title": "Streaming Transformer for Hardware Efficient Voice Trigger Detection and False Trigger Mitigation", "abstract": "We present a unified and hardware efficient architecture for two stage voice trigger detection (VTD) and false trigger mitigation (FTM) tasks. Two stage VTD systems of voice assistants can get falsely activated to audio segments acoustically similar to the trigger phrase of interest. FTM systems cancel such activations by using post trigger audio context. Traditional FTM systems rely on automatic speech recognition lattices which are computationally expensive to obtain on device. We propose a streaming transformer (TF) encoder architecture, which progressively processes incoming audio chunks and maintains audio context to perform both VTD and FTM tasks using only acoustic features. The proposed joint model yields an average 18% relative reduction in false reject rate (FRR) for the VTD task at a given false alarm rate. Moreover, our model suppresses 95% of the false triggers with an additional one second of posttrigger audio. Finally, on-device measurements show 32% reduction in runtime memory and 56% reduction in inference time compared to non-streaming version of the model.", "venue": "Interspeech 2021", "authors": ["Vineet  Garg", "Wonil  Chang", "Siddharth  Sigtia", "Saurabh  Adya", "Pramod  Simha", "Pranay  Dighe", "Chandra  Dhir"], "year": 2021, "n_citations": 2}
{"id": 3359605, "s2_id": "eae477b8567f030e2fdc98e3d335381aa84a6e02", "title": "Augmenting Reinforcement Learning with Behavior Primitives for Diverse Manipulation Tasks", "abstract": "Realistic manipulation tasks require a robot to interact with an environment with a prolonged sequence of motor actions. While deep reinforcement learning methods have recently emerged as a promising paradigm for automating manipulation behaviors, they usually fall short in long-horizon tasks due to the exploration burden. This work introduces MAnipulation Primitive-augmented reinforcement LEarning (MAPLE), a learning framework that augments standard reinforcement learning algorithms with a pre-defined library of behavior primitives. These behavior primitives are robust functional modules specialized in achieving manipulation goals, such as grasping and pushing. To use these heterogeneous primitives, we develop a hierarchical policy that involves the primitives and instantiates their executions with input parameters. We demonstrate that MAPLE outperforms baseline approaches by a significant margin on a suite of simulated manipulation tasks. We also quantify the compositional structure of the learned behaviors and highlight our method\u2019s ability to transfer policies to new task variants and to physical hardware. Videos and code are available at https://ut-austin-rpl.github.", "venue": "ArXiv", "authors": ["Soroush  Nasiriany", "Huihan  Liu", "Yuke  Zhu"], "year": 2021, "n_citations": 1}
{"id": 3375172, "s2_id": "788b7f16b7412817499689b5c6c25851c7e03fde", "title": "Multi-Label Gold Asymmetric Loss Correction with Single-Label Regulators", "abstract": "Multi-label learning is an emerging extension of the multi-class classification where an image contains multiple labels. Not only acquiring a clean and fully labeled dataset in multi-label learning is extremely expensive, but also many of the actual labels are corrupted or missing due to the automated or non-expert annotation techniques. Noisy label data decrease the prediction performance drastically. In this paper, we propose a novel Gold Asymmetric Loss Correction with Single-Label Regulators (GALC-SLR) that operates robust against noisy labels. GALC-SLR estimates the noise confusion matrix using single-label samples, then constructs an asymmetric loss correction via estimated confusion matrix to avoid overfitting to the noisy labels. Empirical results show that our method outperforms the state-of-the-art original asymmetric loss multi-label classifier under all corruption levels, showing mean average precision improvement up to 28.67% on a real world dataset of MS-COCO, yielding a better generalization of the unseen data and increased prediction performance.", "venue": "ArXiv", "authors": ["Cosmin Octavian Pene", "Amirmasoud  Ghiassi", "Taraneh  Younesian", "Robert  Birke", "Lydia Y. Chen"], "year": 2021, "n_citations": 0}
{"id": 3391361, "s2_id": "5b52a19c064d730acc891f1a1ce1f4083f636b34", "title": "Neural Network Gaussian Process Considering Input Uncertainty for Composite Structures Assembly", "abstract": "Developing machine learning enabled smart manufacturing is promising for composite structures assembly process. To improve production quality and efficiency of the assembly process, accurate predictive analysis on dimensional deviations and residual stress of the composite structures is required. The novel composite structures assembly involves two challenges: (i) the highly nonlinear and anisotropic properties of composite materials; and (ii) inevitable uncertainty in the assembly process. To overcome those problems, we propose a neural network Gaussian process model considering input uncertainty for composite structures assembly. Deep architecture of our model allows us to approximate a complex process better, and consideration of input uncertainty enables robust modeling with complete incorporation of the process uncertainty. Based on simulation and case study, the NNGPIU can outperform other benchmark methods when the response function is nonsmooth and nonlinear. Although we use composite structure assembly as an example, the proposed methodology can be applicable to other engineering systems with intrinsic uncertainties.", "venue": "ArXiv", "authors": ["Cheolhei  Lee", "Jianguo  Wu", "Wenjia  Wang", "Xiaowei  Yue"], "year": 2020, "n_citations": 3}
{"id": 3408749, "s2_id": "c2758071e2c64a7af80b9001d493e67b84352a16", "title": "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification", "abstract": "Black-box machine learning learning methods are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Distributionfree uncertainty quantification (distribution-free UQ) is a user-friendly paradigm for creating statistically rigorous confidence intervals/sets for such predictions. Critically, the intervals/sets are valid without distributional assumptions or model assumptions, possessing explicit guarantees even with finitely many datapoints. Moreover, they adapt to the difficulty of the input; when the input example is difficult, the uncertainty intervals/sets are large, signaling that the model might be wrong. Without much work and without retraining, one can use distribution-free methods on any underlying algorithm, such as a neural network, to produce confidence sets guaranteed to contain the ground truth with a user-specified probability, such as 90%. Indeed, the methods are easy-to-understand and general, applying to many modern prediction problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed at a reader interested in the practical implementation of distribution-free UQ who is not necessarily a statistician. We lead the reader through the practical theory and applications of distribution-free UQ, beginning with conformal prediction and culminating with distribution-free control of any risk, such as the false-discovery rate, false positive rate of out-of-distribution detection, and so on. We will include many explanatory illustrations, examples, and code samples in Python, with PyTorch syntax. The goal is to provide the reader a working understanding of distribution-free UQ, allowing them to put confidence intervals on their algorithms, with one self-contained document. 1 ar X iv :2 10 7. 07 51 1v 2 [ cs .L G ] 2 7 D ec 2 02 1", "venue": "ArXiv", "authors": ["Anastasios N. Angelopoulos", "Stephen  Bates"], "year": 2021, "n_citations": 5}
{"id": 3411865, "s2_id": "efd05de549928c4d5b17335342ad55b1827452fb", "title": "Nonparametric Hierarchical Clustering of Functional Data", "abstract": "In this paper, we deal with the problem of curves clustering. We propose a nonparametric method which partitions the curves into clusters and discretizes the dimensions of the curve points into intervals. The cross-product of these partitions forms a data-grid which is obtained using a Bayesian model selection approach while making no assumptions regarding the curves. Finally, a post-processing technique, aiming at reducing the number of clusters in order to improve the interpretability of the clustering, is proposed. It consists in optimally merging the clusters step by step, which corresponds to an agglomerative hierarchical classification whose dissimilarity measure is the variation of the criterion. Interestingly this measure is none other than the sum of the Kullback-Leibler divergences between clusters distributions before and after the merges. The practical interest of the approach for functional data exploratory analysis is presented and compared with an alternative approach on an artificial and a real world data set.", "venue": "EGC", "authors": ["Marc  Boull\u00e9", "Romain  Guigour\u00e8s", "Fabrice  Rossi"], "year": 2012, "n_citations": 8}
{"id": 3452594, "s2_id": "2319888519e62a6c30ba67d5e21ad7712a310b0c", "title": "Path Planning Using Probability Tensor Flows", "abstract": "Probability models are emerging as a promising framework to account for \u201cintelligent\u201d behavior. In this article, probability propagation is discussed to model agent's motion in potentially complex grids that include goals and obstacles. Tensor messages in the state-action space (due to grid structure, states are 2-D and the concomitant probability distributions are represented by 3-D arrays), propagated bi-directionally on a Markov chain, provide crucial information to guide the agent's decisions. The discussion is carried out with reference to a set of simulated grids and includes scenarios with multiple goals and multiple agents. The visualization of the tensor flow reveals interesting clues about how decisions are made by the agents. The emerging behaviors are very realistic and demonstrate great potential for the application of this framework to real environments.", "venue": "IEEE Aerospace and Electronic Systems Magazine", "authors": ["Francesco A. N. Palmieri", "Krishna R. Pattipati", "Giovanni  Fioretti", "Giovanni Di Gennaro", "Amedeo  Buonanno"], "year": 2021, "n_citations": 1}
{"id": 3460394, "s2_id": "065e8501d106d15c207c92ae09fb9a85701187c1", "title": "Prospective Artificial Intelligence Approaches for Active Cyber Defence", "abstract": "Cyber criminals are rapidly developing new malicious tools that leverage artificial intelligence (AI) to enable new classes of adaptive and stealthy attacks. New defensive methods need to be developed to counter these threats. Some cyber security professionals are speculating AI will enable corresponding new classes of active cyber defence measures \u2013 is this realistic, or currently mostly hype? The Alan Turing Institute, with expert guidance from the UK National Cyber Security Centre and Defence Science Technology Laboratory, published a research roadmap for AI for ACD last year. This position paper updates the roadmap for two of the most promising AI approaches \u2013 reinforcement learning and causal inference and describes why they could help tip the balance back towards defenders.", "venue": "ArXiv", "authors": ["Neil  Dhir", "Henrique  Hoeltgebaum", "Niall  Adams", "Mark  Briers", "Anthony  Burke", "Paul  Jones"], "year": 2021, "n_citations": 3}
{"id": 3484360, "s2_id": "e211debbd10710aa51cb6796aca6f13132067c3f", "title": "SPot: A Tool for Identifying Operating Segments in Financial Tables", "abstract": "In this paper we present SPot, an automated tool for detecting operating segments and their related performance indicators from earnings reports. Due to their company-specific nature, operating segments cannot be detected using taxonomy-based approaches. Instead, we train a bidirectional RNN classifier that can distinguish between common metrics such as \"revenue\" and company-specific metrics that are likely to be operating segments, such as \"iPhone\" or \"cloud services\". SPot surfaces the results in an interactive web interface that allows users to trace and adjust performance metrics for each operating segment. This facilitates credit monitoring, enables them to perform competitive benchmarking more effectively, and can be used for trend analysis at company and sector levels.", "venue": "SIGIR", "authors": ["Zhiqiang  Ma", "Steven  Pomerville", "Mingyang  Di", "Armineh  Nourbakhsh"], "year": 2020, "n_citations": 1}
{"id": 3497723, "s2_id": "64b5eb307920c9c009a574b612c9a6150334b76f", "title": "Artificial Intelligence-Assisted Energy and Thermal Comfort Control for Sustainable Buildings: An Extended Representation of the Systematic Review", "abstract": "Different factors such as thermal comfort, humidity, air quality, and noise have significant combined effects on the acceptability and quality of the activities performed by the building occupants who spend most of their times indoors. Among the factors cited, thermal comfort, which contributes to the human well-being because of its connection with the thermoregulation of the human body. Therefore, the creation of thermally comfortable and energy efficient environments is of great importance in the design of the buildings and hence the heating, ventilation and air-conditioning systems. Recent works have been directed towards more advanced control strategies, based mainly on artificial intelligence which has the ability to imitate human behavior. This systematic literature review aims to provide an overview of the intelligent control strategies inside building and to investigate their ability to balance thermal comfort and energy efficiency optimization in indoor environments. Methods. A systematic literature review examined the peer-reviewed research works using ACM Digital Library, Scopus, Google Scholar, IEEE Xplore (IEOL), Web of Science, and Science Direct (SDOL), besides other sources from manual search. With the following string terms: thermal comfort, comfort temperature, preferred temperature, intelligent control, advanced control, artificial intelligence, computational intelligence, building, indoors, and built environment. Inclusion criteria were: English, studies monitoring, mainly, human thermal comfort in buildings and energy efficiency simultaneously based on control strategies using the intelligent approaches. Preferred Reporting Items for Systematic Reviews and Meta-Analysis guidelines were used. Initially, 1,077 articles were yielded, and 120 ultimately met inclusion criteria and were reviewed.", "venue": "ArXiv", "authors": ["Ghezlane Halhoul Merabet", "Mohammed  Essaaidi", "Mohamed Ben Haddou", "Basheer  Qolomany", "Junaid  Qadir", "Muhammad T. Anan", "Ala I. Al-Fuqaha", "Mohamed Riduan Abid", "Driss  Benhaddou"], "year": 2020, "n_citations": 1}
{"id": 3504948, "s2_id": "e41b3264e92637f71777067ec42f9697051b6db9", "title": "Sparse Separable Nonnegative Matrix Factorization", "abstract": "We propose a new variant of nonnegative matrix factorization (NMF), combining separability and sparsity assumptions. Separability requires that the columns of the first NMF factor are equal to columns of the input matrix, while sparsity requires that the columns of the second NMF factor are sparse. We call this variant sparse separable NMF (SSNMF), which we prove to be NP-complete, as opposed to separable NMF which can be solved in polynomial time. The main motivation to consider this new model is to handle underdetermined blind source separation problems, such as multispectral image unmixing. We introduce an algorithm to solve SSNMF, based on the successive nonnegative projection algorithm (SNPA, an effective algorithm for separable NMF), and an exact sparse nonnegative least squares solver. We prove that, in noiseless settings and under mild assumptions, our algorithm recovers the true underlying sources. This is illustrated by experiments on synthetic data sets and the unmixing of a multispectral image.", "venue": "ECML/PKDD", "authors": ["Nicolas  Nadisic", "Arnaud  Vandaele", "Jeremy E. Cohen", "Nicolas  Gillis"], "year": 2020, "n_citations": 0}
{"id": 3513498, "s2_id": "2e8bfb42f17715dd7b6c56c7812b6782d1fad6d8", "title": "LipschitzLR: Using theoretically computed adaptive learning rates for fast convergence", "abstract": "Optimizing deep neural networks is largely thought to be an empirical process, requiring manual tuning of several hyper-parameters, such as learning rate, weight decay, and dropout rate. Arguably, the learning rate is the most important of these to tune, and this has gained more attention in recent works. In this paper, we propose a novel method to compute the learning rate for training deep neural networks with stochastic gradient descent. We first derive a theoretical framework to compute learning rates dynamically based on the Lipschitz constant of the loss function. We then extend this framework to other commonly used optimization algorithms, such as gradient descent with momentum and Adam. We run an extensive set of experiments that demonstrate the efficacy of our approach on popular architectures and datasets, and show that commonly used learning rates are an order of magnitude smaller than the ideal value.", "venue": "Appl. Intell.", "authors": ["Rahul  Yedida", "Snehanshu  Saha"], "year": 2021, "n_citations": 9}
{"id": 3520314, "s2_id": "f8ff94520350485ef7ff5c2a640cc999986d6d9a", "title": "The CodRep Machine Learning on Source Code Competition", "abstract": "CodRep is a machine learning competition on source code data. It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis. In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact. The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at this https URL.", "venue": "ArXiv", "authors": ["Zimin  Chen", "Martin  Monperrus"], "year": 2018, "n_citations": 8}
{"id": 3542058, "s2_id": "7c14816cdfa9a1489b5009409e9a1cd78d791be0", "title": "Shade: Information-Based Regularization for Deep Learning", "abstract": "Regularization is a big issue for training deep neural networks. In this paper, we propose a new information-theory-based regularization scheme named SHADE for SHAnnon DEcay. The originality of the approach is to define a prior based on conditional entropy, which explicitly decouples the learning of invariant representations in the regularizer and the learning of correlations between inputs and labels in the data fitting term. Our second contribution is to derive a stochastic version of the regularizer compatible with deep learning, resulting in a tractable training scheme. We empirically validate the efficiency of our approach to improve classification performances compared to standard regularization schemes on several standard architectures.", "venue": "2018 25th IEEE International Conference on Image Processing (ICIP)", "authors": ["Michael  Blot", "Thomas  Robert", "Nicolas  Thome", "Matthieu  Cord"], "year": 2018, "n_citations": 9}
{"id": 3546967, "s2_id": "f406f7f7abe99517e0540aacde8013f8e7b42f55", "title": "On the Estimation of Complex Circuits Functional Failure Rate by Machine Learning Techniques", "abstract": "De-Rating or Vulnerability Factors are a major feature of failure analysis efforts mandated by today's Functional Safety requirements. Determining the Functional De-Rating of sequential logic cells typically requires computationally intensive fault-injection simulation campaigns. In this paper a new approach is proposed which uses Machine Learning to estimate the Functional De-Rating of individual flip-flops and thus, optimising and enhancing fault injection efforts. Therefore, first, a set of per-instance features is described and extracted through an analysis approach combining static elements (cell properties, circuit structure, synthesis attributes) and dynamic elements (signal activity). Second, reference data is obtained through first-principles fault simulation approaches. Finally, one part of the reference dataset is used to train the Machine Learning algorithm and the remaining is used to validate and benchmark the accuracy of the trained tool. The intended goal is to obtain a trained model able to provide accurate per-instance Functional De-Rating data for the full list of circuit instances, an objective that is difficult to reach using classical methods. The presented methodology is accompanied by a practical example to determine the performance of various Machine Learning models for different training sizes.", "venue": "2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks \u2013 Supplemental Volume (DSN-S)", "authors": ["Thomas  Lange", "Aneesh  Balakrishnan", "Maximilien  Glorieux", "Dan  Alexandrescu", "Luca  Sterpone"], "year": 2019, "n_citations": 2}
{"id": 3560381, "s2_id": "2458f0276773fadc199f6a51d1139717077c0b0a", "title": "A Parallel and Efficient Algorithm for Learning to Match", "abstract": "Many tasks in data mining and related fields can be formalized as matching between objects in two heterogeneous domains, including collaborative filtering, link prediction, image tagging, and web search. Machine learning techniques, referred to as learning-to-match in this paper, have been successfully applied to the problems. Among them, a class of state-of-the-art methods, named feature-based matrix factorization, formalize the task as an extension to matrix factorization by incorporating auxiliary features into the model. Unfortunately, making those algorithms scale to real world problems is challenging, and simple parallelization strategies fail due to the complex cross talking patterns between sub-tasks. In this paper, we tackle this challenge with a novel parallel and efficient algorithm. Our algorithm, based on coordinate descent, can easily handle hundreds of millions of instances and features on a single machine. The key recipe of this algorithm is an iterative relaxation of the objective to facilitate parallel updates of parameters, with guaranteed convergence on minimizing the original objective function. Experimental results demonstrate that the proposed method is effective on a wide range of matching problems, with efficiency significantly improved upon the baselines while accuracy retained unchanged.", "venue": "2014 IEEE International Conference on Data Mining", "authors": ["Jingbo  Shang", "Tianqi  Chen", "Hang  Li", "Zhengdong  Lu", "Yong  Yu"], "year": 2014, "n_citations": 9}
{"id": 3563683, "s2_id": "2f794c818acb16ccfa5552bf2b0bd04b71767538", "title": "Single photon in hierarchical architecture for physical reinforcement learning: Photon intelligence", "abstract": "Understanding and using natural processes for intelligent functionalities, referred to as natural intelligence, has recently attracted interest from a variety of fields, including post-silicon computing for artificial intelligence and decision making in the behavioural sciences. In a past study, we successfully used the wave-particle duality of single photons to solve the two-armed bandit problem, which constitutes the foundation of reinforcement learning and decision making. In this study, we propose and confirm a hierarchical architecture for single-photon-based reinforcement learning and decision making that verifies the scalability of the principle. Specifically, the four-armed bandit problem is solved given zero prior knowledge in a two-layer hierarchical architecture, where polarization is autonomously adapted in order to effect adequate decision making using single-photon measurements. In the hierarchical structure, the notion of layer-dependent decisions emerges. The optimal solutions in the coarse layer and in the fine layer, however, conflict with each other in some contradictive problems. We show that while what we call a tournament strategy resolves such contradictions, the probabilistic nature of single photons allows for the direct location of the optimal solution even for contradictive problems, hence manifesting the exploration ability of single photons. This study provides insights into photon intelligence in hierarchical architectures for future artificial intelligence as well as the potential of natural processes for intelligent functionalities.", "venue": "ArXiv", "authors": ["Makoto  Naruse", "Martin  Berthel", "Aur\u00e9lien  Drezet", "Serge  Huant", "Hirokazu  Hori", "Song-Ju  Kim"], "year": 2016, "n_citations": 0}
{"id": 3565259, "s2_id": "91ace1fc8e4d2ab76e08117563f6f5a33ae54eb7", "title": "An asymptotically optimal strategy for constrained multi-armed bandit problems", "abstract": "This note considers the model of \u201cconstrained multi-armed bandit\u201d (CMAB) that generalizes that of the classical stochastic MAB by adding a feasibility constraint for each action. The feasibility is in fact another (conflicting) objective that should be kept in order for a playing-strategy to achieve the optimality of the main objective. While the stochastic MAB model is a special case of the Markov decision process (MDP) model, the CMAB model is a special case of the constrained MDP model. For the asymptotic optimality measured by the probability of choosing an optimal feasible arm over infinite horizon, we show that the optimality is achievable by a simple strategy extended from the $$\\epsilon _t$$ \u03f5 t -greedy strategy used for unconstrained MAB problems. We provide a finite-time lower bound on the probability of correct selection of an optimal near-feasible arm that holds for all time steps. Under some conditions, the bound approaches one as time t goes to infinity. A particular example sequence of $$\\{\\epsilon _t\\}$$ { \u03f5 t } having the asymptotic convergence rate in the order of $$(1-\\frac{1}{t})^4$$ ( 1 - 1 t ) 4 that holds from a sufficiently large t is also discussed.", "venue": "Math. Methods Oper. Res.", "authors": ["Hyeong Soo Chang"], "year": 2020, "n_citations": 5}
{"id": 3595021, "s2_id": "61487711b15a69c367ca759820350622ba57c6d8", "title": "Conditioning LSTM Decoder and Bi-directional Attention Based Question Answering System", "abstract": "Applying neural-networks on Question Answering has gained increasing popularity in recent years. In this paper, I implemented a model with Bi-directional attention flow layer, connected with a Multi-layer LSTM encoder, connected with one start-index decoder and one conditioning end-index decoder. I introduce a new end-index decoder layer, conditioning on start-index output. The Experiment shows this has increased model performance by 15.16%. For prediction, I proposed a new smart-span equation, rewarding both short answer length and high probability in start-index and end-index, which further improved the prediction accuracy. The best single model achieves an F1 score of 73.97% and EM score of 64.95% on test set.", "venue": "ArXiv", "authors": ["Heguang  Liu"], "year": 2019, "n_citations": 1}
{"id": 3598792, "s2_id": "069a05c0ae7ac49ad7eb8e0a0744c212c58bd863", "title": "Meta-learning for Few-shot Natural Language Processing: A Survey", "abstract": "Few-shot natural language processing (NLP) refers to NLP tasks that are accompanied with merely a handful of labeled examples. This is a real-world challenge that an AI system must learn to handle. Usually we rely on collecting more auxiliary information or developing a more efficient learning algorithm. However, the general gradient-based optimization in high capacity models, if training from scratch, requires many parameter-updating steps over a large number of labeled examples to perform well (Snell et al., 2017). If the target task itself cannot provide more information, how about collecting more tasks equipped with rich annotations to help the model learning? The goal of meta-learning is to train a model on a variety of tasks with rich annotations, such that it can solve a new task using only a few labeled samples. The key idea is to train the model's initial parameters such that the model has maximal performance on a new task after the parameters have been updated through zero or a couple of gradient steps. There are already some surveys for meta-learning, such as (Vilalta and Drissi, 2002; Vanschoren, 2018; Hospedales et al., 2020). Nevertheless, this paper focuses on NLP domain, especially few-shot applications. We try to provide clearer definitions, progress summary and some common datasets of applying meta-learning to few-shot NLP.", "venue": "ArXiv", "authors": ["Wenpeng  Yin"], "year": 2020, "n_citations": 23}
{"id": 3645287, "s2_id": "c280c95c17194de83e5234cca3586cd567ecd47c", "title": "Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure", "abstract": "As machine learning systems move from computer-science laboratories into the open world, their accountability becomes a high priority problem. Accountability requires deep understanding of system behavior and its failures. Current evaluation methods such as single-score error metrics and confusion matrices provide aggregate views of system performance that hide important shortcomings. Understanding details about failures is important for identifying pathways for refinement, communicating the reliability of systems in different settings, and for specifying appropriate human oversight and engagement. Characterization of failures and shortcomings is particularly complex for systems composed of multiple machine learned components. For such systems, existing evaluation methods have limited expressiveness in describing and explaining the relationship among input content, the internal states of system components, and final output quality. We present Pandora, a set of hybrid human-machine methods and tools for describing and explaining system failures. Pandora leverages both human and system-generated observations to summarize conditions of system malfunction with respect to the input content and system architecture. We share results of a case study with a machine learning pipeline for image captioning that show how detailed performance views can be beneficial for analysis and debugging.", "venue": "HCOMP", "authors": ["Besmira  Nushi", "Ece  Kamar", "Eric  Horvitz"], "year": 2018, "n_citations": 56}
{"id": 3683954, "s2_id": "5944499d410e01040dea78dc6913a3b097988c49", "title": "Transformer Transducer: One Model Unifying Streaming and Non-streaming Speech Recognition", "abstract": "In this paper we present a Transformer-Transducer model architecture and a training technique to unify streaming and non-streaming speech recognition models into one model. The model is composed of a stack of transformer layers for audio encoding with no lookahead or right context and an additional stack of transformer layers on top trained with variable right context. In inference time, the context length for the variable context layers can be changed to trade off the latency and the accuracy of the model. We also show that we can run this model in a Y-model architecture with the top layers running in parallel in low latency and high latency modes. This allows us to have streaming speech recognition results with limited latency and delayed speech recognition results with large improvements in accuracy (20% relative improvement for voice-search task). We show that with limited right context (1-2 seconds of audio) and small additional latency (50-100 milliseconds) at the end of decoding, we can achieve similar accuracy with models using unlimited audio right context. We also present optimizations for audio and label encoders to speed up the inference in streaming and non-streaming speech decoding.", "venue": "ArXiv", "authors": ["Anshuman  Tripathi", "Jaeyoung  Kim", "Qian  Zhang", "Han  Lu", "Hasim  Sak"], "year": 2020, "n_citations": 15}
{"id": 3686569, "s2_id": "3740cf184a13e33ac2f0debeeaa9fdf8b95aae76", "title": "Gait Recovery System for Parkinson\u2019s Disease using Machine Learning on Embedded Platforms", "abstract": "Freezing of Gait (FoG) is a common gait deficit among patients diagnosed with Parkinson\u2019s Disease (PD). In order to help these patients recover from FoG episodes, Rhythmic Auditory Stimulation (RAS) is needed. The authors propose a ubiquitous embedded system that detects FOG events with a Machine Learning (ML) subsystem from accelerometer signals. By making inferences on-device, we avoid issues prevalent in cloud-based systems such as latency and network connection dependency. The resource-efficient classifier used, reduces the model size requirements by approximately 400 times compared to the best performing standard ML systems, with a trade-off of a mere 1.3% in best classification accuracy. The aforementioned trade-off facilitates deployability in a wide range of embedded devices including microcontroller based systems. The research also explores the optimization procedure to deploy the model on an ATMega2560 microcontroller with a minimum system latency of 44.5 ms. The smallest model size of the proposed resource efficient ML model was 1.4 KB with an average recall score of 93.58%.", "venue": "2020 IEEE International Systems Conference (SysCon)", "authors": ["H  Gokul", "Prithvi  Suresh", "Hari Vignesh Baskar", "R  PravinKumaar", "Vineeth  Vijayaraghavan"], "year": 2020, "n_citations": 2}
{"id": 3702142, "s2_id": "d05c092eba7a4c127b3b69a6183ccb5ea2140522", "title": "DWMD: Dimensional Weighted Orderwise Moment Discrepancy for Domain-specific Hidden Representation Matching", "abstract": "Knowledge transfer from a source domain to a different but semantically related target domain has long been an important topic in the context of unsupervised domain adaptation (UDA). A key challenge in this field is establishing a metric that can exactly measure the data distribution discrepancy between two homogeneous domains and adopt it in distribution alignment, especially in the matching of feature representations in the hidden activation space. Existing distribution matching approaches can be interpreted as failing to either explicitly orderwise align higher-order moments or satisfy the prerequisite of certain assumptions in practical uses. We propose a novel moment-based probability distribution metric termed dimensional weighted orderwise moment discrepancy (DWMD) for feature representation matching in the UDA scenario. Our metric function takes advantage of a series for high-order moment alignment, and we theoretically prove that our DWMD metric function is error-free, which means that it can strictly reflect the distribution differences between domains and is valid without any feature distribution assumption. In addition, since the discrepancies between probability distributions in each feature dimension are different, dimensional weighting is considered in our function. We further calculate the error bound of the empirical estimate of the DWMD metric in practical applications. Comprehensive experiments on benchmark datasets illustrate that our method yields state-of-the-art distribution metrics.", "venue": "ArXiv", "authors": ["Rongzhe  Wei", "Fa  Zhang", "Bo  Dong", "Qinghua  Zheng"], "year": 2020, "n_citations": 0}
{"id": 3704952, "s2_id": "f73ccd90617b3fcbadbe09816c2773360a2df4e5", "title": "Sphynx: ReLU-Efficient Network Design for Private Inference", "abstract": "The emergence of deep learning has been accompanied by privacy concerns surrounding users\u2019 data and service providers\u2019 models. We focus on private inference (PI), where the goal is to perform inference on a user\u2019s data sample using a service provider\u2019s model. Existing PI methods for deep networks enable cryptographically secure inference with little drop in functionality; however, they incur severe latency costs, primarily caused by non-linear network operations (such as ReLUs). This paper presents SPHYNX, a ReLU-efficient network design method based on micro-search strategies for convolutional cell design. SPHYNX achieves Pareto dominance over all existing private inference methods on CIFAR-100. We also design large-scale networks that support cryptographically private inference on Tiny-ImageNet and ImageNet.", "venue": "ArXiv", "authors": ["Minsu  Cho", "Zahra  Ghodsi", "Brandon  Reagen", "Siddharth  Garg", "Chinmay  Hegde"], "year": 2021, "n_citations": 0}
{"id": 3707382, "s2_id": "19501a2db9db38d04918fcaade357347869ee950", "title": "Classifications based on response times for detecting early-stage Alzheimer's disease", "abstract": "Introduction This paper mainly describes a way to detect with high accuracy patients with early-stage Alzheimer\u2019s disease (ES-AD) versus healthy control (HC) subjects, from datasets built with handwriting and drawing task records. Method The proposed approach uses subject\u2019s response times. An optimal subset of tasks is first selected with a \u201cSupport Vector Machine\u201d (SVM) associated with a grid search. Mixtures of Gaussian distributions defined in the space of task durations are then used to reproduce and explain the results of the SVM. Finally, a surprisingly simple and efficient ad hoc classification algorithm is deduced from the Gaussian mixtures. Results The solution presented in this paper makes two or even four times fewer errors than the best results of the state of the art concerning the classification HC/ES-AD from handwriting and drawing tasks. Discussion The best SVM learning model reaches a high accuracy for this classification but its learning capacity is too large to ensure a low overfitting risk regarding the small size of the dataset. The proposed ad hoc classification algorithm only requires to optimize three real-parameters. It should therefore benefit from a good generalization ability.", "venue": "ArXiv", "authors": ["Alain  Petrowski"], "year": 2021, "n_citations": 0}
{"id": 3739706, "s2_id": "a0cd3667dea23ec7b10b8991d5b8a29b55ad064a", "title": "MT-CGCNN: Integrating Crystal Graph Convolutional Neural Network with Multitask Learning for Material Property Prediction", "abstract": "Developing accurate, transferable and computationally inexpensive machine learning models can rapidly accelerate the discovery and development of new materials. Some of the major challenges involved in developing such models are, (i) limited availability of materials data as compared to other fields, (ii) lack of universal descriptor of materials to predict its various properties. The limited availability of materials data can be addressed through transfer learning, while the generic representation was recently addressed by Xie and Grossman [1], where they developed a crystal graph convolutional neural network (CGCNN) that provides a unified representation of crystals. In this work, we develop a new model (MT-CGCNN) by integrating CGCNN with transfer learning based on multi-task (MT) learning. We demonstrate the effectiveness of MT-CGCNN by simultaneous prediction of various material properties such as Formation Energy, Band Gap and Fermi Energy for a wide range of inorganic crystals (46774 materials). MT-CGCNN is able to reduce the test error when employed on correlated properties by upto 8%. The model prediction has lower test error compared to CGCNN, even when the training data is reduced by 10%. We also demonstrate our model's better performance through prediction of end user scenario related to metal/non-metal classification. These results encourage further development of machine learning approaches which leverage multi-task learning to address the aforementioned challenges in the discovery of new materials. We make MT-CGCNN's source code available to encourage reproducible research.", "venue": "ArXiv", "authors": ["Soumya  Sanyal", "Janakiraman  Balachandran", "Naganand  Yadati", "Abhishek  Kumar", "Padmini  Rajagopalan", "Suchismita  Sanyal", "Partha  Talukdar"], "year": 2018, "n_citations": 13}
{"id": 3742146, "s2_id": "623b80ee5e4d834c395ff98fbb47cfe516ca8b33", "title": "PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution", "abstract": "We introduce PreCo, a large-scale English dataset for coreference resolution. The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering. To strengthen the training-test overlap, we collect a large corpus of 38K documents and 12.5M words which are mostly from the vocabulary of English-speaking preschoolers. Experiments show that with higher training-test overlap, error analysis on PreCo is more efficient than the one on OntoNotes, a popular existing dataset. Furthermore, we annotate singleton mentions making it possible for the first time to quantify the influence that a mention detector makes on coreference resolution performance. The dataset is freely available at https://preschool-lab.github.io/PreCo/.", "venue": "EMNLP", "authors": ["Hong  Chen", "Zhenhua  Fan", "Hao  Lu", "Alan L. Yuille", "Shu  Rong"], "year": 2018, "n_citations": 23}
{"id": 3745851, "s2_id": "8d168d38c5971c2540c180c688051a7b9b21e1f6", "title": "An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks", "abstract": "Deep Neural Networks(DNN) have excessively advanced the field of computer vision by achieving state of the art performance in various vision tasks. These results are not limited to the field of vision but can also be seen in speech recognition and machine translation tasks. Recently, DNNs are found to poorly fail when tested with samples that are crafted by making imperceptible changes to the original input images. This causes a gap between the validation and adversarial performance of a DNN. An effective and generalizable robustness metric for evaluating the performance of DNN on these adversarial inputs is still missing from the literature. In this paper, we propose Noise Sensitivity Score (NSS), a metric that quantifies the performance of a DNN on a specific input under different forms of fix-directional attacks. An insightful mathematical explanation is provided for deeply understanding the proposed metric. By leveraging the NSS, we also proposed a skewness based dataset robustness metric for evaluating a DNN's adversarial performance on a given dataset. Extensive experiments using widely used state of the art architectures along with popular classification datasets, such as MNIST, CIFAR-10, CIFAR-100, and ImageNet, are used to validate the effectiveness and generalization of our proposed metrics. Instead of simply measuring a DNN's adversarial robustness in the input domain, as previous works, the proposed NSS is built on top of insightful mathematical understanding of the adversarial attack and gives a more explicit explanation of the robustness.", "venue": "ArXiv", "authors": ["Chirag  Agarwal", "Bo  Dong", "Dan  Schonfeld", "Anthony  Hoogs"], "year": 2018, "n_citations": 1}
{"id": 3752427, "s2_id": "4552cde4624e2f50ebefc0b03e343e814b1f521c", "title": "FedAR: Activity and Resource-Aware Federated Learning Model for Distributed Mobile Robots", "abstract": "Smartphones, autonomous vehicles, and the Internet-of-things (IoT) devices are considered the primary data source for a distributed network. Due to a revolutionary breakthrough in internet availability and continuous improvement of the IoT devices capabilities, it is desirable to store data locally and perform computation at the edge, as opposed to share all local information with a centralized computation agent. A recently proposed Machine Learning (ML) algorithm called Federated Learning (FL) paves the path towards preserving data privacy, performing distributed learning, and reducing communication overhead in large-scale machine learning (ML) problems. This paper proposes an FL model by monitoring client activities and leveraging available local computing resources, particularly for resource-constrained IoT devices (e.g., mobile robots), to accelerate the learning process. We assign a trust score to each FL client, which is updated based on the client\u2019s activities. We consider a distributed mobile robot as an FL client with resource limitations either in memory, bandwidth, processor, or battery life. We consider such mobile robots as FL clients to understand their resource-constrained behavior in a real-world setting. We consider an FL client to be untrustworthy if the client infuses incorrect models or repeatedly gives slow responses during the FL process. After disregarding the ineffective and unreliable client, we perform local training on the selected FL clients. To further reduce the straggler issue, we enable an asynchronous FL mechanism by performing aggregation on the FL server without waiting for a long period to receive a particular client\u2019s response.", "venue": "2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)", "authors": ["Ahmed  Imteaj", "M. Hadi Amini"], "year": 2020, "n_citations": 9}
{"id": 3758155, "s2_id": "0242acadf4940cf94596b54f7fd6fb75af7bb20d", "title": "Beyond Model Extraction: Imitation Attack for Black-Box NLP APIs", "abstract": "Machine-learning-as-a-service (MLaaS) has attracted millions of users to their outperforming sophisticated models. Although published as black-box APIs, the valuable models behind these services are still vulnerable to imitation attacks. Recently, a series of works have demonstrated that attackers manage to steal or extract the victim models. Nonetheless, none of the previous stolen models can outperform the original black-box APIs. In this work, we take the first step of showing that attackers could potentially surpass victims via unsupervised domain adaptation and multivictim ensemble. Extensive experiments on benchmark datasets and real-world APIs validate that the imitators can succeed in outperforming the original black-box models. We consider this as a milestone in the research of imitation attack, especially on NLP APIs, as the superior performance could influence the defense or even publishing strategy of API providers.", "venue": "ArXiv", "authors": ["Qiongkai  Xu", "Xuanli  He", "Lingjuan  Lyu", "Lizhen  Qu", "Gholamreza  Haffari"], "year": 2021, "n_citations": 2}
{"id": 3763549, "s2_id": "425a4a9c0598e4101ca2f2b930f5c6986ce40a99", "title": "Privacy Regularization: Joint Privacy-Utility Optimization in LanguageModels", "abstract": "Neural language models are known to have a high capacity for memorization of training samples. This may have serious privacy im- plications when training models on user content such as email correspondence. Differential privacy (DP), a popular choice to train models with privacy guarantees, comes with significant costs in terms of utility degradation and disparate impact on subgroups of users. In this work, we introduce two privacy-preserving regularization methods for training language models that enable joint optimization of utility and privacy through (1) the use of a discriminator and (2) the inclusion of a novel triplet-loss term. We compare our methods with DP through extensive evaluation. We show the advantages of our regularizers with favorable utility-privacy trade-off, faster training with the ability to tap into existing optimization approaches, and ensuring uniform treatment of under-represented subgroups.", "venue": "NAACL", "authors": ["Fatemehsadat  Mireshghallah", "Huseyin A. Inan", "Marcello  Hasegawa", "Victor  Ruhle", "Taylor  Berg-Kirkpatrick", "Robert  Sim"], "year": 2021, "n_citations": 3}
{"id": 3766283, "s2_id": "3e227ab8259f36757b2fc29f63db2f52f8e77ea2", "title": "Deep Learning for Classification of Hyperspectral Data: A Comparative Review", "abstract": "In recent years, deep-learning techniques revolutionized the way remote sensing data are processed. The classification of hyperspectral data is no exception to the rule, but it has intrinsic specificities that make the application of deep learning less straightforward than with other optical data. This article presents the state of the art of previous machine-learning approaches, reviews the various deeplearning approaches currently proposed for hyperspectral classification, and identifies the problems and difficulties that arise in the implementation of deep neural networks for this task. In particular, the issues of spatial and spectral resolution, data volume, and transfer of models from multimedia images to hyperspectral data are addressed. Additionally, a comparative study of various families of network architectures is provided, and a software toolbox is publicly released to allow experimenting with these methods (https://github.com/nshaud/DeepHyperX). This article is intended for both data scientists with interest in hyperspectral data and remote sensing experts eager to apply deeplearning techniques to their own data set.", "venue": "IEEE Geoscience and Remote Sensing Magazine", "authors": ["Nicolas  Audebert", "Bertrand  Le Saux", "Sebastien  Lefevre"], "year": 2019, "n_citations": 100}
{"id": 3773721, "s2_id": "76f7c06670fbdbb308c2e4109c1ac80c048cd447", "title": "Sparse-Interest Network for Sequential Recommendation", "abstract": "Recent methods in sequential recommendation focus on learning an overall embedding vector from a user's behavior sequence for the next-item recommendation. However, from empirical analysis, we discovered that a user's behavior sequence often contains multiple conceptually distinct items, while a unified embedding vector is primarily affected by one's most recent frequent actions. Thus, it may fail to infer the next preferred item if conceptually similar items are not dominant in recent interactions. To this end, an alternative solution is to represent each user with multiple embedding vectors encoding different aspects of the user's intentions. Nevertheless, recent work on multi-interest embedding usually considers a small number of concepts discovered via clustering, which may not be comparable to the large pool of item categories in real systems. It is a non-trivial task to effectively model a large number of diverse conceptual prototypes, as items are often not conceptually well clustered in fine granularity. Besides, an individual usually interacts with only a sparse set of concepts. In light of this, we propose a novel Sparse Interest NEtwork (SINE) for sequential recommendation. Our sparse-interest module can adaptively infer a sparse set of concepts for each user from the large concept pool and output multiple embeddings accordingly. Given multiple interest embeddings, we develop an interest aggregation module to actively predict the user's current intention and then use it to explicitly model multiple interests for next-item prediction. Empirical results on several public benchmark datasets and one large-scale industrial dataset demonstrate that SINE can achieve substantial improvement over state-of-the-art methods.", "venue": "WSDM", "authors": ["Qiaoyu  Tan", "Jianwei  Zhang", "Jiangchao  Yao", "Ninghao  Liu", "Jingren  Zhou", "Hongxia  Yang", "Xia  Hu"], "year": 2021, "n_citations": 10}
{"id": 3788015, "s2_id": "a850d2cf4628ae3b9f790855ec762997b4e6e77f", "title": "Cluster Naturalistic Driving Encounters Using Deep Unsupervised Learning", "abstract": "Learning knowledge from driving encounters could help self-driving cars make appropriate decisions when driving in complex settings with nearby vehicles engaged. This paper develops an unsupervised classifier to group naturalistic driving encounters into distinguishable clusters by combining an auto-encoder with k-means clustering (AE-kMC). The effectiveness of AE-kMC was validated using the data of 10,000 naturalistic driving encounters which were collected by the University of Michigan, Ann Arbor in the past five years. We compare our developed method with the k-means clustering methods and experimental results demonstrate that the AE-kMC method outperforms the original k-means clustering method.", "venue": "2018 IEEE Intelligent Vehicles Symposium (IV)", "authors": ["Sisi  Li", "Wenshuo  Wang", "Zhaobin  Mo", "Ding  Zhao"], "year": 2018, "n_citations": 11}
{"id": 3798545, "s2_id": "dcb3dc45cb1ff444c59f8741c07901752ffc4d6e", "title": "Bi-Level Poisoning Attack Model and Countermeasure for Appliance Consumption Data of Smart Homes", "abstract": "Accurate building energy prediction is useful in various applications starting from building energy automation and management to optimal storage control. However, vulnerabilities should be considered when designing building energy prediction models, as intelligent attackers can deliberately influence the model performance using sophisticated attack models. These may consequently degrade the prediction accuracy, which may affect the efficiency and performance of the building energy management systems. In this paper, we investigate the impact of bi-level poisoning attacks on regression models of energy usage obtained from household appliances. Furthermore, an effective countermeasure against the poisoning attacks on the prediction model is proposed in this paper. Attacks and defenses are evaluated on a benchmark dataset. Experimental results show that an intelligent cyber-attacker can poison the prediction model to manipulate the decision. However, our proposed solution successfully ensures defense against such poisoning attacks effectively compared to other benchmark techniques.", "venue": "ArXiv", "authors": ["Mustain  Billah", "Adnan  Anwar", "Ziaur  Rahman", "Syed Md. Galib"], "year": 2021, "n_citations": 0}
{"id": 3799167, "s2_id": "9dd202ba65241df4a9ad643b5fe9d369d54d2821", "title": "A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence", "abstract": "Deep learning based localization and mapping has recently attracted significant attention. Instead of creating hand-designed algorithms through exploitation of physical models or geometric theories, deep learning based solutions provide an alternative to solve the problem in a data-driven way. Benefiting from ever-increasing volumes of data and computational power, these methods are fast evolving into a new area that offers accurate and robust systems to track motion and estimate scenes and their structure for real-world applications. In this work, we provide a comprehensive survey, and propose a new taxonomy for localization and mapping using deep learning. We also discuss the limitations of current models, and indicate possible future directions. A wide range of topics are covered, from learning odometry estimation, mapping, to global localization and simultaneous localization and mapping (SLAM). We revisit the problem of perceiving self-motion and scene understanding with on-board sensors, and show how to solve it by integrating these modules into a prospective spatial machine intelligence system (SMIS). It is our hope that this work can connect emerging works from robotics, computer vision and machine learning communities, and serve as a guide for future researchers to apply deep learning to tackle localization and mapping problems.", "venue": "ArXiv", "authors": ["Changhao  Chen", "Bing  Wang", "Chris Xiaoxuan Lu", "Agathoniki  Trigoni", "Andrew  Markham"], "year": 2020, "n_citations": 30}
{"id": 3803082, "s2_id": "78284b07d9df8f9f9e3f36d38393b4112bc5845a", "title": "Deep Active Learning by Model Interpretability", "abstract": "Recent successes of Deep Neural Networks (DNNs) in a variety of research tasks, however, heavily rely on the large amounts of labeled samples. This may require considerable annotation cost in real-world applications. Fortunately, active learning is a promising methodology to train high-performing model with minimal annotation cost. In the deep learning context, the critical question of active learning is how to precisely identify the informativeness of samples for DNN. In this paper, inspired by piece-wise linear interpretability in DNN, we introduce the linearly separable regions of samples to the problem of active learning, and propose a novel Deep Active learning approach by Model Interpretability (DAMI). To keep the maximal representativeness of the entire unlabeled data, DAMI tries to select and label samples on different linearly separable regions introduced by the piece-wise linear interpretability in DNN. We focus on modeling Multi-Layer Perception (MLP) for modeling tabular data. Specifically, we use the local piece-wise interpretation in MLP as the representation of each sample, and directly run K-Center clustering to select and label samples. To be noted, this whole process of DAMI does not require any hyper-parameters to tune manually. To verify the effectiveness of our approach, extensive experiments have been conducted on several tabular datasets. The experimental results demonstrate that DAMI constantly outperforms several state-of-the-art compared approaches.", "venue": "ArXiv", "authors": ["Qiang  Liu", "Zhaocheng  Liu", "Xiaofang  Zhu", "Yeliang  Xiu", "Jun  Zhu"], "year": 2020, "n_citations": 1}
{"id": 3803786, "s2_id": "434bcaf981acec729c299b6b69b06d82df4210b3", "title": "Combining Model-Based and Model-Free Methods for Nonlinear Control: A Provably Convergent Policy Gradient Approach", "abstract": "Model-free learning-based control methods have seen great success recently. However, such methods typically suffer from poor sample complexity and limited convergence guarantees. This is in sharp contrast to classical model-based control, which has a rich theory but typically requires strong modeling assumptions. In this paper, we combine the two approaches to achieve the best of both worlds. We consider a dynamical system with both linear and non-linear components and develop a novel approach to use the linear model to define a warm start for a model-free, policy gradient method. We show this hybrid approach outperforms the model-based controller while avoiding the convergence issues associated with model-free approaches via both numerical experiments and theoretical analyses, in which we derive sufficient conditions on the non-linear component such that our approach is guaranteed to converge to the (nearly) global optimal controller.", "venue": "ArXiv", "authors": ["Guannan  Qu", "Chenkai  Yu", "Steven  Low", "Adam  Wierman"], "year": 2020, "n_citations": 5}
{"id": 3814558, "s2_id": "250a5043ac8147d5d3a52134d64900b54ae6f6c7", "title": "Policy Gradient Methods Find the Nash Equilibrium in N-player General-sum Linear-quadratic Games", "abstract": "We consider a general-sum N-player linear-quadratic game with stochastic dynamics over a finite horizon and prove the global convergence of the natural policy gradient method to the Nash equilibrium. In order to prove convergence of the method we require a certain amount of noise in the system. We give a condition, essentially a lower bound on the covariance of the noise in terms of the model parameters, in order to guarantee convergence. We illustrate our results with numerical experiments to show that even in situations where the policy gradient method may not converge in the deterministic setting, the addition of noise leads to convergence.", "venue": "ArXiv", "authors": ["Ben  Hambly", "Renyuan  Xu", "Huining  Yang"], "year": 2021, "n_citations": 2}
{"id": 3822391, "s2_id": "b4e5c082ce9b72abbfbb9f9511244d59e0c8401f", "title": "Approximate Message Passing with Spectral Initialization for Generalized Linear Models", "abstract": "We consider the problem of estimating a signal from measurements obtained via a generalized linear model. We focus on estimators based on approximate message passing (AMP), a family of iterative algorithms with many appealing features: the performance of AMP in the high-dimensional limit can be succinctly characterized under suitable model assumptions; AMP can also be tailored to the empirical distribution of the signal entries, and for a wide class of estimation problems, AMP is conjectured to be optimal among all polynomial-time algorithms. \nHowever, a major issue of AMP is that in many models (such as phase retrieval), it requires an initialization correlated with the ground-truth signal and independent from the measurement matrix. Assuming that such an initialization is available is typically not realistic. In this paper, we solve this problem by proposing an AMP algorithm initialized with a spectral estimator. With such an initialization, the standard AMP analysis fails since the spectral estimator depends in a complicated way on the design matrix. Our main technical contribution is the construction and analysis of a two-phase artificial AMP algorithm that first produces the spectral estimator, and then closely approximates the iterates of the true AMP. Our analysis yields a rigorous characterization of the performance of AMP with spectral initialization in the high-dimensional limit. We also provide numerical results that demonstrate the validity of the proposed approach.", "venue": "AISTATS", "authors": ["Marco  Mondelli", "Ramji  Venkataramanan"], "year": 2021, "n_citations": 4}
{"id": 3826342, "s2_id": "a7c5f2cbb1553f3de2b415b962558cb7437f0fdf", "title": "Editing Factual Knowledge in Language Models", "abstract": "The factual knowledge acquired during pretraining and stored in the parameters of Language Models (LM) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KNOWLEDGEEDITOR, a method which can be used to edit this knowledge and, thus, fix \u2018bugs\u2019 or unexpected predictions without the need for expensive retraining or fine-tuning. Besides being computationally efficient, KNOWLEDGEEDITOR does not require any modifications in LM pretraining (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KNOWLEDGEEDITOR\u2019s efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a \u2018probe\u2019 revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components.1", "venue": "EMNLP", "authors": ["Nicola De Cao", "Wilker  Aziz", "Ivan  Titov"], "year": 2021, "n_citations": 8}
{"id": 3839168, "s2_id": "808aadd71333cfc6ebcb66c0f24d1a03d806acc5", "title": "Restricted Isometry Property under High Correlations", "abstract": "Matrices satisfying the Restricted Isometry Property (RIP) play an important role in the areas of compressed sensing and statistical learning. RIP matrices with optimal parameters are mainly obtained via probabilistic arguments, as explicit constructions seem hard. It is therefore interesting to ask whether a fixed matrix can be incorporated into a construction of restricted isometries. In this paper, we construct a new broad ensemble of random matrices with dependent entries that satisfy the restricted isometry property. Our construction starts with a fixed (deterministic) matrix $X$ satisfying some simple stable rank condition, and we show that the matrix $XR$, where $R$ is a random matrix drawn from various popular probabilistic models (including, subgaussian, sparse, low-randomness, satisfying convex concentration property), satisfies the RIP with high probability. These theorems have various applications in signal recovery, random matrix theory, dimensionality reduction, etc. Additionally, motivated by an application for understanding the effectiveness of word vector embeddings popular in natural language processing and machine learning applications, we investigate the RIP of the matrix $XR^{(l)}$ where $R^{(l)}$ is formed by taking all possible (disregarding order) $l$-way entrywise products of the columns of a random matrix $R$.", "venue": "ArXiv", "authors": ["Shiva Prasad Kasiviswanathan", "Mark  Rudelson"], "year": 2019, "n_citations": 6}
{"id": 3841111, "s2_id": "30741e7e30976f7b482eef3d00295ae4740ec21b", "title": "On-Line Building Energy Optimization Using Deep Reinforcement Learning", "abstract": "Unprecedented high volumes of data are becoming available with the growth of the advanced metering infrastructure. These are expected to benefit planning and operation of the future power systems and to help customers transition from a passive to an active role. In this paper, we explore for the first time in the smart grid context the benefits of using deep reinforcement learning, a hybrid type of methods that combines reinforcement learning with deep learning, to perform on-line optimization of schedules for building energy management systems. The learning procedure was explored using two methods, Deep Q-learning and deep policy gradient, both of which have been extended to perform multiple actions simultaneously. The proposed approach was validated on the large-scale Pecan Street Inc. database. This highly dimensional database includes information about photovoltaic power generation, electric vehicles and buildings appliances. Moreover, these on-line energy scheduling strategies could be used to provide real-time feedback to consumers to encourage more efficient use of electricity.", "venue": "IEEE Transactions on Smart Grid", "authors": ["Elena  Mocanu", "Decebal Constantin Mocanu", "Phuong H. Nguyen", "Antonio  Liotta", "Michael E. Webber", "Madeleine  Gibescu", "J. G. Slootweg"], "year": 2019, "n_citations": 230}
{"id": 3841841, "s2_id": "b7456c48802e6bbccaa99fbc063df92ecd7db04a", "title": "Predicting kills in Game of Thrones using network properties", "abstract": "TV series such as HBO's Game of Thrones have seen a high number of dedicated followers, mostly due to the dramatic murders of the most important characters. In our work, we try to predict killer and victim pairs using data about previous kills and additional metadata. We construct a network where two character nodes are linked if one killed the other and use a link prediction framework to evaluate different techniques for kill predictions. Lastly, we compute various network properties on a social network of characters and use them as features in conjunction with classic data mining techniques. Due to the small size of the dataset and the somewhat random kill distribution, we cannot predict much with standard indices alone, although using them in conjunction with additional rules based on degrees works surprisingly well. The features we compute on the social network help the classic machine learning approaches, but do not yield very accurate predictions. The best results overall are achieved using indices that use simple degree information, the best of which gives us the Area Under the ROC Curve of 0.875.", "venue": "ArXiv", "authors": ["Jaka  Stavanja", "Matej  Klemen"], "year": 2019, "n_citations": 3}
{"id": 3862586, "s2_id": "44b6d43ddff1f2d677119643a6030aa039d182ea", "title": "On the sample complexity of learning graphical games", "abstract": "We analyze the sample complexity of learning graphical games from purely behavioral data. We assume that we can only observe the players' joint actions and not their payoffs. We analyze the sufficient and necessary number of samples for the correct recovery of the set of pure-strategy Nash equilibria (PSNE) of the true game. Our analysis focuses on directed graphs with n nodes and at most k parents per node. Sparse graphs correspond to k \u220a O(1) with respect to n, while dense graphs correspond to k \u220a O(n). By using VC dimension arguments, we show that if the number of samples is greater than O(kn log2 n) for sparse graphs or O(n2 log n) for dense graphs, then maximum likelihood estimation correctly recovers the PSNE with high probability. By using information-theoretic arguments, we show that if the number of samples is less than Q(kn log2 n) for sparse graphs or Q(n2 log n) for dense graphs, then any conceivable method fails to recover the PSNE with arbitrary probability.", "venue": "2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)", "authors": ["Jean  Honorio"], "year": 2017, "n_citations": 0}
{"id": 3875787, "s2_id": "071cf7be1a66649e0d5e15858a4bfe2e89481429", "title": "Lessons Learned from Applying off-the-shelf BERT: There is no Silver Bullet", "abstract": "One of the challenges in the NLP field is training large classification models, a task that is both difficult and tedious. It is even harder when GPU hardware is unavailable. The increased availability of pre-trained and off-the-shelf word embeddings, models, and modules aim at easing the process of training large models and achieving a competitive performance. We explore the use of off-the-shelf BERT models and share the results of our experiments and compare their results to those of LSTM networks and more simple baselines. We show that the complexity and computational cost of BERT is not a guarantee for enhanced predictive performance in the classification tasks at hand.", "venue": "ArXiv", "authors": ["Victor  Makarenkov", "Lior  Rokach"], "year": 2020, "n_citations": 2}
{"id": 3878105, "s2_id": "a169c94d910540c6d30b6d8fe53983eb7b610ec9", "title": "A Novel Bayesian Classifier using Copula Functions", "abstract": "A useful method for representing Bayesian classifiers is through \\emph{discriminant functions}. Here, using copula functions, we propose a new model for discriminants. This model provides a rich and generalized class of decision boundaries. These decision boundaries significantly boost the classification accuracy especially for high dimensional feature spaces. We strengthen our analysis through simulation results.", "venue": "ArXiv", "authors": ["Saket  Sathe"], "year": 2006, "n_citations": 7}
{"id": 3919483, "s2_id": "a403099192180450edc594d2fed7a19296e9ff48", "title": "Towards Robust, Locally Linear Deep Networks", "abstract": "Deep networks realize complex mappings that are often understood by their locally linear behavior at or around points of interest. For example, we use the derivative of the mapping with respect to its inputs for sensitivity analysis, or to explain (obtain coordinate relevance for) a prediction. One key challenge is that such derivatives are themselves inherently unstable. In this paper, we propose a new learning problem to encourage deep networks to have stable derivatives over larger regions. While the problem is challenging in general, we focus on networks with piecewise linear activation functions. Our algorithm consists of an inference step that identifies a region around a point where linear approximation is provably stable, and an optimization step to expand such regions. We propose a novel relaxation to scale the algorithm to realistic models. We illustrate our method with residual and recurrent networks on image and sequence datasets.", "venue": "ICLR", "authors": ["Guang-He  Lee", "David  Alvarez-Melis", "Tommi S. Jaakkola"], "year": 2019, "n_citations": 24}
{"id": 3925358, "s2_id": "33ba1c0cd1857b107d658cb900be75083c5a35b3", "title": "Minimax Filter: Learning to Preserve Privacy from Inference Attacks", "abstract": "Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios, can be challenging with syntactic anonymization methods which are designed for discrete attributes. Differential privacy, which provides a more formal definition of privacy, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately infer sensitive attributes from sanitized data. The paper proposes a novel filter-based mechanism which preserves privacy of continuous and high-dimensional attributes against inference attacks. Finding the optimal utility-privacy tradeoff is formulated as a min-diff-max optimization problem. The paper provides an ERM-like analysis of the generalization error and also a practical algorithm to perform the optimization. In addition, the paper proposes an extension that combines minimax filter and differentially-private noisy mechanism. Advantages of the method over purely noisy mechanisms is explained and demonstrated with examples. Experiments with several real-world tasks including facial expression classification, speech emotion classification, and activity classification from motion, show that the minimax filter can simultaneously achieve similar or better target task accuracy and lower inference accuracy, often significantly lower than previous methods.", "venue": "J. Mach. Learn. Res.", "authors": ["Jihun  Hamm"], "year": 2017, "n_citations": 58}
{"id": 3932764, "s2_id": "111da19e8911ba04cdd38fec42587c8b8c976287", "title": "A ground-truth dataset and classification model for detecting bots in GitHub issue and PR comments", "abstract": "Bots are frequently used in Github repositories to automate repetitive activities that are part of the distributed software development process. They communicate with human actors through comments. While detecting their presence is important for many reasons, no large and representative ground-truth dataset is available, nor are classification models to detect and validate bots on the basis of such a dataset. This paper proposes such a ground-truth dataset, based on a manual analysis with high interrater agreement, of pull request and issue comments in 5,000 distinct Github accounts of which 527 accounts have been identified as bots. Using this dataset we propose an automated classification model based on the random forest classifier, taking as main features the number of empty and non-empty comments of each account, the number of comment patterns, and the inequality between comments within comment patterns. We obtained a very high accuracy (weighted F1-score of 0.99) on the remaining test set containing 40% of the data. Only 8 out of 211 bots in the test set are misclassified as humans. We integrated the classification model into an open source command-line tool, to allow practitioners to detect which accounts in a given Github repository actually correspond to bots.", "venue": "J. Syst. Softw.", "authors": ["Mehdi  Golzadeh", "Alexandre  Decan", "Damien  Legay", "Tom  Mens"], "year": 2021, "n_citations": 9}
{"id": 3954237, "s2_id": "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).", "venue": "ACL", "authors": ["Kai Sheng Tai", "Richard  Socher", "Christopher D. Manning"], "year": 2015, "n_citations": 2340}
{"id": 3983866, "s2_id": "e7926ed72ac5ad7b9f7aaf19c27412ae2db8eafd", "title": "Generalization Bounds for Weighted Automata", "abstract": "This paper studies the problem of learning weighted automata from a finite labeled training sample. We consider several general families of weighted automata defined in terms of three different measures: the norm of an automaton's weights, the norm of the function computed by an automaton, or the norm of the corresponding Hankel matrix. We present new data-dependent generalization guarantees for learning weighted automata expressed in terms of the Rademacher complexity of these families. We further present upper bounds on these Rademacher complexities, which reveal key new data-dependent terms related to the complexity of learning weighted automata.", "venue": "ArXiv", "authors": ["Borja  Balle", "Mehryar  Mohri"], "year": 2016, "n_citations": 0}
{"id": 4009671, "s2_id": "8fd17fcf75810cead21e45e67d4a98a5ff3f3707", "title": "On the Evaluation of Dialogue Systems with Next Utterance Classification", "abstract": "An open challenge in constructing dialogue systems is developing methods for automatically learning dialogue strategies from large amounts of unlabelled data. Recent work has proposed Next-Utterance-Classification (NUC) as a surrogate task for building dialogue systems from text data. In this paper we investigate the performance of humans on this task to validate the relevance of NUC as a method of evaluation. Our results show three main findings: (1) humans are able to correctly classify responses at a rate much better than chance, thus confirming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus confirming the utility of this class of tasks for driving further research in automated dialogue systems.", "venue": "SIGDIAL Conference", "authors": ["Ryan  Lowe", "Iulian  Serban", "Michael  Noseworthy", "Laurent  Charlin", "Joelle  Pineau"], "year": 2016, "n_citations": 55}
{"id": 4011128, "s2_id": "cd6cb85e4a1ca3c141dac1725c8be0b512d77dd2", "title": "NFL Play Prediction", "abstract": "Based on NFL game data we try to predict the outcome of a play in multiple different ways. An application of this is the following: by plugging in various play options one could determine the best play for a given situation in real time. While the outcome of a play can be described in many ways we had the most promising results with a newly defined measure that we call \"progress\". We see this work as a first step to include predictive analysis into NFL playcalling.", "venue": "ArXiv", "authors": ["Brendan  Teich", "Roman  Lutz", "Valentin  Kassarnig"], "year": 2016, "n_citations": 2}
{"id": 4022730, "s2_id": "ef357174ed84714dbbcb48b53b65ae2d0e2a7dcb", "title": "Differentially Private Sliced Wasserstein Distance", "abstract": "Developing machine learning methods that are privacy preserving is today a central topic of research, with huge practical impacts. Among the numerous ways to address privacy-preserving learning, we here take the perspective of computing the divergences between distributions under the Differential Privacy (DP) framework \u2014 being able to compute divergences between distributions is pivotal for many machine learning problems, such as learning generative models or domain adaptation problems. Instead of resorting to the popular gradient-based sanitization method for DP, we tackle the problem at its roots by focusing on the Sliced Wasserstein Distance and seamlessly making it differentially private. Our main contribution is as follows: we analyze the property of adding a Gaussian perturbation to the intrinsic randomized mechanism of the Sliced Wasserstein Distance, and we establish the sensitivity of the resulting differentially private mechanism. One of our important findings is that this DP mechanism transforms the Sliced Wasserstein distance into another distance, that we call the Smoothed Sliced Wasserstein Distance. This new differentially private distribution distance can be plugged into generative models and domain adaptation algorithms in a transparent way, and we empirically show that it yields highly competitive performance compared with gradient-based DP approaches from the literature, with almost no loss in accuracy for the domain adaptation problems that we consider.", "venue": "ICML", "authors": ["Alain  Rakotomamonjy", "Liva  Ralaivola"], "year": 2021, "n_citations": 1}
{"id": 4050190, "s2_id": "6174a5c245d9daac0c1e728622e93686bd565e2e", "title": "Transfer learning extensions for the probabilistic classification vector machine", "abstract": "Abstract Transfer learning is focused on the reuse of supervised learning models in a new context. Prominent applications can be found in robotics, image processing or web mining. In these fields, the learning scenarios are naturally changing but often remain related to each other motivating the reuse of existing supervised models. Current transfer learning models are neither sparse nor interpretable. Sparsity is very desirable if the methods have to be used in technically limited environments and interpretability is getting more critical due to privacy regulations. In this work, we propose two transfer learning extensions integrated into the sparse and interpretable probabilistic classification vector machine. They are compared to standard benchmarks in the field and show their relevance either by sparsity or performance improvements.", "venue": "Neurocomputing", "authors": ["Christoph  Raab", "Frank-Michael  Schleif"], "year": 2020, "n_citations": 5}
{"id": 4116819, "s2_id": "475315fa75357c116e845ee98d7a1410dd07887e", "title": "Automated Architecture Design for Deep Neural Networks", "abstract": "Machine learning has made tremendous progress in recent years and received large amounts of public attention. Though we are still far from designing a full artificially intelligent agent, machine learning has brought us many applications in which computers solve human learning tasks remarkably well. Much of this progress comes from a recent trend within machine learning, called deep learning. Deep learning models are responsible for many state-of-the-art applications of machine learning. Despite their success, deep learning models are hard to train, very difficult to understand, and often times so complex that training is only possible on very large GPU clusters. Lots of work has been done on enabling neural networks to learn efficiently. However, the design and architecture of such neural networks is often done manually through trial and error and expert knowledge. This thesis inspects different approaches, existing and novel, to automate the design of deep feedforward neural networks in an attempt to create less complex models with good performance that take away the burden of deciding on an architecture and make it more efficient to design and train such deep networks.", "venue": "ArXiv", "authors": ["Steven  Abreu"], "year": 2019, "n_citations": 4}
{"id": 4120760, "s2_id": "b993b5fa8139e178e8e0262bd0ce62f838db7c64", "title": "Accountable Error Characterization", "abstract": "Customers of machine learning systems demand accountability from the companies employing these algorithms for various prediction tasks. Accountability requires understanding of system limit and condition of erroneous predictions, as customers are often interested in understanding the incorrect predictions, and model developers are absorbed in finding methods that can be used to get incremental improvements to an existing system. Therefore, we propose an accountable error characterization method, AEC, to understand when and where errors occur within the existing black-box models. AEC, as constructed with human-understandable linguistic features, allows the model developers to automatically identify the main sources of errors for a given classification system. It can also be used to sample for the set of most informative input points for a next round of training. We perform error detection for a sentiment analysis task using AEC as a case study. Our results on the sample sentiment task show that AEC is able to characterize erroneous predictions into human understandable categories and also achieves promising results on selecting erroneous samples when compared with the uncertainty-based sampling.", "venue": "TRUSTNLP", "authors": ["Amita  Misra", "Zhe  Liu", "Jalal  Mahmud"], "year": 2021, "n_citations": 0}
{"id": 4146955, "s2_id": "007dce37ba6bc87d50cfb3fdc9d2bfb1d1ecd0c9", "title": "Investigation of REFINED CNN ensemble learning for anti-cancer drug sensitivity prediction", "abstract": "Abstract Motivation Anti-cancer drug sensitivity prediction using deep learning models for individual cell line is a significant challenge in personalized medicine. Recently developed REFINED (REpresentation of Features as Images with NEighborhood Dependencies) CNN (Convolutional Neural Network)-based models have shown promising results in improving drug sensitivity prediction. The primary idea behind REFINED-CNN is representing high dimensional vectors as compact images with spatial correlations that can benefit from CNN architectures. However, the mapping from a high dimensional vector to a compact 2D image depends on the a priori choice of the distance metric and projection scheme with limited empirical procedures guiding these choices. Results In this article, we consider an ensemble of REFINED-CNN built under different choices of distance metrics and/or projection schemes that can improve upon a single projection based REFINED-CNN model. Results, illustrated using NCI60 and NCI-ALMANAC databases, demonstrate that the ensemble approaches can provide significant improvement in prediction performance as compared to individual models. We also develop the theoretical framework for combining different distance metrics to arrive at a single 2D mapping. Results demonstrated that distance-averaged REFINED-CNN produced comparable performance as obtained from stacking REFINED-CNN ensemble but with significantly lower computational cost. Availability and implementation The source code, scripts, and data used in the paper have been deposited in GitHub (https://github.com/omidbazgirTTU/IntegratedREFINED). Supplementary information Supplementary data are available at Bioinformatics online.", "venue": "Bioinform.", "authors": ["Omid  Bazgir", "Souparno  Ghosh", "Ranadip  Pal"], "year": 2021, "n_citations": 1}
{"id": 4166404, "s2_id": "9098ff0aaa13f64e72676765059e08c926411f58", "title": "Deep Learning and Statistical Models for Time-Critical Pedestrian Behaviour Prediction", "abstract": "The time it takes for a classifier to make an accurate prediction can be crucial in many behaviour recognition problems. For example, an autonomous vehicle should detect hazardous pedestrian behaviour early enough for it to take appropriate measures. In this context, we compare the switching linear dynamical system (SLDS) and a three-layered bi-directional long short-term memory (LSTM) neural network, which are applied to infer pedestrian behaviour from motion tracks. We show that, though the neural network model achieves an accuracy of 80%, it requires long sequences to achieve this (100 samples or more). The SLDS, has a lower accuracy of 74%, but it achieves this result with short sequences (10 samples). To our knowledge, such a comparison on sequence length has not been considered in the literature before. The results provide a key intuition of the suitability of the models in time-critical problems.", "venue": "ICONIP", "authors": ["Joel Janek Dabrowski", "Johan Pieter de Villiers", "Ashfaqur  Rahman", "Conrad  Beyers"], "year": 2019, "n_citations": 0}
{"id": 4202063, "s2_id": "85e53712730f67bdb144f765c7cc4ed2f326655c", "title": "Affective Movement Generation using Laban Effort and Shape and Hidden Markov Models", "abstract": "Body movements are an important communication medium through which affective states can be discerned. Movements that convey affect can also give machines life-like attributes and help to create a more engaging human-machine interaction. This paper presents an approach for automatic affective movement generation that makes use of two movement abstractions: 1) Laban movement analysis (LMA), and 2) hidden Markov modeling. The LMA provides a systematic tool for an abstract representation of the kinematic and expressive characteristics of movements. Given a desired motion path on which a target emotion is to be overlaid, the proposed approach searches a labeled dataset in the LMA Effort and Shape space for similar movements to the desired motion path that convey the target emotion. An HMM abstraction of the identified movements is obtained and used with the desired motion path to generate a novel movement that is a modulated version of the desired motion path that conveys the target emotion. The extent of modulation can be varied, trading-off between kinematic and affective constraints in the generated movement. The proposed approach is tested using a full-body movement dataset. The efficacy of the proposed approach in generating movements with recognizable target emotions is assessed using a validated automatic recognition model and a user study. The target emotions were correctly recognized from the generated movements at a rate of 72% using the recognition model. Furthermore, participants in the user study were able to correctly perceive the target emotions from a sample of generated movements, although some cases of confusion were also observed.", "venue": "ArXiv", "authors": ["Ali  Samadani", "Rob  Gorbet", "Dana  Kulic"], "year": 2020, "n_citations": 0}
{"id": 4209550, "s2_id": "08bd089b9faeb19178685e917f8be47dc2a959f2", "title": "Toward Neural-Network-Guided Program Synthesis and Verification", "abstract": "We propose a novel framework of program and invariant synthesis called neural network-guided synthesis. We first show that, by suitably designing and training neural networks, we can extract logical formulas over integers from the weights and biases of the trained neural networks. Based on the idea, we have implemented a tool to synthesize formulas from positive/negative examples and implication constraints, and obtained promising experimental results. We also discuss two applications of our synthesis method. One is the use of our tool for qualifier discovery in the framework of ICE-learning-based CHC solving, which can in turn be applied to program verification and inductive invariant synthesis. Another application is to a new program development framework called oracle-based programming, which is a neural-network-guided variation of Solar-Lezama's program synthesis by sketching.", "venue": "SAS", "authors": ["Naoki  Kobayashi", "Taro  Sekiyama", "Issei  Sato", "Hiroshi  Unno"], "year": 2021, "n_citations": 1}
{"id": 4221604, "s2_id": "4c0be6974a92205a249ddeadd31c88f2ed382573", "title": "The Limit of the Batch Size", "abstract": "Large-batch training is an efficient approach for current distributed deep learning systems. It has enabled researchers to reduce the ImageNet/ResNet-50 training from 29 hours to around 1 minute. In this paper, we focus on studying the limit of the batch size. We think it may provide a guidance to AI supercomputer and algorithm designers. We provide detailed numerical optimization instructions for step-by-step comparison. Moreover, it is important to understand the generalization and optimization performance of huge batch training. Hoffer et al. introduced \"ultra-slow diffusion\" theory to large-batch training. However, our experiments show contradictory results with the conclusion of Hoffer et al. We provide comprehensive experimental results and detailed analysis to study the limitations of batch size scaling and \"ultra-slow diffusion\" theory. For the first time we scale the batch size on ImageNet to at least a magnitude larger than all previous work, and provide detailed studies on the performance of many state-of-the-art optimization schemes under this setting. We propose an optimization recipe that is able to improve the top-1 test accuracy by 18% compared to the baseline.", "venue": "ArXiv", "authors": ["Yang  You", "Yuhui  Wang", "Huan  Zhang", "Zhao  Zhang", "James  Demmel", "Cho-Jui  Hsieh"], "year": 2020, "n_citations": 2}
{"id": 4221833, "s2_id": "d3039004048f9f91975081d0f30052d9d08711a7", "title": "Distributed TD(0) with Almost No Communication", "abstract": "We provide a new non-asymptotic analysis of distributed TD(0) with linear function approximation. Our approach relies on \u201cone-shot averaging,\u201d where N agents run local copies of TD(0) and average the outcomes only once at the very end. We consider two models: one in which the agents interact with an environment they can observe and whose transitions depends on all of their actions (which we call the global state model), and one in which each agent can run a local copy of an identical Markov Decision Process, which we call the local state model. In the global state model, we show that the convergence rate of our distributed one-shot averaging method matches the known convergence rate of TD(0). By contrast, the best convergence rate in the previous literature showed a rate which, in the worst case, underperformed the non-distributed version by O(N) in terms of the number of agents N. In the local state model, we demonstrate a version of the linear time speedup phenomenon, where the convergence time of the distributed process is a factor of N faster than the convergence time of TD(0). As far as we are aware, this is the first result rigorously showing benefits from parallelism for temporal difference methods.", "venue": "ArXiv", "authors": ["Rui  Liu", "Alex  Olshevsky"], "year": 2021, "n_citations": 1}
{"id": 4229918, "s2_id": "6e6018152ca739bdc88bf9018bc936f11c1b3f6a", "title": "NATS-Bench: Benchmarking NAS algorithms for Architecture Topology and Size", "abstract": "Neural architecture search (NAS) has attracted much attention and has been illustrated to bring tangible benefits in a large number of applications in the past few years. Architecture topology and architecture size have been regarded as two of the most important aspects for the performance of deep learning models and the community has spawned lots of searching algorithms for both of those aspects of the neural architectures. However, the performance gain from these searching algorithms is achieved under different search spaces and training setups. This makes the overall performance of the algorithms incomparable and the improvement from a sub-module of the searching model unclear. In this paper, we propose NATS-Bench, a unified benchmark on searching for both topology and size, for (almost) any up-to-date algorithm. NATS-Bench includes the search space of 15,625 neural cell candidates for architecture topology and 32,768 for architecture size on three datasets. We analyze the validity of our benchmark in terms of various criteria and performance comparison of all candidates in the search space. We show the versatility of NATS-Bench by benchmarking 13 recent state-of-the-art NAS algorithms. This facilitates a much larger community of researchers to focus on developing better algorithms in a more comparable environment.", "venue": "IEEE transactions on pattern analysis and machine intelligence", "authors": ["Xuanyi  Dong", "Lu  Liu", "Katarzyna  Musial", "Bogdan  Gabrys"], "year": 2021, "n_citations": 25}
{"id": 4256373, "s2_id": "d49987308963cc9cba8a2e44d356c779df51b7fd", "title": "Rethinking Fully Convolutional Networks for the Analysis of Photoluminescence Wafer Images", "abstract": "The manufacturing of light-emitting diodes is a complex semiconductor-manufacturing process, interspersed with different measurements. Among the employed measurements, photoluminescence imaging has several advantages, namely being a non-destructive, fast and thus cost-effective measurement. On a photoluminescence measurement image of an LED wafer, every pixel corresponds to an LED chip's brightness after photo-excitation, revealing chip performance information. However, generating a chip-fine defect map of the LED wafer, based on photoluminescence images, proves challenging for multiple reasons: on the one hand, the measured brightness values vary from image to image, in addition to local spots of differing brightness. On the other hand, certain defect structures may assume multiple shapes, sizes and brightness gradients, where salient brightness values may correspond to defective LED chips, measurement artefacts or non-defective structures. In this work, we revisit the creation of chip-fine defect maps using fully convolutional networks and show that the problem of segmenting objects at multiple scales can be improved by the incorporation of densely connected convolutional blocks and atrous spatial pyramid pooling modules. We also share implementation details and our experiences with training networks with small datasets of measurement images. The proposed architecture significantly improves the segmentation accuracy of highly variable defect structures over our previous version.", "venue": "ArXiv", "authors": ["Maike Lorena Stern", "Hans  Lindberg", "Klaus  Meyer-Wegener"], "year": 2020, "n_citations": 0}
{"id": 4272669, "s2_id": "ac36d410f70fb112268fdaecc7f2a4416d1807c7", "title": "Maximum and Leaky Maximum Propagation", "abstract": "In this work, we present an alternative to conventional residual connections, which is inspired by maxout nets. This means that instead of the addition in residual connections, our approach only propagates the maximum value or, in the leaky formulation, propagates a percentage of both. In our evaluation, we show on different public data sets that the presented approaches are comparable to the residual connections and have other interesting properties, such as better generalization with a constant batch normalization, faster learning, and also the possibility to generalize without additional activation functions. In addition, the proposed approaches work very well if ensembles together with residual networks are formed. Code link: https://atreus.informatik.unituebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FMaximumPropagation&mode=list", "venue": "ArXiv", "authors": ["Wolfgang  Fuhl"], "year": 2021, "n_citations": 1}
{"id": 4275993, "s2_id": "638e41912f314c74436205aa8d332dca963ab1dc", "title": "Parameterized quantum circuits as machine learning models", "abstract": "Hybrid quantum-classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be thought of as machine learning models with remarkable expressive power. This Review presents components of these models and discusses their application to a variety of data-driven tasks such as supervised learning and generative modeling. With experimental demonstrations carried out on actual quantum hardware, and with software actively being developed, this rapidly growing field could become one of the first instances of quantum computing that addresses real world problems.", "venue": "Quantum Science and Technology", "authors": ["Marcello  Benedetti", "Erika  Lloyd", "Stefan  Sack"], "year": 2019, "n_citations": 214}
{"id": 4278795, "s2_id": "0ec724a5a8837e36522f1aba890ddebc347180eb", "title": "Deceiving Google\u2019s Cloud Video Intelligence API Built for Summarizing Videos", "abstract": "Despite the rapid progress of the techniques for image classification, video annotation has remained a challenging task. Automated video annotation would be a breakthrough technology, enabling users to search within the videos. Recently, Google introduced the Cloud Video Intelligence API for video analysis. As per the website, the system can be used to \"separate signal from noise, by retrieving relevant information at the video, shot or per frame\" level. A demonstration website has been also launched, which allows anyone to select a video for annotation. The API then detects the video labels (objects within the video) as well as shot labels (description of the video events over time).,,,,,,In this paper, we examine the usability of the Google's Cloud Video Intelligence API in adversarial environments. In particular, we investigate whether an adversary can subtly manipulate a video in such a way that the API will return only the adversary-desired labels. For this, we select an image, which is different from the video content, and insert it, periodically and at a very low rate, into the video. We found that if we insert one image every two seconds, the API is deceived into annotating the video as if it only contained the inserted image. Note that the modification to the video is hardly noticeable as, for instance, for a typical frame rate of 25, we insert only one image per 50 video frames. We also found that, by inserting one image per second, all the shot labels returned by the API are related to the inserted image. We perform the experiments on the sample videos provided by the API demonstration website and show that our attack is successful with different videos and images.", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": ["Hossein  Hosseini", "Baicen  Xiao", "Radha  Poovendran"], "year": 2017, "n_citations": 16}
{"id": 4288038, "s2_id": "3eb1f7c547375030447b2af7059ece960d2602bc", "title": "Multi-Model Least Squares-Based Recomputation Framework for Large Data Analysis", "abstract": "Most multilayer least squares (LS)-based neural networks are structured with two separate stages: unsupervised feature encoding and supervised pattern classification. Once the unsupervised learning is finished, the latent encoding would be fixed without supervised fine-tuning. However, in complex tasks such as handling the ImageNet dataset, there are often many more clues that can be directly encoded, while the unsupervised learning, by definition cannot know exactly what is useful for a certain task. This serves as the motivation to retrain the latent space representations to learn some clues that unsupervised learning has not yet learned. In particular, the error matrix from the output layer is pulled back to each hidden layer, and the parameters of the hidden layer are recalculated with Moore-Penrose (MP) inverse for more generalized representations. In this paper, a recomputation-based multilayer network using MP inverse (RML-MP) is developed. A sparse RML-MP (SRML-MP) model to boost the performance of RML-MP is then proposed. The experimental results with varying training samples (from 3K to 1.8M ) show that the proposed models provide better generalization performance than most representation learning algorithms.", "venue": "ArXiv", "authors": ["Wandong  Zhang", "Q. M. Jonathan Wu", "Yimin  Yang", "W. G. Will Zhao", "Hui  Zhang"], "year": 2021, "n_citations": 0}
{"id": 4308034, "s2_id": "57b3a6673c00060a0e4f052dc0d1e12a60dfaaf3", "title": "Bayesian Structure Learning for Markov Random Fields with a Spike and Slab Prior", "abstract": "In recent years a number of methods have been developed for automatically learning the (sparse) connectivity structure of Markov Random Fields. These methods are mostly based on L1-regularized optimization which has a number of disadvantages such as the inability to assess model uncertainty and expensive crossvalidation to find the optimal regularization parameter. Moreover, the model's predictive performance may degrade dramatically with a suboptimal value of the regularization parameter (which is sometimes desirable to induce sparseness). We propose a fully Bayesian approach based on a \"spike and slab\" prior (similar to L0 regularization) that does not suffer from these shortcomings. We develop an approximate MCMC method combining Langevin dynamics and reversible jump MCMC to conduct inference in this model. Experiments show that the proposed model learns a good combination of the structure and parameter values without the need for separate hyper-parameter tuning. Moreover, the model's predictive performance is much more robust than L1-based methods with hyper-parameter settings that induce highly sparse model structures.", "venue": "UAI", "authors": ["Yutian  Chen", "Max  Welling"], "year": 2012, "n_citations": 2}
{"id": 4380771, "s2_id": "0e981e76ce69ae525ced323d1c9682afa161ca9e", "title": "Warming-up recurrent neural networks to maximize reachable multi-stability greatly improves learning", "abstract": "Training recurrent neural networks is known to be difficult when time dependencies become long. Consequently, training standard gated cells such as gated recurrent units and long-short term memory on benchmarks where long-term memory is required remains an arduous task. In this work, we propose a general way to initialize any recurrent network connectivity through a process called \u201cwarmup\u201d to improve its capability to learn arbitrarily long time dependencies. This initialization process is designed to maximize network reachable multi-stability, i.e. the number of attractors within the network that can be reached through relevant input trajectories. Warming-up is performed before training, using stochastic gradient descent on a specifically designed loss. We show that warming-up greatly improves recurrent neural network performance on long-term memory benchmarks for multiple recurrent cell types, but can sometimes impede precision. We therefore introduce a parallel recurrent network structure with partial warm-up that is shown to greatly improve learning on long time-series while maintaining high levels of precision. This approach provides a general framework for improving learning abilities of any recurrent cell type when long-term memory is required.", "venue": "ArXiv", "authors": ["Nicolas  Vecoven", "Damien  Ernst", "Guillaume  Drion"], "year": 2021, "n_citations": 0}
{"id": 4381532, "s2_id": "dfbe3b69d078f9da2bc64fb99b1ecfbe4097c573", "title": "Robotic Testbed for Rendezvous and Optical Navigation: Multi-Source Calibration and Machine Learning Use Cases", "abstract": "This work presents the most recent advances of the Robotic Testbed for Rendezvous and Optical Navigation (TRON) at Stanford University, including multisource calibration and machine learning applications. TRON is among the first robotic testbeds used to train robust machine learning algorithms for spaceborne optical navigation. First, this paper addresses the data fusion approach and algorithms used to calibrate the two-robots facility to end-to-end pose accuracies of millimeters (translational) and sub-degrees (rotational). Second, it is shown how TRON can be used to create large-scale datasets consisting of thousands of images of a target spacecraft mockup model in a high-fidelity space illumination environment with maximally varying pose distribution. Finally, TRON is used to train and validate spacecraft pose estimation algorithms in representative rendezvous and proximity operation trajectories.", "venue": "ArXiv", "authors": ["Tae Ha Park", "Juergen  Bosse", "Simone  D\u2019Amico"], "year": 2021, "n_citations": 1}
{"id": 4385589, "s2_id": "db66a983e4176f9dab06b5cc1da26e9731014da7", "title": "How Bayesian should Bayesian optimisation be?", "abstract": "Bayesian optimisation (BO) uses probabilistic surrogate models - usually Gaussian processes (GPs) - for the optimisation of expensive black-box functions. At each BO iteration, the GP hyperparameters are fit to previously-evaluated data by maximising the marginal likelihood. However, this fails to account for uncertainty in the hyperparameters themselves, leading to overconfident model predictions. This uncertainty can be accounted for by taking the Bayesian approach of marginalising out the model hyperparameters. We investigate whether a fully-Bayesian treatment of the Gaussian process hyperparameters in BO (FBBO) leads to improved optimisation performance. Since an analytic approach is intractable, we compare FBBO using three approximate inference schemes to the maximum likelihood approach, using the Expected Improvement (EI) and Upper Confidence Bound (UCB) acquisition functions paired with ARD and isotropic Mat\u00e9rn kernels, across 15 well-known benchmark problems for 4 observational noise settings. FBBO using EI with an ARD kernel leads to the best performance in the noise-free setting, with much less difference between combinations of BO components when the noise is increased. FBBO leads to over-exploration with UCB, but is not detrimental with EI. Therefore, we recommend that FBBO using EI with an ARD kernel as the default choice for BO.", "venue": "GECCO Companion", "authors": ["George De Ath", "Richard  Everson", "Jonathan  Fieldsend"], "year": 2021, "n_citations": 0}
{"id": 4388106, "s2_id": "ef48e4ca6c0e97661c9363626e9edc49d21167dd", "title": "Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit", "abstract": "Subspace clustering methods based on \u21131, \u21132 or nuclear norm regularization have become very popular due to their simplicity, theoretical guarantees and empirical success. However, the choice of the regularizer can greatly impact both theory and practice. For instance, \u21131 regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad conditions (e.g., arbitrary subspaces and corrupted data). However, it requires solving a large scale convex optimization problem. On the other hand, \u21132 and nuclear norm regularization provide efficient closed form solutions, but require very strong assumptions to guarantee a subspace-preserving affinity, e.g., independent subspaces and uncorrupted data. In this paper we study a subspace clustering method based on orthogonal matching pursuit. We show that the method is both computationally efficient and guaranteed to give a subspace-preserving affinity under broad conditions. Experiments on synthetic data verify our theoretical analysis, and applications in handwritten digit and face clustering show that our approach achieves the best trade off between accuracy and efficiency. Moreover, our approach is the first one to handle 100,000 data points.", "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "authors": ["Chong  You", "Daniel P. Robinson", "Ren\u00e9  Vidal"], "year": 2016, "n_citations": 260}
{"id": 4393414, "s2_id": "d7726499e1d9a1ae883d792b82b58068d9a4de91", "title": "Learning to Rehearse in Long Sequence Memorization", "abstract": "Existing reasoning tasks often have an important assumption that the input contents can be always accessed while reasoning, requiring unlimited storage resources and suffering from severe time delay on long sequences. To achieve efficient reasoning on long sequences with limited storage resources, memory augmented neural networks introduce a human-like write-read memory to compress and memorize the long input sequence in one pass, trying to answer subsequent queries only based on the memory. But they have two serious drawbacks: 1) they continually update the memory from current information and inevitably forget the early contents; 2) they do not distinguish what information is important and treat all contents equally. In this paper, we propose the Rehearsal Memory (RM) to enhance long-sequence memorization by self-supervised rehearsal with a history sampler. To alleviate the gradual forgetting of early information, we design self-supervised rehearsal training with recollection and familiarity tasks. Further, we design a history sampler to select informative fragments for rehearsal training, making the memory focus on the crucial information. We evaluate the performance of our rehearsal memory by the synthetic bAbI task and several downstream tasks, including text/video question answering and recommendation on long sequences.", "venue": "ICML", "authors": ["Zhu  Zhang", "Chang  Zhou", "Jianxin  Ma", "Zhijie  Lin", "Jingren  Zhou", "Hongxia  Yang", "Zhou  Zhao"], "year": 2021, "n_citations": 1}
{"id": 4407302, "s2_id": "048cb4d1b7712b7499a9d7db6d24caaab5ddd9ce", "title": "Separate But Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data", "abstract": "We propose FedEnhance, an unsupervised federated learning (FL) approach for speech enhancement and separation with non-IID distributed data across multiple clients. We simulate a realworld scenario where each client only has access to a few noisy recordings from a limited and disjoint number of speakers (hence non-IID). Each client trains their model in isolation using mixture invariant training while periodically providing updates to a central server. Our experiments show that our approach achieves competitive enhancement performance compared to IID training on a single device and that we can further facilitate the convergence speed and the overall performance using transfer learning on the server-side. Moreover, we show that we can effectively combine updates from clients trained locally with supervised and unsupervised losses. We also release a new dataset LibriFSD50K and its creation recipe in order to facilitate FL research for source separation problems.", "venue": "2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)", "authors": ["Efthymios  Tzinis", "Jonah  Casebeer", "Zhepei  Wang", "Paris  Smaragdis"], "year": 2021, "n_citations": 2}
{"id": 4420786, "s2_id": "d32c060205e4c8156a6b0c97418aac0da1e41afb", "title": "Position paper: a general framework for applying machine learning techniques in operating room", "abstract": "In this position paper we describe a general framework for applying machine learning and pattern recognition techniques in healthcare. In particular, we are interested in providing an automated tool for monitoring and incrementing the level of awareness in the operating room and for identifying human errors which occur during the laparoscopy surgical operation. The framework that we present is divided in three different layers: each layer implements algorithms which have an increasing level of complexity and which perform functionality with an higher degree of abstraction. In the first layer, raw data collected from sensors in the operating room during surgical operation, they are pre-processed and aggregated. The results of this initial phase are transferred to a second layer, which implements pattern recognition techniques and extract relevant features from the data. Finally, in the last layer, expert systems are employed to take high level decisions, which represent the final output of the system.", "venue": "ArXiv", "authors": ["Filippo Maria Bianchi", "Enrico De Santis", "Hedieh  Montazeri", "Parisa  Naraei", "Alireza  Sadeghian"], "year": 2015, "n_citations": 0}
{"id": 4429173, "s2_id": "3ad95671f78d3205ef9f183f241d5884758a2799", "title": "Predicting the Popularity of Games on Steam", "abstract": "The video game industry has seen rapid growth over the last decade. Thousands of video games are released and played by millions of people every year, creating a large community of players. Steam is a leading gaming platform and social networking site, which allows its users to purchase and store games. A by-product of Steam is a large database of information about games, players, and gaming behavior. In this paper, we take recent video games released on Steam and aim to discover the relation between game popularity and a game\u2019s features that can be acquired through Steam. We approach this task by predicting the popularity of Steam games in the early stages after their release and we use a Bayesian approach to understand the influence of a game\u2019s price, size, supported languages, release date, and genres on its player count. We implement several models and discover that a genre-based hierarchical approach achieves the best performance. We further analyze the model and interpret its coefficients, which indicate that games released at the beginning of the month and games of certain genres correlate with game popularity.", "venue": "ArXiv", "authors": ["Andraz De Luisa", "Jan  Hartman", "David  Nabergoj", "Samo  Pahor", "Marko  Rus", "Bozhidar  Stevanoski", "Jure  Demsar", "Erik  Strumbelj"], "year": 2021, "n_citations": 0}
{"id": 4481524, "s2_id": "699f4c8cbff14d82a26d85bc5b61e7dc4e2aca86", "title": "Contrastive learning, multi-view redundancy, and linear models", "abstract": "Self-supervised learning is an empirically successful approach to unsupervised learning based on creating artificial supervised learning problems. A popular self-supervised approach to representation learning is contrastive learning, which leverages naturally occurring pairs of similar and dissimilar data points, or multiple views of the same data. This work provides a theoretical analysis of contrastive learning in the multi-view setting, where two views of each datum are available. The main result is that linear functions of the learned representations are nearly optimal on downstream prediction tasks whenever the two views provide redundant information about the label.", "venue": "ALT", "authors": ["Christopher  Tosh", "Akshay  Krishnamurthy", "Daniel  Hsu"], "year": 2021, "n_citations": 29}
{"id": 4509849, "s2_id": "cdd7e29b9ac909188104bfc56ac3d6a0117c01ca", "title": "Semi-supervised Relation Extraction via Incremental Meta Self-Training", "abstract": "To alleviate human efforts from obtaining large-scale annotations, Semi-Supervised Relation Extraction methods aim to leverage unlabeled data in addition to learning from limited samples. Existing self-training methods suffer from the gradual drift problem, where noisy pseudo labels on unlabeled data are incorporated during training. To alleviate the noise in pseudo labels, we propose a method called MetaSRE, where a Relation Label Generation Network generates accurate quality assessment on pseudo labels by (meta) learning from the successful and failed attempts on Relation Classification as an additional meta-objective. To reduce the influence of noisy pseudo labels, MetaSRE adopts a pseudo label selection and exploitation scheme which assesses pseudo label quality on unlabeled samples and only exploits high-quality pseudo labels in a self-training fashion to incrementally augment labeled samples for both robustness and accuracy. Experimental results on two public datasets demonstrate the effectiveness of the proposed approach.", "venue": "EMNLP", "authors": ["Xuming  Hu", "Fukun  Ma", "Chenyao  Liu", "Chenwei  Zhang", "Lijie  Wen", "Philip S. Yu"], "year": 2021, "n_citations": 4}
{"id": 4527056, "s2_id": "81d74966fd1071bc418b48745153af2d2ab0654e", "title": "CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds", "abstract": "Forecasting the formation and development of clouds is a central element of modern weather forecasting systems. Incorrect cloud forecasts can lead to major uncertainty in the overall accuracy of weather forecasts due to their intrinsic role in the Earth's climate system. Few studies have tackled this challenging problem from a machine learning point-of-view due to a shortage of high-resolution datasets with many historical observations globally. In this article, we present a novel satellite-based dataset called \u201cCloudCast.\u201d It consists of 70\u2009080 images with 10 different cloud types for multiple layers of the atmosphere annotated on a pixel level. The spatial resolution of the dataset is 928 \u00a0\u00d7\u00a01530\u00a0pixels (3 \u00a0\u00d7\u00a03\u00a0km per pixel) with 15-min intervals between frames for the period January 1, 2017 to December 31, 2018. All frames are centered and projected over Europe. To supplement the dataset, we conduct an evaluation study with current state-of-the-art video prediction methods such as convolutional long short-term memory networks, generative adversarial networks, and optical flow-based extrapolation methods. As the evaluation of video prediction is difficult in practice, we aim for a thorough evaluation in the spatial and temporal domain. Our benchmark models show promising results but with ample room for improvement. This is the first publicly available global-scale dataset with high-resolution cloud types on a high temporal granularity to the authors\u2019 best knowledge.", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "authors": ["A. H. Nielsen", "A.  Iosifidis", "H.  Karstoft"], "year": 2021, "n_citations": 2}
{"id": 4558818, "s2_id": "41a5aa95a65dfbacc0c8246a39902f92abe88dd2", "title": "Applying deep learning techniques on medical corpora from the World Wide Web: a prototypical system and evaluation", "abstract": "BACKGROUND: The amount of biomedical literature is rapidly growing and it is becoming increasingly difficult to keep manually curated knowledge bases and ontologies up-to-date. In this study we applied the word2vec deep learning toolkit to medical corpora to test its potential for identifying relationships from unstructured text. We evaluated the efficiency of word2vec in identifying properties of pharmaceuticals based on mid-sized, unstructured medical text corpora available on the web. Properties included relationships to diseases ('may treat') or physiological processes ('has physiological effect'). We compared the relationships identified by word2vec with manually curated information from the National Drug File - Reference Terminology (NDF-RT) ontology as a gold standard. RESULTS: Our results revealed a maximum accuracy of 49.28% which suggests a limited ability of word2vec to capture linguistic regularities on the collected medical corpora compared with other published results. We were able to document the influence of different parameter settings on result accuracy and found and unexpected trade-off between ranking quality and accuracy. Pre-processing corpora to reduce syntactic variability proved to be a good strategy for increasing the utility of the trained vector models. CONCLUSIONS: Word2vec is a very efficient implementation for computing vector representations and for its ability to identify relationships in textual data without any prior domain knowledge. We found that the ranking and retrieved results generated by word2vec were not of sufficient quality for automatic population of knowledge bases and ontologies, but could serve as a starting point for further manual curation.", "venue": "ArXiv", "authors": ["Jos\u00e9 Antonio Mi\u00f1arro-Gim\u00e9nez", "Oscar  Mar\u00edn-Alonso", "Matthias  Samwald"], "year": 2015, "n_citations": 23}
{"id": 4606739, "s2_id": "899e21c5b32933128019e4280105f004176d82a2", "title": "Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation", "abstract": "While recent developments in autonomous vehicle (AV) technology highlight substantial progress, we lack tools for rigorous and scalable testing. Real-world testing, the $\\textit{de facto}$ evaluation environment, places the public in danger, and, due to the rare nature of accidents, will require billions of miles in order to statistically validate performance claims. We implement a simulation framework that can test an entire modern autonomous driving system, including, in particular, systems that employ deep-learning perception and control algorithms. Using adaptive importance-sampling methods to accelerate rare-event probability evaluation, we estimate the probability of an accident under a base distribution governing standard traffic behavior. We demonstrate our framework on a highway scenario, accelerating system evaluation by $2$-$20$ times over naive Monte Carlo sampling methods and $10$-$300 \\mathsf{P}$ times (where $\\mathsf{P}$ is the number of processors) over real-world testing.", "venue": "NeurIPS", "authors": ["Matthew  O'Kelly", "Aman  Sinha", "Hongseok  Namkoong", "John  Duchi", "Russ  Tedrake"], "year": 2018, "n_citations": 87}
{"id": 4614508, "s2_id": "1c9b239bd06f6a7421874fb0f0540b47a305f846", "title": "Learning to Detect Collisions for Continuum Manipulators without a Prior Model", "abstract": "Due to their flexibility, dexterity, and compact size, Continuum Manipulators (CMs) can enhance minimally invasive interventions. In these procedures, the CM may be operated in proximity of sensitive organs; therefore, requiring accurate and appropriate feedback when colliding with their surroundings. Conventional CM collision detection algorithms rely on a combination of exact CM constrained kinematics model, geometrical assumptions such as constant curvature behavior, a priori knowledge of the environmental constraint geometry, and/or additional sensors to scan the environment or sense contacts. In this paper, we propose a data-driven machine learning approach using only the available sensory information, without requiring any prior geometrical assumptions, model of the CM or the surrounding environment. The proposed algorithm is implemented and evaluated on a non-constant curvature CM, equipped with Fiber Bragg Grating (FBG) optical sensors for shape sensing purposes. Results demonstrate successful detection of collisions in constrained environments with soft and hard obstacles with unknown stiffness and location.", "venue": "MICCAI", "authors": ["Shahriar  Sefati", "Shahin  Sefati", "Iulian  Iordachita", "Russell H. Taylor", "Mehran  Armand"], "year": 2019, "n_citations": 2}
{"id": 4622864, "s2_id": "cbc5a42df9e64d870d9565f474588e65598dfcef", "title": "Filter Pruning using Hierarchical Group Sparse Regularization for Deep Convolutional Neural Networks", "abstract": "Since the convolutional neural networks are often trained with redundant parameters, it is possible to reduce redundant kernels or filters to obtain a compact network without dropping the classification accuracy. In this paper, we propose a filter pruning method using the hierarchical group sparse regularization. It is shown in our previous work that the hierarchical group sparse regularization is effective in obtaining sparse networks in which filters connected to unnecessary channels are automatically close to zero. After training the convolutional neural network with the hierarchical group sparse regularization, the unnecessary filters are selected based on the increase of the classification loss of the randomly selected training samples to obtain a compact network. It is shown that the proposed method can reduce more than 50% parameters of ResNet for CIFAR-10 with only 0.3 % decrease in the accuracy of test samples. Also, 34% parameters of ResNet are reduced for TinyImageNet-200 with higher accuracy than the baseline network.", "venue": "2020 25th International Conference on Pattern Recognition (ICPR)", "authors": ["Kakeru  Mitsuno", "Takio  Kurita"], "year": 2021, "n_citations": 1}
{"id": 4623079, "s2_id": "0775adfdb82b38eef26440a2cd8034800d5a1a68", "title": "Systematic Mapping Study on the Machine Learning Lifecycle", "abstract": "The development of artificial intelligence (AI) has made various industries eager to explore the benefits of AI. There is an increasing amount of research surrounding AI, most of which is centred on the development of new AI algorithms and techniques. However, the advent of AI is bringing an increasing set of practical problems related to AI model lifecycle management that need to be investigated. We address this gap by conducting a systematic mapping study on the lifecycle of AI model. Through quantitative research, we provide an overview of the field, identify research opportunities, and provide suggestions for future research. Our study yields 405 publications published from 2005 to 2020, mapped in 5 different main research topics, and 31 sub-topics. We observe that only a minority of publications focus on data management and model production problems, and that more studies should address the AI lifecycle from a holistic perspective.", "venue": "2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN)", "authors": ["Yuanhao  Xie", "Lu'is  Cruz", "Petra  Heck", "Jan S. Rellermeyer"], "year": 2021, "n_citations": 0}
{"id": 4642488, "s2_id": "99f9b41a8d0560d90f514e6862bee0abc8e8761c", "title": "The Kikuchi Hierarchy and Tensor PCA", "abstract": "For the tensor PCA (principal component analysis) problem, we propose a new hierarchy of increasingly powerful algorithms with increasing runtime. Our hierarchy is analogous to the sum-of-squares (SOS) hierarchy but is instead inspired by statistical physics and related algorithms such as belief propagation and AMP (approximate message passing). Our level-t algorithm can be thought of as a linearized message-passing algorithm that keeps track of t-wise dependencies among the hidden variables. Specifically, our algorithms are spectral methods based on the Kikuchi Hessian, which generalizes the well-studied Bethe Hessian to the higher-order Kikuchi free energies. It is known that AMP, the flagship algorithm of statistical physics, has substantially worse performance than SOS for tensor PCA. In this work we 'redeem' the statistical physics approach by showing that our hierarchy gives a polynomial-time algorithm matching the performance of SOS. Our hierarchy also yields a continuum of subexponential-time algorithms, and we prove that these achieve the same (conjecturally optimal) tradeoff between runtime and statistical power as SOS. Our proofs are much simpler than prior work, and also apply to the related problem of refuting random k-XOR formulas. The results we present here apply to tensor PCA for tensors of all orders, and to k-XOR when k is even. Our methods suggest a new avenue for systematically obtaining optimal algorithms for Bayesian inference problems, and our results constitute a step toward unifying the statistical physics and sum-of-squares approaches to algorithm design.", "venue": "2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS)", "authors": ["Alexander S. Wein", "Ahmed El Alaoui", "Cristopher  Moore"], "year": 2019, "n_citations": 26}
{"id": 4646085, "s2_id": "19908640236767427ebf0524dc3a4bb09d65145e", "title": "Loopy Belief Propagation for Approximate Inference: An Empirical Study", "abstract": "Recently, researchers have demonstrated that \"loopy belief propagation\" -- the use of Pearl's polytree algorithm in a Bayesian network with loops -- can perform well in the context of error-correcting codes. The most dramatic instance of this is the near Shannon-limit performance of \"Turbo Codes\" -- codes whose decoding algorithm is equivalent to loopy belief propagation in a chain-structured Bayesian network. \n \nIn this paper we ask: is there something special about the error-correcting code context, or does loopy propagation work as an approximate inference scheme in a more general setting? We compare the marginals computed using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR. We find that the loopy beliefs often converge and when they do, they give a good approximation to the correct marginals. However, on the QMR network, the loopy beliefs oscillated and had no obvious relationship to the correct posteriors. We present some initial investigations into the cause of these oscillations, and show that some simple methods of preventing them lead to the wrong results.", "venue": "UAI", "authors": ["Kevin P. Murphy", "Yair  Weiss", "Michael I. Jordan"], "year": 1999, "n_citations": 1750}
{"id": 4650131, "s2_id": "0f6b1a3f032a111ab3b54ee0b8a4731cb04c9823", "title": "Efficient Sample Reuse in Policy Gradients with Parameter-Based Exploration", "abstract": "The policy gradient approach is a flexible and powerful reinforcement learning method particularly for problems with continuous actions such as robot control. A common challenge is how to reduce the variance of policy gradient estimates for reliable policy updates. In this letter, we combine the following three ideas and give a highly effective policy gradient method: (1) policy gradients with parameter-based exploration, a recently proposed policy search method with low variance of gradient estimates; (2) an importance sampling technique, which allows us to reuse previously gathered data in a consistent way; and (3) an optimal baseline, which minimizes the variance of gradient estimates with their unbiasedness being maintained. For the proposed method, we give a theoretical analysis of the variance of gradient estimates and show its usefulness through extensive experiments.", "venue": "Neural Computation", "authors": ["Tingting  Zhao", "Hirotaka  Hachiya", "Voot  Tangkaratt", "Jun  Morimoto", "Masashi  Sugiyama"], "year": 2013, "n_citations": 26}
{"id": 4654305, "s2_id": "0d303442695ac1e420e3682f08e008e02beef5a8", "title": "M3D-GAN: Multi-Modal Multi-Domain Translation with Universal Attention", "abstract": "Generative adversarial networks have led to significant advances in cross-modal/domain translation. However, typically these networks are designed for a specific task (e.g., dialogue generation or image synthesis, but not both). We present a unified model, M3D-GAN, that can translate across a wide range of modalities (e.g., text, image, and speech) and domains (e.g., attributes in images or emotions in speech). Our model consists of modality subnets that convert data from different modalities into unified representations, and a unified computing body where data from different modalities share the same network architecture. We introduce a universal attention module that is jointly trained with the whole network and learns to encode a large range of domain information into a highly structured latent space. We use this to control synthesis in novel ways, such as producing diverse realistic pictures from a sketch or varying the emotion of synthesized speech. We evaluate our approach on extensive benchmark tasks, including image-to-image, text-to-image, image captioning, text-to-speech, speech recognition, and machine translation. Our results show state-of-the-art performance on some of the tasks.", "venue": "ArXiv", "authors": ["Shuang  Ma", "Daniel  McDuff", "Yale  Song"], "year": 2019, "n_citations": 3}
{"id": 4658347, "s2_id": "ee0d54416972d593dda27d0cdce4f5ee5388ea0c", "title": "NeurWIN: Neural Whittle Index Network For Restless Bandits Via Deep RL", "abstract": "Whittle index policy is a powerful tool to obtain asymptotically optimal solutions for the notoriously intractable problem of restless bandits. However, finding the Whittle indices remains a difficult problem for many practical restless bandits with convoluted transition kernels. This paper proposes NeurWIN, a neural Whittle index network that seeks to learn the Whittle indices for any restless bandits by leveraging mathematical properties of the Whittle indices. We show that a neural network that produces the Whittle index is also one that produces the optimal control for a set of Markov decision problems. This property motivates using deep reinforcement learning for the training of NeurWIN. We demonstrate the utility of NeurWIN by evaluating its performance for three recently studied restless bandit problems. Our experiment results show that the performance of NeurWIN is significantly better than other RL algorithms.", "venue": "ArXiv", "authors": ["Khaled  Nakhleh", "Santosh  Ganji", "Ping-Chun  Hsieh", "I-Hong  Hou", "Srinivas  Shakkottai"], "year": 2021, "n_citations": 0}
{"id": 4660741, "s2_id": "a7644eb291ecd6196ab7636a50d3f0ede2c4f37b", "title": "Causal Inference under Networked Interference and Intervention Policy Enhancement", "abstract": "Estimating individual treatment effects from data of randomized experiments is a critical task in causal inference. The Stable Unit Treatment Value Assumption (SUTVA) is usually made in causal inference. However, interference can introduce bias when the assigned treatment on one unit affects the potential outcomes of the neighboring units. This interference phenomenon is known as spillover effect in economics or peer effect in social science. Usually, in randomized experiments, or observational studies with interconnected units, one can only observe treatment responses under interference. Hence, the issue of how to estimate the superimposed causal effect and recover the individual treatment effect in the presence of interference becomes a challenging task. In this work, we study causal effect estimation under general network interference using Graph Neural Networks, which are powerful tools for capturing node and link dependencies in graphs. After deriving causal effect estimators, we further study intervention policy improvement on the graph under capacity constraint. We give policy regret bounds under network interference and treatment capacity constraint.", "venue": "AISTATS", "authors": ["Yunpu  Ma", "Volker  Tresp"], "year": 2021, "n_citations": 1}
{"id": 4689668, "s2_id": "30dfc924a55d4e978c9b84139b53142a1cf758d7", "title": "Explaining Visual Models by Causal Attribution", "abstract": "Model explanations based on pure observational data cannot compute the effects of features reliably, due to their inability to estimate how each factor alteration could affect the rest. We argue that explanations should be based on the causal model of the data and the derived intervened causal models, that represent the data distribution subject to interventions. With these models, we can compute counterfactuals, new samples that will inform us how the model reacts to feature changes on our input. We propose a novel explanation methodology based on Causal Counterfactuals and identify the limitations of current Image Generative Models in their application to counterfactual creation.", "venue": "ICCV 2019", "authors": ["\u00c1lvaro Parafita Mart\u00ednez", "Jordi Vitri\u00e0 Marca"], "year": 2019, "n_citations": 1}
{"id": 4697166, "s2_id": "7385865d44617872a1e29d39adb850fa1ef84b9b", "title": "A Performance-Explainability Framework to Benchmark Machine Learning Methods: Application to Multivariate Time Series Classifiers", "abstract": "Our research aims to propose a new performance-explainability analytical framework to assess and benchmark machine learning methods. The framework details a set of characteristics that operationalize the performance-explainability assessment of existing machine learning methods. In order to illustrate the use of the framework, we apply it to benchmark the current state-of-the-art multivariate time series classifiers.", "venue": "ArXiv", "authors": ["Kevin  Fauvel", "V\u00e9ronique  Masson", "\u00c9lisa  Fromont"], "year": 2020, "n_citations": 6}
{"id": 4706816, "s2_id": "d81beef1648471c1964fb624d74c0eb8cb2a053a", "title": "Ultra-dense low data rate (UDLD) communication in the THz", "abstract": "In the future, with the advent of the Internet of Things (IoT), wireless sensors, and multiple 5G applications yet to be developed, an indoor room might be filled with 1000s of devices. These devices will have different Quality of Service (QoS) demands and resource constraints, such as mobility, hardware, and efficiency requirements. The THz band has a massive greenfield spectrum and is envisioned to cater to these dense-indoor deployments. However, THz has multiple caveats, such as high absorption rate, limited coverage range, low transmit power, sensitivity to mobility, and frequent outages, making it challenging to deploy. THz might compel networks to be dependent on additional infrastructure, which might not be profitable for network operators and can even result in inefficient resource utilization for devices demanding low to moderate data rates. Using distributed Device-to-Device (D2D) communication in the THz, we can cater to these ultra-dense low data rate type applications in a constrained resource situation. We propose a 2-Layered distributed D2D model, where devices use coordinated multi-agent reinforcement learning (MARL) to maximize efficiency and user coverage for dense-indoor deployment. We explore the choice of features required to train the algorithms and how it impacts the system efficiency. We show that densification and mobility in a network can be used to further the limited coverage range of THz devices, without the need for extra infrastructure or resources.", "venue": "NANOCOM", "authors": ["Rohit  Singh", "Doug  Sicker"], "year": 2020, "n_citations": 2}
{"id": 4719073, "s2_id": "01a0e9326519291c1948e89838f11d3ca63d16da", "title": "Non-IID Recommender Systems: A Review and Framework of Recommendation Paradigm Shifting", "abstract": "ABSTRACT While recommendation plays an increasingly critical role in our living, study, work, and entertainment, the recommendations we receive are often for irrelevant, duplicate, or uninteresting products and services. A critical reason for such bad recommendations lies in the intrinsic assumption that recommended users and items are independent and identically distributed (IID) in existing theories and systems. Another phenomenon is that, while tremendous efforts have been made to model specific aspects of users or items, the overall user and item characteristics and their non-IIDness have been overlooked. In this paper, the non-IID nature and characteristics of recommendation are discussed, followed by the non-IID theoretical framework in order to build a deep and comprehensive understanding of the intrinsic nature of recommendation problems, from the perspective of both couplings and heterogeneity. This non-IID recommendation research triggers the paradigm shift from IID to non-IID recommendation research and can hopefully deliver informed, relevant, personalized, and actionable recommendations. It creates exciting new directions and fundamental solutions to address various complexities including cold-start, sparse data-based, cross-domain, group-based, and shilling attack-related issues.", "venue": "ArXiv", "authors": ["Longbing  Cao"], "year": 2020, "n_citations": 34}
{"id": 4736468, "s2_id": "dddc3f4a6d2d668eb9a840ca8ff7c2d83366a507", "title": "Efficient Domain Generalization via Common-Specific Low-Rank Decomposition", "abstract": "Domain generalization refers to the task of training a model which generalizes to new domains that are not seen during training. We present CSD (Common Specific Decomposition), for this setting,which jointly learns a common component (which generalizes to new domains) and a domain specific component (which overfits on training domains). The domain specific components are discarded after training and only the common component is retained. The algorithm is extremely simple and involves only modifying the final linear classification layer of any given neural network architecture. We present a principled analysis to understand existing approaches, provide identifiability results of CSD,and study effect of low-rank on domain generalization. We show that CSD either matches or beats state of the art approaches for domain generalization based on domain erasure, domain perturbed data augmentation, and meta-learning. Further diagnostics on rotated MNIST, where domains are interpretable, confirm the hypothesis that CSD successfully disentangles common and domain specific components and hence leads to better domain generalization.", "venue": "ICML", "authors": ["Vihari  Piratla", "Praneeth  Netrapalli", "Sunita  Sarawagi"], "year": 2020, "n_citations": 51}
{"id": 4738822, "s2_id": "bcf95d7a0ee88c183b2fa2aadd2207b6a3fc21d0", "title": "Policy Optimization in Bayesian Network Hybrid Models of Biomanufacturing Processes", "abstract": "Biopharmaceutical manufacturing is a rapidly growing industry with impact in virtually all branches of medicine. Biomanufacturing processes require close monitoring and control, in the presence of complex bioprocess dynamics with many interdependent factors, as well as extremely limited data due to the high cost and long duration of experiments. We develop a novel model-based reinforcement learning framework that can achieve human-level control in low-data environments. The model uses a probabilistic knowledge graph to capture causal interdependencies between factors in the underlying stochastic decision process, leveraging information from existing kinetic models from different unit operations while incorporating real-world experimental data. We then present a computationally efficient, provably convergent stochastic gradient method for policy optimization. Validation is conducted on a realistic application with a multi-dimensional, continuous state variable.", "venue": "ArXiv", "authors": ["Hua  Zheng", "Wei  Xie", "Ilya O. Ryzhov", "Dongming  Xie"], "year": 2021, "n_citations": 1}
{"id": 4743109, "s2_id": "fe018f98cd919539ffc43fd9c882e39ef3421e5d", "title": "Safe crossover of neural networks through neuron alignment", "abstract": "One of the main and largely unexplored challenges in evolving the weights of neural networks using genetic algorithms is to find a sensible crossover operation between parent networks. Indeed, naive crossover leads to functionally damaged offspring that do not retain information from the parents. This is because neural networks are invariant to permutations of neurons, giving rise to multiple ways of representing the same solution. This is often referred to as the competing conventions problem. In this paper, we propose a two-step safe crossover (SC) operator. First, the neurons of the parents are functionally aligned by computing how well they correlate, and only then are the parents recombined. We compare two ways of measuring relationships between neurons: Pairwise Correlation (PwC) and Canonical Correlation Analysis (CCA). We test our safe crossover operators (SC-PwC and SC-CCA) on MNIST and CIFAR-10 by performing arithmetic crossover on the weights of feed-forward neural network pairs. We show that it effectively transmits information from parents to offspring and significantly improves upon naive crossover. Our method is computationally fast, can serve as a way to explore the fitness landscape more efficiently and makes safe crossover a potentially promising operator in future neuroevolution research and applications.", "venue": "GECCO", "authors": ["Thomas  Uriot", "Dario  Izzo"], "year": 2020, "n_citations": 1}
{"id": 4748333, "s2_id": "746fe962e309960aa5d1cd8864cf37f5aacacbcf", "title": "Correlation based Multi-phasal models for improved imagined speech EEG recognition", "abstract": "Translation of imagined speech electroencephalogram(EEG) into human understandable commands greatly facilitates the design of naturalistic brain computer interfaces. To achieve improved imagined speech unit classification, this work aims to profit from the parallel information contained in multi-phasal EEG data recorded while speaking, imagining and performing articulatory movements corresponding to specific speech units. A bi-phase common representation learning module using neural networks is designed to model the correlation and reproducibility between an analysis phase and a support phase. The trained Correlation Network is then employed to extract discriminative features of the analysis phase. These features are further classified into five binary phonological categories using machine learning models such as Gaussian mixture based hidden Markov model and deep neural networks. The proposed approach further handles the non-availability of multi-phasal data during decoding. Topographic visualizations along with result-based inferences suggest that the multi-phasal correlation modelling approach proposed in the paper enhances imagined-speech EEG recognition performance.", "venue": "ArXiv", "authors": ["Rini A Sharon", "Hema A Murthy"], "year": 2020, "n_citations": 0}
{"id": 4826693, "s2_id": "059f66d928a4349b2e5f0834c986db36b41d3d77", "title": "Prediction Focused Topic Models via Feature Selection", "abstract": "Supervised topic models are often sought to balance prediction quality and interpretability. However, when models are (inevitably) misspecified, standard approaches rarely deliver on both. We introduce a novel approach, the prediction-focused topic model, that uses the supervisory signal to retain only vocabulary terms that improve, or at least do not hinder, prediction performance. By removing terms with irrelevant signal, the topic model is able to learn task-relevant, coherent topics. We demonstrate on several data sets that compared to existing approaches, prediction-focused topic models learn much more coherent topics while maintaining competitive predictions.", "venue": "AISTATS", "authors": ["Jason  Ren", "Russell  Kunes", "Finale  Doshi-Velez"], "year": 2020, "n_citations": 4}
{"id": 4827470, "s2_id": "96e59aecafa4ea002d93924b3f1e7f21a333cd5d", "title": "Downbeat Tracking with Tempo-Invariant Convolutional Neural Networks", "abstract": "The human ability to track musical downbeats is robust to changes in tempo, and it extends to tempi never previously encountered. We propose a deterministic time-warping operation that enables this skill in a convolutional neural network (CNN) by allowing the network to learn rhythmic patterns independently of tempo. Unlike conventional deep learning approaches, which learn rhythmic patterns at the tempi present in the training dataset, the patterns learned in our model are tempo-invariant, leading to better tempo generalisation and more efficient usage of the network capacity. We test the generalisation property on a synthetic dataset created by rendering the Groove MIDI Dataset using FluidSynth, split into a training set containing the original performances and a test set containing temposcaled versions rendered with different SoundFonts (testtime augmentation). The proposed model generalises nearly perfectly to unseen tempi (F-measure of 0.89 on both training and test sets), whereas a comparable conventional CNN achieves similar accuracy only for the training set (0.89) and drops to 0.54 on the test set. The generalisation advantage of the proposed model extends to real music, as shown by results on the GTZAN and Ballroom datasets.", "venue": "ISMIR", "authors": ["Bruno Di Giorgi", "Matthias  Mauch", "Mark  Levy"], "year": 2020, "n_citations": 6}
{"id": 4831345, "s2_id": "54837070553ef1ffeb8811df2c51c95588dd7c18", "title": "A Practical Algorithm for Multiplayer Bandits when Arm Means Vary Among Players", "abstract": "We study a multiplayer stochastic multi-armed bandit problem in which players cannot communicate, and if two or more players pull the same arm, a collision occurs and the involved players receive zero reward. We consider the challenging heterogeneous setting, in which different arms may have different means for different players, and propose a new and efficient algorithm that combines the idea of leveraging forced collisions for implicit communication and that of performing matching eliminations. We present a finite-time analysis of our algorithm, giving the first sublinear minimax regret bound for this problem, and prove that if the optimal assignment of players to arms is unique, our algorithm attains the optimal $O(\\ln(T))$ regret, solving an open question raised at NeurIPS 2018.", "venue": "AISTATS", "authors": ["Etienne  Boursier", "Emilie  Kaufmann", "Abbas  Mehrabian", "Vianney  Perchet"], "year": 2020, "n_citations": 34}
{"id": 4835428, "s2_id": "ac9299369e92249be3082286299202efd395cfaf", "title": "A Novel Paper Recommendation Method Empowered by Knowledge Graph: for Research Beginners", "abstract": "Searching for papers from different academic databases is the most commonly used method by research beginners to obtain cross-domain technical solutions. However, it is usually inefficient and sometimes even useless because traditional search methods neither consider knowledge heterogeneity in different domains nor build the bottom layer of search, including but not limited to the characteristic description text of target solutions and solutions to be excluded. To alleviate this problem, a novel paper recommendation method is proposed herein by introducing \u201cmaster\u2013slave\u201d domain knowledge graphs, which not only help users express their requirements more accurately but also helps the recommendation system better express knowledge. Specifically, it is not restricted by the cold start problem and is a challengeoriented method. To identify the rationality and usefulness of the proposed method, we selected two cross-domains and three different academic databases for verification. The experimental results demonstrate the feasibility of obtaining new technical papers in the cross-domain scenario by research beginners using the proposed method. Further, a new research paradigm for research beginners in the early stages is proposed herein. Keywords\u2014paper recommendation, knowledge graph, requirements traceability, information retrieval, machine learning", "venue": "ArXiv", "authors": ["Bangchao  Wang", "Ziyang  Weng", "Yanping  Wang"], "year": 2021, "n_citations": 0}
{"id": 4838834, "s2_id": "8c8a622d3f1837332d389a21e547a8515cfda36b", "title": "A Block Coordinate Descent Proximal Method for Simultaneous Filtering and Parameter Estimation", "abstract": "We propose and analyze a block coordinate descent proximal algorithm (BCD-prox) for simultaneous filtering and parameter estimation of ODE models. As we show on ODE systems with up to d=40 dimensions, as compared to state-of-the-art methods, BCD-prox exhibits increased robustness (to noise, parameter initialization, and hyperparameters), decreased training times, and improved accuracy of both filtered states and estimated parameters. We show how BCD-prox can be used with multistep numerical discretizations, and we establish convergence of BCD-prox under hypotheses that include real systems of interest.", "venue": "ICML", "authors": ["Ramin  Raziperchikolaei", "Harish S. Bhat"], "year": 2019, "n_citations": 5}
{"id": 4842591, "s2_id": "c754dbfb069d4e2e190d847c5c6fb991b6231948", "title": "Training a Large Scale Classifier with the Quantum Adiabatic Algorithm", "abstract": "In a previous publication we proposed discrete global optimization as a method to train a strong binary classifier constructed as a thresholded sum over weak classifiers. Our motivation was to cast the training of a classifier into a format amenable to solution by the quantum adiabatic algorithm. Applying adiabatic quantum computing (AQC) promises to yield solutions that are superior to those which can be achieved with classical heuristic solvers. Interestingly we found that by using heuristic solvers to obtain approximate solutions we could already gain an advantage over the standard method AdaBoost. In this communication we generalize the baseline method to large scale classifier training. By large scale we mean that either the cardinality of the dictionary of candidate weak classifiers or the number of weak learners used in the strong classifier exceed the number of variables that can be handled effectively in a single global optimization. For such situations we propose an iterative and piecewise approach in which a subset of weak classifiers is selected in each iteration via global optimization. The strong classifier is then constructed by concatenating the subsets of weak classifiers. We show in numerical studies that the generalized method again successfully competes with AdaBoost. We also provide theoretical arguments as to why the proposed optimization method, which does not only minimize the empirical loss but also adds L0-norm regularization, is superior to versions of boosting that only minimize the empirical loss. By conducting a Quantum Monte Carlo simulation we gather evidence that the quantum adiabatic algorithm is able to handle a generic training problem efficiently.", "venue": "ArXiv", "authors": ["Hartmut  Neven", "Vasil S. Denchev", "Geordie  Rose", "William G. Macready"], "year": 2009, "n_citations": 60}
{"id": 4854360, "s2_id": "1082eaebdd69118f169c5c31da481f7ebc0c69c8", "title": "Weighted Quantum Channel Compiling through Proximal Policy Optimization", "abstract": "We propose a general and systematic strategy to compile arbitrary quantum channels without using ancillary qubits, based on proximal policy optimization\u2014a powerful deep reinforcement learning algorithm. We rigorously prove that, in sharp contrast to the case of compiling unitary gates, it is impossible to compile an arbitrary channel to arbitrary precision with any given finite elementary channel set, regardless of the length of the decomposition sequence. However, for a fixed accuracy one can construct a universal set with constant number of -dependent elementary channels, such that an arbitrary quantum channel can be decomposed into a sequence of these elementary channels followed by a unitary gate, with the sequence length bounded by O( 1 log 1 ). Through a concrete example concerning topological compiling of Majorana fermions, we show that our proposed algorithm can conveniently and effectively reduce the use of expensive elementary gates through adding the weighted cost into the reward function of the proximal policy optimization.", "venue": "ArXiv", "authors": ["Weiyuan  Gong", "Si  Jiang", "Dong-Ling  Deng"], "year": 2021, "n_citations": 0}
{"id": 4883110, "s2_id": "39f676d1d548de20371a3819029d0bcde36f15fd", "title": "Locally Differentially Private Bayesian Inference", "abstract": "In recent years, local differential privacy (LDP) has emerged as a technique of choice for privacy-preserving data collection in several scenarios when the aggregator is not trustworthy. LDP provides client-side privacy by adding noise at the user\u2019s end. Thus, clients need not rely on the trustworthiness of the aggregator. In this work, we provide a noise-aware probabilistic modeling framework, which allows Bayesian inference to take into account the noise added for privacy under LDP, conditioned on locally perturbed observations. Stronger privacy protection (compared to the central model) provided by LDP protocols comes at a much harsher privacy-utility trade-off. Our framework tackles several computational and statistical challenges posed by LDP for accurate uncertainty quantification under Bayesian settings. We demonstrate the efficacy of our framework in parameter estimation for univariate and multi-variate distributions as well as logistic and linear regression.", "venue": "ArXiv", "authors": ["Tejas  Kulkarni", "Joonas  J\u00e4lk\u00f6", "Samuel  Kaski", "Antti  Honkela"], "year": 2021, "n_citations": 0}
{"id": 4886880, "s2_id": "f3db49ba4a3bccc8f073c616e0223342f238d73b", "title": "Detecting Target-Area Link-Flooding DDoS Attacks using Traffic Analysis and Supervised Learning", "abstract": "A novel class of extreme link-flooding DDoS (Distributed Denial of Service) attacks is designed to cut off entire geographical areas such as cities and even countries from the Internet by simultaneously targeting a selected set of network links. The Crossfire attack is a target-area link-flooding attack, which is orchestrated in three complex phases. The attack uses a massively distributed large-scale botnet to generate low-rate benign traffic aiming to congest selected network links, so-called target links. The adoption of benign traffic, while simultaneously targeting multiple network links, makes detecting the Crossfire attack a serious challenge. In this paper, we present analytical and emulated results showing hitherto unidentified vulnerabilities in the execution of the attack, such as a correlation between coordination of the botnet traffic and the quality of the attack, and a correlation between the attack distribution and detectability of the attack. Additionally, we identified a warm-up period due to the bot synchronization. For attack detection, we report results of using two supervised machine learning approaches: Support Vector Machine (SVM) and Random Forest (RF) for classification of network traffic to normal and abnormal traffic, i.e, attack traffic. These machine learning models have been trained in various scenarios using the link volume as the main feature set.", "venue": "Advances in Intelligent Systems and Computing", "authors": ["Mostafa  Rezazad", "Matthias R. Brust", "Mohammad  Akbari", "Pascal  Bouvry", "Ngai-Man  Cheung"], "year": 2018, "n_citations": 6}
{"id": 4889747, "s2_id": "ed413ec4838c11382ab0d001005b86880b0ded98", "title": "Learning transformed product distributions", "abstract": "We consider the problem of learning an unknown product distribution $X$ over $\\{0,1\\}^n$ using samples $f(X)$ where $f$ is a \\emph{known} transformation function. Each choice of a transformation function $f$ specifies a learning problem in this framework. \nInformation-theoretic arguments show that for every transformation function $f$ the corresponding learning problem can be solved to accuracy $\\eps$, using $\\tilde{O}(n/\\eps^2)$ examples, by a generic algorithm whose running time may be exponential in $n.$ We show that this learning problem can be computationally intractable even for constant $\\eps$ and rather simple transformation functions. Moreover, the above sample complexity bound is nearly optimal for the general problem, as we give a simple explicit linear transformation function $f(x)=w \\cdot x$ with integer weights $w_i \\leq n$ and prove that the corresponding learning problem requires $\\Omega(n)$ samples. \nAs our main positive result we give a highly efficient algorithm for learning a sum of independent unknown Bernoulli random variables, corresponding to the transformation function $f(x)= \\sum_{i=1}^n x_i$. Our algorithm learns to $\\eps$-accuracy in poly$(n)$ time, using a surprising poly$(1/\\eps)$ number of samples that is independent of $n.$ We also give an efficient algorithm that uses $\\log n \\cdot \\poly(1/\\eps)$ samples but has running time that is only $\\poly(\\log n, 1/\\eps).$", "venue": "ArXiv", "authors": ["Constantinos  Daskalakis", "Ilias  Diakonikolas", "Rocco A. Servedio"], "year": 2011, "n_citations": 1}
{"id": 4929751, "s2_id": "658018e556484e3d9c6bcc00c726bf5eb503ef86", "title": "Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences", "abstract": "Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference. Bayesian REX can learn to play Atari games from demonstrations, without access to the game score and can generate 100,000 samples from the posterior over reward functions in only 5 minutes on a personal laptop. Bayesian REX also results in imitation learning performance that is competitive with or better than state-of-the-art methods that only learn point estimates of the reward function. Finally, Bayesian REX enables efficient high-confidence policy evaluation without having access to samples of the reward function. These high-confidence performance bounds can be used to rank the performance and risk of a variety of evaluation policies and provide a way to detect reward hacking behaviors.", "venue": "ICML", "authors": ["Daniel S. Brown", "Russell  Coleman", "Ravi  Srinivasan", "Scott  Niekum"], "year": 2020, "n_citations": 25}
{"id": 4957594, "s2_id": "8ede8a21329c34c15d80a79b2bfe6d26cd018bb0", "title": "Specialized Decision Surface and Disentangled Feature for Weakly-Supervised Polyphonic Sound Event Detection", "abstract": "In this article, a special decision surface for the weakly-supervised sound event detection (SED) and a disentangled feature (DF) for the multi-label problem in polyphonic SED are proposed. We approach SED as a multiple instance learning (MIL) problem and utilize a neural network framework with a pooling module to solve it. General MIL approaches include two kinds: the instance-level approaches and embedding-level approaches. We present a method of generating instance-level probabilities for the embedding level approaches which tend to perform better than the instance-level approaches in terms of bag-level classification but can not provide instance-level probabilities in current approaches. Moreover, we further propose a specialized decision surface (SDS) for the embedding-level attention pooling. We analyze and explained why an embedding-level attention module with SDS is better than other typical pooling modules from the perspective of the high-level feature space. As for the problem of the unbalanced dataset and the co-occurrence of multiple categories in the polyphonic event detection task, we propose a DF to reduce interference among categories, which optimizes the high-level feature space by disentangling it based on class-wise identifiable information and obtaining multiple different subspaces. Experiments on the dataset of DCASE 2018 Task 4 show that the proposed SDS and DF significantly improve the detection performance of the embedding-level MIL approach with an attention pooling module and outperform the first place system in the challenge by $\\mathbf {6.6}$ percentage points.", "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing", "authors": ["Liwei  Lin", "Xiangdong  Wang", "Hong  Liu", "Yueliang  Qian"], "year": 2020, "n_citations": 18}
{"id": 5027446, "s2_id": "3c095e9bcdd5fa1859d7f630d2bc48457796230f", "title": "Signal classification using weighted orthogonal regression method", "abstract": "In this paper, a new classifier based on the intrinsic properties of the data is proposed. Classification is an essential task in data mining-based applications. The classification problem will be challenging when the size of the training set is not sufficient to compare to the dimension of the problem. This paper proposes a new classification method that exploits the intrinsic structure of each class through the corresponding Eigen components. Each component contributes to the learned span of each class by specific weight. The weight is determined by the associated eigenvalue. This approach results in reliable learning robust in the case of facing a classification problem with limited training data. The proposed method involves the obtained Eigenvectors by SVD of data from each class to select the bases for each subspace. Moreover, it considers an efficient weighting for the decision-making criterion to discriminate two classes. In addition to high performance on artificial data, this method has increased the best result of international competition.", "venue": "ArXiv", "authors": ["Sahar  Tavakoli"], "year": 2020, "n_citations": 0}
{"id": 5046871, "s2_id": "79a0d243241270f74b07373e51ca47cbc1a7b7af", "title": "A Simple Fusion of Deep and Shallow Learning for Acoustic Scene Classification", "abstract": "Comunicacio presentada a: 15th Sound and Music Computing Conference (SMC2018). Sonic crossing, celebrat a Limassol, Xipre, del 4 al 7 de juliol de 2018.", "venue": "ArXiv", "authors": ["Eduardo  Fonseca", "Rong  Gong", "Xavier  Serra"], "year": 2018, "n_citations": 10}
{"id": 5050009, "s2_id": "67861c3963a45c7ddde20ec3e0acaca360ec4f13", "title": "Comparing linear structure-based and data-driven latent spatial representations for sequence prediction", "abstract": "Predicting the future of Graph-supported Time Series (GTS) is a key challenge in many domains, such as climate monitoring, finance or neuroimaging. Yet it is a highly difficult problem as it requires to account jointly for time and graph (spatial) dependencies. To simplify this process, it is common to use a two-step procedure in which spatial and time dependencies are dealt with separately. In this paper, we are interested in comparing various linear spatial representations, namely structure-based ones and data-driven ones, in terms of how they help predict the future of GTS. To that end, we perform experiments with various datasets including spontaneous brain activity and raw videos.", "venue": "Optical Engineering + Applications", "authors": ["Myriam  Bontonou", "Carlos  Lassance", "Vincent  Gripon", "Nicolas  Farrugia"], "year": 2019, "n_citations": 0}
{"id": 5051206, "s2_id": "14f8d5eb2b16e6649afba9a525336586f7ae4bcb", "title": "From which world is your graph", "abstract": "Discovering statistical structure from links is a fundamental problem in the analysis of social networks. Choosing a misspecified model, or equivalently, an incorrect inference algorithm will result in an invalid analysis or even falsely uncover patterns that are in fact artifacts of the model. This work focuses on unifying two of the most widely used link-formation models: the stochastic blockmodel (SBM) and the small world (or latent space) model (SWM). Integrating techniques from kernel learning, spectral graph theory, and nonlinear dimensionality reduction, we develop the first statistically sound polynomial-time algorithm to discover latent patterns in sparse graphs for both models. When the network comes from an SBM, the algorithm outputs a block structure. When it is from an SWM, the algorithm outputs estimates of each node's latent position.", "venue": "NIPS", "authors": ["Cheng  Li", "Felix Ming Fai Wong", "Zhenming  Liu", "Varun  Kanade"], "year": 2017, "n_citations": 5}
{"id": 5105013, "s2_id": "ad79934c2a495a36830bfae1afcb39eb61d804c4", "title": "Predicting colorectal polyp recurrence using time-to-event analysis of medical records", "abstract": "Identifying patient characteristics that influence the rate of colorectal polyp recurrence can provide important insights into which patients are at higher risk for recurrence. We used natural language processing to extract polyp morphological characteristics from 953 polyp-presenting patients' electronic medical records. We used subsequent colonoscopy reports to examine how the time to polyp recurrence (731 patients experienced recurrence) is influenced by these characteristics as well as anthropometric features using Kaplan-Meier curves, Cox proportional hazards modeling, and random survival forest models. We found that the rate of recurrence differed significantly by polyp size, number, and location and patient smoking status. Additionally, right-sided colon polyps increased recurrence risk by 30% compared to left-sided polyps. History of tobacco use increased polyp recurrence risk by 20% compared to never-users. A random survival forest model showed an AUC of 0.65 and identified several other predictive variables, which can inform development of personalized polyp surveillance plans.", "venue": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science", "authors": ["Lia X. Harrington", "Jason W. Wei", "Arief A. Suriawinata", "Todd A. Mackenzie", "Saeed  Hassanpour"], "year": 2020, "n_citations": 2}
{"id": 5109247, "s2_id": "0f0da488df446c5fd9ea2f438a37535ee86f709a", "title": "Fingerprinting-Based Positioning in Distributed Massive MIMO Systems", "abstract": "Location awareness in wireless networks may enable many applications such as emergency services, autonomous driving and geographic routing. Although there are many available positioning techniques, none of them is adapted to work with massive multiple-in-multiple-out (MIMO) systems, which represent a leading 5G technology candidate. In this paper, we discuss possible solutions for positioning of mobile stations using a vector of signals at the base station, equipped with many antennas distributed over deployment area. Our main proposal is to use fingerprinting techniques based on a vector of received signal strengths. This kind of methods are able to work in highly-cluttered multipath environments, and require just one base station, in contrast to standard range-based and angle-based techniques. We also provide a solution for fingerprinting-based positioning based on Gaussian process regression, and discuss main applications and challenges.", "venue": "2015 IEEE 82nd Vehicular Technology Conference (VTC2015-Fall)", "authors": ["Vladimir  Savic", "Erik G. Larsson"], "year": 2015, "n_citations": 98}
{"id": 5110402, "s2_id": "b20e117d5d05cba6dd070fe50afbb8a71d9a4618", "title": "Deep learning for neuroimaging: a validation study", "abstract": "Deep learning methods have recently made notable advances in the tasks of classification and representation learning. These tasks are important for brain imaging and neuroscience discovery, making the methods attractive for porting to a neuroimager's toolbox. Success of these methods is, in part, explained by the flexibility of deep learning models. However, this flexibility makes the process of porting to new areas a difficult parameter optimization problem. In this work we demonstrate our results (and feasible parameter ranges) in application of deep learning methods to structural and functional brain imaging data. These methods include deep belief networks and their building block the restricted Boltzmann machine. We also describe a novel constraint-based approach to visualizing high dimensional data. We use it to analyze the effect of parameter choices on data transformations. Our results show that deep learning methods are able to learn physiologically important representations and detect latent relations in neuroimaging data.", "venue": "Front. Neurosci.", "authors": ["Sergey M. Plis", "R. Devon Hjelm", "Ruslan  Salakhutdinov", "Vince D. Calhoun"], "year": 2014, "n_citations": 450}
{"id": 5124587, "s2_id": "e15775d7016a21545b0054257a28570481a1b6f8", "title": "Towards Spatial Variability Aware Deep Neural Networks (SVANN): A Summary of Results", "abstract": "Spatial variability has been observed in many geo-phenomena including climatic zones, USDA plant hardiness zones, and terrestrial habitat types (e.g., forest, grasslands, wetlands, and deserts). However, current deep learning methods follow a spatial-one-size-fits-all(OSFA) approach to train single deep neural network models that do not account for spatial variability. In this work, we propose and investigate a spatial-variability aware deep neural network(SVANN) approach, where distinct deep neural network models are built for each geographic area. We evaluate this approach using aerial imagery from two geographic areas for the task of mapping urban gardens. The experimental results show that SVANN provides better performance than OSFA in terms of precision, recall,and F1-score to identify urban gardens.", "venue": "ArXiv", "authors": ["Jayant  Gupta", "Yiqun  Xie", "Shashi  Shekhar"], "year": 2020, "n_citations": 5}
{"id": 5124987, "s2_id": "ac91af76279b706959a6ba6c40ed6d1ca4ef14fa", "title": "Wavelet Flow: Fast Training of High Resolution Normalizing Flows", "abstract": "Normalizing flows are a class of probabilistic generative models which allow for both fast density computation and efficient sampling and are effective at modelling complex distributions like images. A drawback among current methods is their significant training cost, sometimes requiring months of GPU training time to achieve state-of-the-art results. This paper introduces Wavelet Flow, a multi-scale, normalizing flow architecture based on wavelets. A Wavelet Flow has an explicit representation of signal scale that inherently includes models of lower resolution signals and conditional generation of higher resolution signals, i.e., super resolution. A major advantage of Wavelet Flow is the ability to construct generative models for high resolution data (e.g., 1024 x 1024 images) that are impractical with previous models. Furthermore, Wavelet Flow is competitive with previous normalizing flows in terms of bits per dimension on standard (low resolution) benchmarks while being up to 15x faster to train.", "venue": "NeurIPS", "authors": ["Jason J. Yu", "Konstantinos G. Derpanis", "Marcus A. Brubaker"], "year": 2020, "n_citations": 9}
{"id": 5154978, "s2_id": "0bd487fc82a911e75242212b957e23ac52484e31", "title": "Blended Convolution and Synthesis for Efficient Discrimination of 3D Shapes", "abstract": "Existing models for shape analysis directly learn feature representations on 3D point clouds. We argue that 3D point clouds are highly redundant and hold irregular (permutation-invariant) structure, which makes it difficult to achieve inter-class discrimination efficiently. In this paper, we propose a two-pronged solution to this problem that is seamlessly integrated in a single blended convolution and synthesis layer. This fully differentiable layer performs two critical tasks in succession. In the first step, it projects the input 3D point clouds into a latent 3D space to synthesize a highly compact and inter-class discriminative point cloud representation. Since, 3D point clouds do not follow a Euclidean topology, standard 2/3D convolutional neural networks offer limited representation capability. Therefore, in the second step, we propose a novel 3D convolution operator functioning inside the unit ball to extract useful volumetric features. We derive formulae to achieve both translation and rotation of our novel convolution kernels. Finally, using the proposed techniques we present an extremely light-weight, end-to-end architecture that achieves compelling results on 3D shape recognition and retrieval.", "venue": "2020 IEEE Winter Conference on Applications of Computer Vision (WACV)", "authors": ["Sameera  Ramasinghe", "Salman  Khan", "Nick  Barnes", "Stephen  Gould"], "year": 2020, "n_citations": 4}
{"id": 5161004, "s2_id": "40cc7cdffa0a861cb557410518246d97d1678642", "title": "Benchmarking Reinforcement Learning Algorithms on Real-World Robots", "abstract": "Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning.", "venue": "CoRL", "authors": ["A. Rupam Mahmood", "Dmytro  Korenkevych", "Gautham  Vasan", "William  Ma", "James  Bergstra"], "year": 2018, "n_citations": 69}
{"id": 5163398, "s2_id": "54b679d02d33b6d23df5fc11454f5d7376980a51", "title": "MASON: A Model AgnoStic ObjectNess Framework", "abstract": "This paper proposes a simple, yet very effective method to localize dominant foreground objects in an image, to pixel-level precision. The proposed method \u2018MASON\u2019 (Model-AgnoStic ObjectNess) uses a deep convolutional network to generate category-independent and model-agnostic heat maps for any image. The network is not explicitly trained for the task, and hence, can be used off-the-shelf in tandem with any other network or task. We show that this framework scales to a wide variety of images, and illustrate the effectiveness of MASON in three varied application contexts.", "venue": "ECCV Workshops", "authors": ["K J Joseph", "Vineeth N Balasubramanian"], "year": 2018, "n_citations": 1}
{"id": 5174967, "s2_id": "b3d127c5b8cb84204a73ec2fd9e27b7c29382c08", "title": "DDP-GCN: Multi-graph convolutional network for spatiotemporal traffic forecasting", "abstract": "Traffic speed forecasting is one of the core problems in Intelligent Transportation Systems. For a more accurate prediction, recent studies started using not only the temporal speed patterns but also the spatial information on the road network through the graph convolutional networks. Even though the road network is highly complex due to its non-Euclidean and directional characteristics, previous approaches mainly focus on modeling the spatial dependencies only with the distance. In this paper, we identify two essential spatial dependencies in traffic forecasting in addition to distance, direction and positional relationship, for designing basic graph elements as the smallest building blocks. Using the building blocks, we suggest DDP-GCN (Distance, Direction, and Positional relationship Graph Convolutional Network) to incorporate the three spatial relationships into prediction network for traffic forecasting. We evaluate the proposed model with two large-scale real-world datasets, and find 7.40% average improvement for 1-hour forecasting in highly complex urban networks.", "venue": "Transportation Research Part C: Emerging Technologies", "authors": ["Kyungeun  Lee", "Wonjong  Rhee"], "year": 2022, "n_citations": 10}
{"id": 5190297, "s2_id": "225f67eddecf2e0c3aceeebf2b9aa8449d2e1f6a", "title": "Explore and Exploit with Heterotic Line Bundle Models", "abstract": "We use deep reinforcement learning to explore a class of heterotic $SU(5)$ GUT models constructed from line bundle sums over Complete Intersection Calabi Yau (CICY) manifolds. We perform several experiments where A3C agents are trained to search for such models. These agents significantly outperform random exploration, in the most favourable settings by a factor of 1700 when it comes to finding unique models. Furthermore, we find evidence that the trained agents also outperform random walkers on new manifolds. We conclude that the agents detect hidden structures in the compactification data, which is partly of general nature. The experiments scale well with $h^{(1,1)}$, and may thus provide the key to model building on CICYs with large $h^{(1,1)}$.", "venue": "ArXiv", "authors": ["Magdalena  Larfors", "Robin  Schneider"], "year": 2020, "n_citations": 22}
{"id": 5196940, "s2_id": "e17629d79efdc3cb98873373a8db8cfd2e9aff56", "title": "Efficiently Estimating Erdos-Renyi Graphs with Node Differential Privacy", "abstract": "We give a simple, computationally efficient, and node-differentially-private algorithm for estimating the parameter of an Erdos-Renyi graph---that is, estimating p in a G(n,p)---with near-optimal accuracy. Our algorithm nearly matches the information-theoretically optimal exponential-time algorithm for the same problem due to Borgs et al. (FOCS 2018). More generally, we give an optimal, computationally efficient, private algorithm for estimating the edge-density of any graph whose degree distribution is concentrated on a small interval.", "venue": "NeurIPS", "authors": ["Adam  Sealfon", "Jonathan  Ullman"], "year": 2019, "n_citations": 14}
{"id": 5225246, "s2_id": "2776f846a9e84d1ef446b6d464865348eb663e42", "title": "MEGAN: Memory Enhanced Graph Attention Network for Space-Time Video Super-Resolution", "abstract": "Space-time video super-resolution (STVSR) aims to construct a high space-time resolution video sequence from the corresponding low-frame-rate, low-resolution video sequence. Inspired by the recent success to consider spatialtemporal information for space-time super-resolution, our main goal in this work is to take full considerations of spatial and temporal correlations within the video sequences of fast dynamic events. To this end, we propose a novel onestage memory enhanced graph attention network (MEGAN) for space-time video super-resolution. Specifically, we build a novel long-range memory graph aggregation (LMGA) module to dynamically capture correlations along the channel dimensions of the feature maps and adaptively aggregate channel features to enhance the feature representations. We introduce a non-local residual block, which enables each channel-wise feature to attend global spatial hierarchical features. In addition, we adopt a progressive fusion module to further enhance the representation ability by extensively exploiting spatial-temporal correlations from multiple frames. Experiment results demonstrate that our method achieves better results compared with the stateof-the-art methods quantitatively and visually.", "venue": "ArXiv", "authors": ["Chenyu  You", "Lianyi  Han", "Aosong  Feng", "Ruihan  Zhao", "Hui  Tang", "Wei  Fan"], "year": 2021, "n_citations": 0}
{"id": 5257113, "s2_id": "c79e2c3e3943d798bd908009fbedc46f3f5567aa", "title": "Using Deep Learning Sequence Models to Identify SARS-CoV-2 Divergence", "abstract": "SARS-CoV-2 is an upper respiratory system RNA virus that has caused over 3 million deaths and infecting over 150 million worldwide as of May 2021. With thousands of strains sequenced to date, SARS-CoV-2 mutations pose significant challenges to scientists on keeping pace with vaccine development and public health measures. Therefore, an efficient method of identifying the divergence of lab samples from patients would greatly aid the documentation of SARS-CoV-2 genomics. In this study, we propose a neural network model that leverages recurrent and convolutional units to directly take in amino acid sequences of spike proteins and classify corresponding clades. We also compared our model\u2019s performance with Bidirectional Encoder Representations from Transformers (BERT) pre-trained on protein database. Our approach has the potential of providing a more computationally efficient alternative to current homology based intra-species differentiation.", "venue": "ArXiv", "authors": ["Yanyi  Ding", "Zhiyi  Kuang", "Yuxin  Pei", "Jeff  Tan", "Ziyu  Zhang", "Joseph  Konan"], "year": 2021, "n_citations": 0}
{"id": 5268338, "s2_id": "660d3472d9c3733dedcf911187b234f2b65561b5", "title": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning", "abstract": "Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsupervised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a single BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or `projected attention layers', we match the performance of separately fine-tuned models on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.", "venue": "ICML", "authors": ["Asa Cooper Stickland", "Iain  Murray"], "year": 2019, "n_citations": 98}
{"id": 5274280, "s2_id": "4bcfd46dd0703f2ed11ff12c5b5533d28c7db34a", "title": "Privacy Guidelines for Contact Tracing Applications", "abstract": "Contact tracing is a very powerful method to implement and enforce social distancing to avoid spreading of infectious diseases. The traditional approach of contact tracing is time consuming, manpower intensive, dangerous and prone to error due to fatigue or lack of skill. Due to this there is an emergence of mobile based applications for contact tracing. These applications primarily utilize a combination of GPS based absolute location and Bluetooth based relative location remitted from user's smartphone to infer various insights. These applications have eased the task of contact tracing; however, they also have severe implication on user's privacy, for example, mass surveillance, personal information leakage and additionally revealing the behavioral patterns of the user. This impact on user's privacy leads to trust deficit in these applications, and hence defeats their purpose. \nIn this work we discuss the various scenarios which a contact tracing application should be able to handle. We highlight the privacy handling of some of the prominent contact tracing applications. Additionally, we describe the various threat actors who can disrupt its working, or misuse end user's data, or hamper its mass adoption. Finally, we present privacy guidelines for contact tracing applications from different stakeholder's perspective. To best of our knowledge, this is the first generic work which provides privacy guidelines for contact tracing applications.", "venue": "ArXiv", "authors": ["Manish  Shukla", "A  RajanM", "Sachin  Lodha", "Gautam  Shroff", "Ramesh  Raskar"], "year": 2020, "n_citations": 14}
{"id": 5295937, "s2_id": "37bd91cee94fd3584d93aa8b3974408cadaa2bec", "title": "Convergence Analysis of Inexact Randomized Iterative Methods", "abstract": "The first author would like to acknowledge Robert Mansel Gower, Georgios Loizou and Rachael Tappenden for useful discussions.", "venue": "SIAM J. Sci. Comput.", "authors": ["Nicolas  Loizou", "Peter  Richt\u00e1rik"], "year": 2020, "n_citations": 13}
{"id": 5300764, "s2_id": "59b8c3492b67537db81997f1bb950df04e6c46bc", "title": "SAFS: A deep feature selection approach for precision medicine", "abstract": "In this paper, we propose a new deep feature selection method based on deep architecture. Our method uses stacked auto-encoders for feature representation in higher-level abstraction. We developed and applied a novel feature learning approach to a specific precision medicine problem, which focuses on assessing and prioritizing risk factors for hypertension (HTN) in a vulnerable demographic subgroup (African-American). Our approach is to use deep learning to identify significant risk factors affecting left ventricular mass indexed to body surface area (LVMI) as an indicator of heart damage risk. The results show that our feature learning and representation approach leads to better results in comparison with others.", "venue": "2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)", "authors": ["Milad Zafar Nezhad", "Dongxiao  Zhu", "Xiangrui  Li", "Kai  Yang", "Phillip  Levy"], "year": 2016, "n_citations": 45}
{"id": 5301097, "s2_id": "075c2a11452a7ce7c87b53d1ed70daaee38fe4b1", "title": "Unsupervised Behaviour Discovery with Quality-Diversity Optimisation", "abstract": "Quality-Diversity algorithms refer to a class of evolutionary algorithms designed to find a collection of diverse and high-performing solutions to a given problem. In robotics, such algorithms can be used for generating a collection of controllers covering most of the possible behaviours of a robot. To do so, these algorithms associate a behavioural descriptor to each of these behaviours. Each behavioural descriptor is used for estimating the novelty of one behaviour compared to the others. In most existing algorithms, the behavioural descriptor needs to be hand-coded, thus requiring prior knowledge about the task to solve. In this paper, we introduce: Autonomous Robots Realising their Abilities, an algorithm that uses a dimensionality reduction technique to automatically learn behavioural descriptors based on raw sensory data. The performance of this algorithm is assessed on three robotic tasks in simulation. The experimental results show that it performs similarly to traditional hand-coded approaches without the requirement to provide any hand-coded behavioural descriptor. In the collection of diverse and highperforming solutions, it also manages to find behaviours that are novel with respect to more features than its hand-coded baselines. Finally, we introduce a variant of the algorithm which is robust to the dimensionality of the behavioural descriptor space.", "venue": "ArXiv", "authors": ["Luca  Grillotti", "Antoine  Cully"], "year": 2021, "n_citations": 2}
{"id": 5320303, "s2_id": "62663f96b166e2d47854d6da739cae48f1cd4827", "title": "Predicting Users' Value Changes by the Friends' Influence from Social Media Usage", "abstract": "Basic human values represent a set of values such as security, independence, success, kindness, and pleasure, which we deem important to our lives. Each of us holds different values with different degrees of significance. Existing studies show that values of a person can be identified from their social network usage. However, the value priority of a person may change over time due to different factors such as life experiences, influence, social structure and technology. Existing studies do not conduct any analysis regarding the change of users\u2019 value from the social influence, i.e., group persuasion, form the social media usage. In our research, first, we predict users\u2019 value score by the influence of friends from their social media usage. We propose a Bounded Confidence Model (BCM) based value dynamics model from 275 different ego networks in Facebook that predicts how social influence may persuade a person to change their value over time. Then, to predict better, we use particle swarm optimization based hyperparameter tuning technique. We observe that these optimized hyperparameters produce accurate future value score. We also run our approach with different machine learning based methods and find support vector regression (SVR) outperforms other regressor models. By using SVR with the best hyperparameters of BCM model, we find the lowest Mean Squared Error (MSE) score 0.00347.", "venue": "ArXiv", "authors": ["Md. Saddam Hossain Mukta", "Ahmed Shahriar Sakib", "Md. Adnanul Islam", "Mohiuddin  Ahmed", "Mumshad Ahamed Rifat"], "year": 2021, "n_citations": 0}
{"id": 5324495, "s2_id": "e9d878e9e40b4c5a63a16a0d838b13e2b437df84", "title": "DeepDiagnosis: Automatically Diagnosing Faults and Recommending Actionable Fixes in Deep Learning Programs", "abstract": "Deep Neural Networks (DNNs) are used in a wide variety of applications. However, as in any software application, DNN-based apps are afflicted with bugs. Previous work observed that DNN bug fix patterns are different from traditional bug fix patterns. Furthermore, those buggy models are non-trivial to diagnose and fix due to inexplicit errors with several options to fix them. To support developers in locating and fixing bugs, we propose DeepDiagnosis, a novel debugging approach that localizes the faults, reports error symptoms and suggests fixes for DNN programs. In the first phase, our technique monitors a training model, periodically checking for eight types of error conditions. Then, in case of problems, it reports messages containing sufficient information to perform actionable repairs to the model. In the evaluation, we thoroughly examine 444 models \u2013 53 real-world from GitHub and Stack Overflow, and 391 curated by AUTOTRAINER. DeepDiagnosis provides superior accuracy when compared to UMLUAT and DeepLocalize. Our technique is faster than AUTOTRAINER for fault localization. The results show that our approach can support additional types of models, while state-of-the-art was only able to handle classification ones. Our technique was able to report bugs that do not manifest as numerical errors during training. Also, it can provide actionable insights for fix whereas DeepLocalize can only report faults that lead to numerical errors during training. DeepDiagnosis manifests the best capabilities of fault detection, bug localization, and symptoms identification when compared to other approaches.", "venue": "ArXiv", "authors": ["Mohammad  Wardat", "Breno Dantas Cruz", "Wei  Le", "Hridesh  Rajan"], "year": 2021, "n_citations": 0}
{"id": 5327511, "s2_id": "84c73102e7bb5634c903faacaacd1def5096f033", "title": "Data-driven learning of the number of states in multi-state autoregressive models", "abstract": "In this work, we consider the class of multi-state autoregressive processes that can be used to model non-stationary time series of interest. In order to capture different autoregressive (AR) states underlying an observed time series, it is crucial to select the appropriate number of states. We propose a new and intuitive model selection technique based on the Gap statistics, which uses a null reference distribution on the stable AR filters to identify whether adding a new AR state significantly improves the performance of the model. To that end, we define a new distance measure between two AR filters based on the mean squared prediction error, and propose an efficient method to generate stable filters that are uniformly distributed in the coefficient space. Numerical results are provided to evaluate the performance of the proposed approach.", "venue": "2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)", "authors": ["Jie  Ding", "Mohammad  Noshad", "Vahid  Tarokh"], "year": 2015, "n_citations": 6}
{"id": 5355911, "s2_id": "58b67d0eb52ccb59d5b153ec0080365092017c0e", "title": "Optimized Hidden Markov Model based on Constrained Particle Swarm Optimization", "abstract": "As one of Bayesian analysis tools, Hidden Markov Model (HMM) has been used to in extensive applications. Most HMMs are solved by Baum-Welch algorithm (BWHMM) to predict the model parameters, which is difficult to find global optimal solutions. This paper proposes an optimized Hidden Markov Model with Particle Swarm Optimization (PSO) algorithm and so is called PSOHMM. In order to overcome the statistical constraints in HMM, the paper develops re-normalization and re-mapping mechanisms to ensure the constraints in HMM. The experiments have shown that PSOHMM can search better solution than BWHMM, and has faster convergence speed.", "venue": "ArXiv", "authors": ["L.  Chang", "Yacine  Ouzrout", "Antoine  Nongaillard", "Abdelaziz  Bouras"], "year": 2018, "n_citations": 4}
{"id": 5360605, "s2_id": "8fb545dc65d519ba000357089d6316ef9345dbd8", "title": "Towards Semi-Supervised Semantics Understanding from Speech", "abstract": "Much recent work on Spoken Language Understanding (SLU) falls short in at least one of three ways: models were trained on oracle text input and neglected the Automatics Speech Recognition (ASR) outputs, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. We proposed a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed speech to address these. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU corpus. In parallel, we identified two inadequate settings under which SLU models have been tested: noise-robustness and E2E semantics evaluation. We tested the proposed framework under realistic environmental noises and with a new metric, the slots edit F1 score, on two public SLU corpora. Experiments show that our SLU framework with speech as input can perform on par with those with oracle text as input in semantics understanding, while environmental noises are present, and a limited amount of labeled semantics data is available.", "venue": "ArXiv", "authors": ["Cheng-I  Lai", "Jin  Cao", "Sravan  Bodapati", "Shang-Wen  Li"], "year": 2020, "n_citations": 3}
{"id": 5378060, "s2_id": "cc6b31ed217794d3788d879768e4602720f9b882", "title": "Detecting Parkinsonian Tremor From IMU Data Collected in-the-Wild Using Deep Multiple-Instance Learning", "abstract": "Parkinson's Disease (PD) is a slowly evolving neurological disease that affects about $\\text{1}\\%$ of the population above 60 years old, causing symptoms that are subtle at first, but whose intensity increases as the disease progresses. Automated detection of these symptoms could offer clues as to the early onset of the disease, thus improving the expected clinical outcomes of the patients via appropriately targeted interventions. This potential has led many researchers to develop methods that use widely available sensors to measure and quantify the presence of PD symptoms such as tremor, rigidity and braykinesia. However, most of these approaches operate under controlled settings, such as in lab or at home, thus limiting their applicability under free-living conditions. In this work, we present a method for automatically identifying tremorous episodes related to PD, based on IMU signals captured via a smartphone device. We propose a Multiple-Instance Learning approach, wherein a subject is represented as an unordered bag of accelerometer signal segments and a single, expert-provided, tremor annotation. Our method combines deep feature learning with a learnable pooling stage that is able to identify key instances within the subject bag, while still being trainable end-to-end. We validate our algorithm on a newly introduced dataset of 45 subjects, containing accelerometer signals collected entirely in-the-wild. The good classification performance obtained in the conducted experiments suggests that the proposed method can efficiently navigate the noisy environment of in-the-wild recordings.", "venue": "IEEE Journal of Biomedical and Health Informatics", "authors": ["Alexandros  Papadopoulos", "Konstantinos  Kyritsis", "Lisa  Klingelhoefer", "Sevasti  Bostanjopoulou", "K. Ray Chaudhuri", "Anastasios  Delopoulos"], "year": 2020, "n_citations": 12}
{"id": 5423920, "s2_id": "5b04e81e77b9932fddad6318c0f4bd826f0ca2da", "title": "Designing Composites with Target Effective Young\u2019s Modulus using Reinforcement Learning", "abstract": "Advancements in additive manufacturing have enabled design and fabrication of materials and structures not previously realizable. In particular, the design space of composite materials and structures has vastly expanded, and the resulting size and complexity has challenged traditional design methodologies, such as brute force exploration and one factor at a time (OFAT) exploration, to find optimum or tailored designs. To address this challenge, supervised machine learning approaches have emerged to model the design space using curated training data; however, the selection of the training data is often determined by the user. In this work, we develop and utilize a Reinforcement learning (RL)-based framework for the design of composite structures which avoids the need for user-selected training data. For a 5 \u00d7 5 composite design space comprised of soft and compliant blocks of constituent material, we find that using this approach, the model can be trained using 2.78% of the total design space consists of 225 design possibilities. Additionally, the developed RL-based framework is capable of finding designs at a success rate exceeding 90%. The success of this approach motivates future learning frameworks to utilize RL for the design of composites and other material systems.", "venue": "SCF", "authors": ["Aldair E. Gongora", "Siddharth  Mysore", "Beichen  Li", "Wan  Shou", "Wojciech  Matusik", "Elise F. Morgan", "Keith A. Brown", "Emily  Whiting"], "year": 2021, "n_citations": 0}
{"id": 5463466, "s2_id": "f8e66760f588279c8b79f3099c46541db9aafad4", "title": "Deep Neural Network Loses Attention to Adversarial Images", "abstract": "Adversarial algorithms have shown to be effective against neural networks for a variety of tasks. Some adversarial algorithms perturb all the pixels in the image minimally for the image classification task in image classification. In contrast, some algorithms perturb few pixels strongly. However, very little information is available regarding why these adversarial samples so diverse from each other exist. Recently, Vargas et al. showed that the existence of these adversarial samples might be due to conflicting saliency within the neural network. We test this hypothesis of conflicting saliency by analysing the Saliency Maps (SM) and Gradient-weighted Class Activation Maps (Grad-CAM) of original and few different types of adversarial samples. We also analyse how different adversarial samples distort the attention of the neural network compared to original samples. We show that in the case of Pixel Attack, perturbed pixels either calls the network attention to themselves or divert the attention from them. Simultaneously, the Projected Gradient Descent Attack perturbs pixels so that intermediate layers inside the neural network lose attention for the correct class. We also show that both attacks affect the saliency map and activation maps differently. Thus, shedding light on why some defences successful against some attacks remain vulnerable against other attacks. We hope that this analysis will improve understanding of the existence and the effect of adversarial samples and enable the community to develop more robust neural networks.", "venue": "AISafety@IJCAI", "authors": ["Shashank  Kotyan", "Danilo Vasconcellos Vargas"], "year": 2021, "n_citations": 0}
{"id": 5484101, "s2_id": "960a8a7cba521c7fc3d56f95900ab4d433fecf63", "title": "Deep Bv: A Fully Automated System for Brain Ventricle Localization and Segmentation In 3D Ultrasound Images of Embryonic Mice", "abstract": "Volumetric analysis of brain ventricle (BV) structure is a key tool in the study of central nervous system development in embryonic mice. High-frequency ultrasound (HFU) is the only non-invasive, real-time modality available for rapid volumetric imaging of embryos in utero. However, manual segmentation of the BV from HFU volumes is tedious, time-consuming, and requires specialized expertise. In this paper, we propose a novel deep learning based BV segmentation system for whole-body HFU images of mouse embryos. Our fully automated system consists of two modules: localization and segmentation. It first applies a volumetric convolutional neural network on a 3D sliding window over the entire volume to identify a 3D bounding box containing the entire BV. It then employs a fully convolutional network to segment the detected bounding box into BV and background. The system achieves a Dice Similarity Coefficient (DSC) of 0.8956 for BV segmentation on an unseen 111 HFU volume test set surpassing the previous state-of-the-art method (DSC of 0.7119) by a margin of 25%.", "venue": "2018 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)", "authors": ["Ziming  Qiu", "Jack  Langerman", "Nitin  Nair", "Orlando  Aristiz\u00e1bal", "Jonathan  Mamou", "Daniel H. Turnbull", "Jeffrey A. Ketterling", "Yao  Wang"], "year": 2018, "n_citations": 7}
{"id": 5516203, "s2_id": "2d84bbc2ad14e7044965b45e51a1324dab381e28", "title": "Labels, Information, and Computation: Efficient, Privacy-Preserving Learning Using Sufficient Labels", "abstract": "In supervised learning, obtaining a large set of fully-labeled training data is expensive. We show that we do not always need full label information on every single training example to train a competent classifier. Specifically, inspired by the principle of sufficiency in statistics, we present a statistic (a summary) of the fully-labeled training set that captures almost all the relevant information for classification but at the same time is easier to obtain directly. We call this statistic \u201csufficiently-labeled data\u201d and prove its sufficiency and efficiency for finding the optimal hidden representations, on which competent classifier heads can be trained using as few as a single randomly-chosen fully-labeled example per class. Sufficiently-labeled data can be obtained from annotators directly without collecting the fully-labeled data first. And we prove that it is easier to directly obtain sufficiently-labeled data than obtaining fully-labeled data. Furthermore, sufficiently-labeled data naturally preserves user privacy by storing relative, instead of absolute, information. Extensive experimental results are provided to support our theory.", "venue": "ArXiv", "authors": ["Shiyu  Duan", "Jose C. Principe"], "year": 2021, "n_citations": 1}
{"id": 5542711, "s2_id": "ef78947a3c17bb8428080e35776275dce97139f0", "title": "Rates of Convergence for Sparse Variational Gaussian Process Regression", "abstract": "Excellent variational approximations to Gaussian process posteriors have been developed which avoid the $\\mathcal{O}\\left(N^3\\right)$ scaling with dataset size $N$. They reduce the computational cost to $\\mathcal{O}\\left(NM^2\\right)$, with $M\\ll N$ being the number of inducing variables, which summarise the process. While the computational cost seems to be linear in $N$, the true complexity of the algorithm depends on how $M$ must increase to ensure a certain quality of approximation. We address this by characterising the behavior of an upper bound on the KL divergence to the posterior. We show that with high probability the KL divergence can be made arbitrarily small by growing $M$ more slowly than $N$. A particular case of interest is that for regression with normally distributed inputs in D-dimensions with the popular Squared Exponential kernel, $M=\\mathcal{O}(\\log^D N)$ is sufficient. Our results show that as datasets grow, Gaussian process posteriors can truly be approximated cheaply, and provide a concrete rule for how to increase $M$ in continual learning scenarios.", "venue": "ICML", "authors": ["David R. Burt", "Carl E. Rasmussen", "Mark van der Wilk"], "year": 2019, "n_citations": 81}
{"id": 5548246, "s2_id": "74d45e83d49c4ca89bbb4479098e7b703f83d7d8", "title": "Large Margin Semi-supervised Structured Output Learning", "abstract": "In structured output learning, obtaining labelled data for real-world applications is usually costly, while unlabelled examples are available in abundance. Semi-supervised structured classification has been developed to handle large amounts of unlabelled structured data. In this work, we consider semi-supervised structural SVMs with domain constraints. The optimization problem, which in general is not convex, contains the loss terms associated with the labelled and unlabelled examples along with the domain constraints. We propose a simple optimization approach, which alternates between solving a supervised learning problem and a constraint matching problem. Solving the constraint matching problem is difficult for structured prediction, and we propose an efficient and effective hill-climbing method to solve it. The alternating optimization is carried out within a deterministic annealing framework, which helps in effective constraint matching, and avoiding local minima which are not very useful. The algorithm is simple to implement and achieves comparable generalization performance on benchmark datasets.", "venue": "ArXiv", "authors": ["Balamurugan  Palaniappan", "Shirish K. Shevade", "Sundararajan  Sellamanickam"], "year": 2013, "n_citations": 2}
{"id": 5589958, "s2_id": "5b216ad72c8132a0d62cadea28f86e146811d45b", "title": "Quantum-inspired annealers as Boltzmann generators for machine learning and statistical physics", "abstract": "Quantum simulators and processors are rapidly improving nowadays, but they are still not able to solve complex and multidimensional tasks of practical value. However, certain numerical algorithms inspired by the physics of real quantum devices prove to be efficient in application to specific problems, related, for example, to combinatorial optimization. Here we implement a numerical annealer based on simulating the coherent Ising machine as a tool to sample from a high-dimensional Boltzmann probability distribution with the energy functional defined by the classical Ising Hamiltonian. Samples provided by such a generator are then utilized for the partition function estimation of this distribution and for the training of a general Boltzmann machine. Our study opens up a door to practical application of numerical quantum-inspired annealers.", "venue": "ArXiv", "authors": ["Alexander E. Ulanov", "Egor S. Tiunov", "A. I. Lvovsky"], "year": 2019, "n_citations": 4}
{"id": 5608258, "s2_id": "65e1ada2360b42368a5f9f5e40ff436051c6fa84", "title": "Pomegranate: fast and flexible probabilistic modeling in python", "abstract": "We present pomegranate, an open source machine learning package for probabilistic modeling in Python. Probabilistic modeling encompasses a wide range of methods that explicitly describe uncertainty using probability distributions. Three widely used probabilistic models implemented in pomegranate are general mixture models, hidden Markov models, and Bayesian networks. A primary focus of pomegranate is to abstract away the complexities of training models from their definition. This allows users to focus on specifying the correct model for their application instead of being limited by their understanding of the underlying algorithms. An aspect of this focus involves the collection of additive sufficient statistics from data sets as a strategy for training models. This approach trivially enables many useful learning strategies, such as out-of-core learning, minibatch learning, and semi-supervised learning, without requiring the user to consider how to partition data or modify the algorithms to handle these tasks themselves. pomegranate is written in Cython to speed up calculations and releases the global interpreter lock to allow for built-in multithreaded parallelism, making it competitive with---or outperform---other implementations of similar algorithms. This paper presents an overview of the design choices in pomegranate, and how they have enabled complex features to be supported by simple code.", "venue": "J. Mach. Learn. Res.", "authors": ["Jacob  Schreiber"], "year": 2017, "n_citations": 98}
{"id": 5630253, "s2_id": "638181120b6754ce42a363cd8ebef078338f9de1", "title": "Casimir effect with machine learning", "abstract": "Vacuum fluctuations of quantum fields between physical objects depend on the shapes, positions, and internal composition of the latter. For objects of arbitrary shapes, even made from idealized materials, the calculation of the associated zero-point (Casimir) energy is an analytically intractable challenge. We propose a new numerical approach to this problem based on machine-learning techniques and illustrate the effectiveness of the method in a (2+1) dimensional scalar field theory. The Casimir energy is first calculated numerically using a Monte-Carlo algorithm for a set of the Dirichlet boundaries of various shapes. Then, a neural network is trained to compute this energy given the Dirichlet domain, treating the latter as black-and-white pixelated images. We show that after the learning phase, the neural network is able to quickly predict the Casimir energy for new boundaries of general shapes with reasonable accuracy.", "venue": "Physical Review Research", "authors": ["M. N. Chernodub", "Harold  Erbin", "I. V. Grishmanovskii", "V. A. Goy", "A. V. Molochkov"], "year": 2020, "n_citations": 4}
{"id": 5642998, "s2_id": "19031f8bb0ed20f238d1017ff7384f2ff30246b7", "title": "Distributed Deep Learning Model for Intelligent Video Surveillance Systems with Edge Computing", "abstract": "In this paper, we propose a Distributed Intelligent Video Surveillance (DIVS) system using Deep Learning (DL) algorithms and deploy it in an edge computing environment. We establish a multi-layer edge computing architecture and a distributed DL training model for the DIVS system. The DIVS system can migrate computing workloads from the network center to network edges to reduce huge network communication overhead and provide low-latency and accurate video analysis solutions. We implement the proposed DIVS system and address the problems of parallel training, model synchronization, and workload balancing. Task-level parallel and model-level parallel training methods are proposed to further accelerate the video analysis process. In addition, we propose a model parameter updating method to achieve model synchronization of the global DL model in a distributed EC environment. Moreover, a dynamic data migration approach is proposed to address the imbalance of workload and computational power of edge nodes. Experimental results showed that the EC architecture can provide elastic and scalable computing power, and the proposed DIVS system can efficiently handle video surveillance and analysis tasks.", "venue": "ArXiv", "authors": ["Jianguo  Chen", "Kenli  Li", "Qingying  Deng", "Keqin  Li", "Philip S. Yu"], "year": 2019, "n_citations": 79}
{"id": 5654561, "s2_id": "a7e001cb86e723f1da944a8257cd80666399b566", "title": "Adversarial attacks in consensus-based multi-agent reinforcement learning", "abstract": "Recently, many cooperative distributed multiagent reinforcement learning (MARL) algorithms have been proposed in the literature. In this work, we study the effect of adversarial attacks on a network that employs a consensus-based MARL algorithm. We show that an adversarial agent can persuade all the other agents in the network to implement policies that optimize an objective that it desires. In this sense, the standard consensus-based MARL algorithms are fragile to attacks.", "venue": "2021 American Control Conference (ACC)", "authors": ["Martin  Figura", "Krishna Chaitanya Kosaraju", "Vijay  Gupta"], "year": 2021, "n_citations": 1}
{"id": 5661470, "s2_id": "80528cf74583bf9c22ae0288d78d195a5ec9e8ff", "title": "A Witness Function Based Construction of Discriminative Models Using Hermite Polynomials", "abstract": "In machine learning, we are given a dataset of the form {(xj,yj)}j=1M, drawn as i.i.d. samples from an unknown probability distribution \u03bc; the marginal distribution for the xj's being \u03bc*, and the marginals of the kth class \u03bck*(x) possibly overlapping. We address the problem of detecting, with a high degree of certainty, for which x we have \u03bck*(x)>\u03bci*(x) for all i \u2260 k. We propose that rather than using a positive kernel such as the Gaussian for estimation of these measures, using a non-positive kernel that preserves a large number of moments of these measures yields an optimal approximation. We use multi-variate Hermite polynomials for this purpose, and prove optimal and local approximation results in a supremum norm in a probabilistic sense. Together with a permutation test developed with the same kernel, we prove that the kernel estimator serves as a \u201cwitness function\u201d in classification problems. Thus, if the value of this estimator at a point x exceeds a certain threshold, then the point is reliably in a certain class. This approach can be used to modify pretrained algorithms, such as neural networks or nonlinear dimension reduction techniques, to identify in-class vs out-of-class regions for the purposes of generative models, classification uncertainty, or finding robust centroids. This fact is demonstrated in a number of real world data sets including MNIST, CIFAR10, Science News documents, and LaLonde data sets.", "venue": "Frontiers in Applied Mathematics and Statistics", "authors": ["Hrushikesh N. Mhaskar", "Alexander  Cloninger", "Xiuyuan  Cheng"], "year": 2020, "n_citations": 4}
{"id": 5663010, "s2_id": "1fe48e1a7f19bc00dc42c8825f4ef1b2297108b8", "title": "Improving Sequential Latent Variable Models with Autoregressive Flows", "abstract": "We propose an approach for improving sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving frame of reference, removing temporal correlations, and simplifying the modeling of higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone flow-based models and as a component within sequential latent variable models. Results are presented on three benchmark video datasets, where autoregressive flow-based dynamics improve log-likelihood performance over baseline models. Finally, we illustrate the decorrelation and improved generalization properties of using flow-based dynamics.", "venue": "AABI", "authors": ["Joseph  Marino", "Lei  Chen", "Jiawei  He", "Stephan  Mandt"], "year": 2019, "n_citations": 8}
{"id": 5665504, "s2_id": "45c6cd948579cb98c9b81cd441b34ffe97d11f69", "title": "CFOF: A Concentration Free Measure for Anomaly Detection", "abstract": "We present a novel notion of outlier, called the Concentration Free Outlier Factor, or CFOF. As a main contribution, we formalize the notion of concentration of outlier scores and theoretically prove that CFOF does not concentrate in the Euclidean space for any arbitrary large dimensionality. To the best of our knowledge, there are no other proposals of data analysis measures related to the Euclidean distance for which it has been provided theoretical evidence that they are immune to the concentration effect. We determine the closed form of the distribution of CFOF scores in arbitrarily large dimensionalities and show that the CFOF score of a point depends on its squared norm standard score and on the kurtosis of the data distribution, thus providing a clear and statistically founded characterization of this notion. Moreover, we leverage this closed form to provide evidence that the definition does not suffer of the hubness problem affecting other measures. We prove that the number of CFOF outliers coming from each cluster is proportional to cluster size and kurtosis, a property that we call semi-locality. We determine that semi-locality characterizes existing reverse nearest neighbor-based outlier definitions, thus clarifying the exact nature of their observed local behavior. We also formally prove that classical distance-based and density-based outliers concentrate both for bounded and unbounded sample sizes and for fixed and variable values of the neighborhood parameter. We introduce the fast-CFOF algorithm for detecting outliers in large high-dimensional dataset. The algorithm has linear cost, supports multi-resolution analysis, and is embarrassingly parallel. Experiments highlight that the technique is able to efficiently process huge datasets and to deal even with large values of the neighborhood parameter, to avoid concentration, and to obtain excellent accuracy.", "venue": "ACM Trans. Knowl. Discov. Data", "authors": ["Fabrizio  Angiulli"], "year": 2020, "n_citations": 4}
{"id": 5700423, "s2_id": "b32466245aaf9c0d1ba6a0c279aa14e948df31d0", "title": "Scalable Probabilistic Entity-Topic Modeling", "abstract": "We present an LDA approach to entity disambiguation. Each topic is associated with a Wikipedia article and topics generate either content words or entity mentions. Training such models is challenging because of the topic and vocabulary size, both in the millions. We tackle these problems using a novel distributed inference and representation framework based on a parallel Gibbs sampler guided by the Wikipedia link graph, and pipelines of MapReduce allowing fast and memoryfrugal processing of large datasets. We report state-of-the-art performance on a public dataset.", "venue": "ArXiv", "authors": ["Neil  Houlsby", "Massimiliano  Ciaramita"], "year": 2013, "n_citations": 4}
{"id": 5705922, "s2_id": "ed268b8875cf37b7c67ec4e5fdbc73c240512c20", "title": "Seeking Visual Discomfort: Curiosity-driven Representations for Reinforcement Learning", "abstract": "Vision-based reinforcement learning (RL) is a promising approach to solve control tasks involving images as the main observation. State-of-the-art RL algorithms still struggle in terms of sample efficiency, especially when using image observations. This has led to increased attention on integrating state representation learning (SRL) techniques into the RL pipeline. Work in this field demonstrates a substantial improvement in sample efficiency among other benefits. However, to take full advantage of this paradigm, the quality of samples used for training plays a crucial role. More importantly, the diversity of these samples could affect the sample efficiency of vision-based RL, but also its generalization capability. In this work, we present an approach to improve sample diversity for state representation learning. Our method enhances the exploration capability of RL algorithms, by taking advantage of the SRL setup. Our experiments show that our proposed approach boosts the visitation of problematic states, improves the learned state representation, and outperforms the baselines for all tested environments. These results are most apparent for environments where the baseline methods struggle. Even in simple environments, our method stabilizes the training, reduces the reward variance, and promotes sample efficiency.", "venue": "ArXiv", "authors": ["Elie  Aljalbout", "Maximilian  Ulmer", "Rudolph  Triebel"], "year": 2021, "n_citations": 0}
{"id": 5707134, "s2_id": "35d688b6a33df5d9e5b85e87ac2c68a7a53b60cf", "title": "Online Feature Ranking for Intrusion Detection Systems", "abstract": "Many current approaches to the design of intrusion detection systems apply feature selection in a static, non-adaptive fashion. These methods often neglect the dynamic nature of network data which requires to use adaptive feature selection techniques. In this paper, we present a simple technique based on incremental learning of support vector machines in order to rank the features in real time within a streaming model for network data. Some illustrative numerical experiments with two popular benchmark datasets show that our approach allows to adapt to the changes in normal network behaviour and novel attack patterns which have not been experienced before.", "venue": "ArXiv", "authors": ["Buse Gul Atli", "Alexander  Jung"], "year": 2018, "n_citations": 1}
{"id": 5707708, "s2_id": "541713726b0ca30a6723b4dd0b6cfb6584e2f523", "title": "Learning unbelievable marginal probabilities", "abstract": "Loopy belief propagation performs approximate inference on graphical models with loops. One might hope to compensate for the approximation by adjusting model parameters. Learning algorithms for this purpose have been explored previously, and the claim has been made that every set of locally consistent marginals can arise from belief propagation run on a graphical model. On the contrary, here we show that many probability distributions have marginals that cannot be reached by belief propagation using any set of model parameters or any learning algorithm. We call such marginals `unbelievable.' This problem occurs whenever the Hessian of the Bethe free energy is not positive-definite at the target marginals. All learning algorithms for belief propagation necessarily fail in these cases, producing beliefs or sets of beliefs that may even be worse than the pre-learning approximation. We then show that averaging inaccurate beliefs, each obtained from belief propagation using model parameters perturbed about some learned mean values, can achieve the unbelievable marginals.", "venue": "ArXiv", "authors": ["Xaq  Pitkow", "Yashar  Ahmadian", "Kenneth D. Miller"], "year": 2011, "n_citations": 0}
{"id": 5717077, "s2_id": "2486a746caed1860cbda16f1342b95ca75b84ad3", "title": "A New Ensemble Adversarial Attack Powered by Long-term Gradient Memories", "abstract": "Deep neural networks are vulnerable to adversarial attacks. More importantly, some adversarial examples crafted against an ensemble of pre-trained source models can transfer to other new target models, thus pose a security threat to blackbox applications (when the attackers have no access to the target models). Despite adopting diverse architectures and parameters, source and target models often share similar decision boundaries. Therefore, if an adversary is capable of fooling several source models concurrently, it can potentially capture intrinsic transferable adversarial information that may allow it to fool a broad class of other black-box target models. Current ensemble attacks, however, only consider a limited number of source models to craft an adversary, and obtain poor transferability. In this paper, we propose a novel black-box attack, dubbed Serial-Mini-BatchEnsemble-Attack (SMBEA). SMBEA divides a large number of pre-trained source models into several mini-batches. For each single batch, we design 3 new ensemble strategies to improve the intra-batch transferability. Besides, we propose a new algorithm that recursively accumulates the \u201clong-term\u201d gradient memories of the previous batch to the following batch. This way, the learned adversarial information can be preserved and the inter-batch transferability can be improved. Experiments indicate that our method outperforms state-ofthe-art ensemble attacks over multiple pixel-to-pixel vision tasks including image translation and salient region prediction. Our method successfully fools two online black-box saliency prediction systems including DeepGaze-II (Kummerer 2017) and SALICON (Huang et al. 2017). Finally, we also contribute a new repository to promote the research on adversarial attack and defense over pixel-to-pixel tasks: https://github.com/CZHQuality/AAA-Pix2pix.", "venue": "AAAI", "authors": ["Zhaohui  Che", "Ali  Borji", "Guangtao  Zhai", "Suiyi  Ling", "Jing  Li", "Patrick Le Callet"], "year": 2020, "n_citations": 4}
{"id": 5723785, "s2_id": "6130a6e60f99dcaaf80c313d701adbbe56651297", "title": "Exact and approximate inference in graphical models: variable elimination and beyond", "abstract": "Probabilistic graphical models offer a powerful framework to account for the dependence structure between variables, which can be represented as a graph. The dependence between variables may render inference tasks such as computing normalizing constant, marginalization or optimization intractable. The objective of this paper is to review techniques exploiting the graph structure for exact inference borrowed from optimization and computer science. They are not yet standard in the statistician toolkit, and we specify under which conditions they are efficient in practice. They are built on the principle of variable elimination whose complexity is dictated in an intricate way by the order in which variables are eliminated in the graph. The so-called treewidth of the graph characterizes this algorithmic complexity: low-treewidth graphs can be processed efficiently. Algorithmic solutions derived from variable elimination and the notion of treewidth are illustrated on problems of treewidth computation and inference in challenging benchmarks from optimization competitions. We also review how efficient techniques for approximate inference such as loopy belief propagation and variational approaches can be linked to variable elimination and we illustrate them in the context of Expectation-Maximisation procedures for parameter estimation in coupled Hidden Markov Models.", "venue": "ArXiv", "authors": ["Nathalie  Peyrard", "Simon de Givry", "Alain  Franc", "St\u00e9phane  Robin", "R\u00e9gis  Sabbadin", "Thomas  Schiex", "Matthieu  Vignes"], "year": 2015, "n_citations": 4}
{"id": 5726270, "s2_id": "44773edf9b3bc863210f49a5eeb06df6f93f8d28", "title": "Common-Knowledge Concept Recognition for SEVA", "abstract": "We build a common-knowledge concept recognition system for a Systems Engineer's Virtual Assistant (SEVA) which can be used for downstream tasks such as relation extraction, knowledge graph construction, and question-answering. The problem is formulated as a token classification task similar to named entity extraction. With the help of a domain expert and text processing methods, we construct a dataset annotated at the word-level by carefully defining a labelling scheme to train a sequence model to recognize systems engineering concepts. We use a pre-trained language model and fine-tune it with the labeled dataset of concepts. In addition, we also create some essential datasets for information such as abbreviations and definitions from the systems engineering domain. Finally, we construct a simple knowledge graph using these extracted concepts along with some hyponym relations.", "venue": "AAAI Spring Symposium: Combining Machine Learning with Knowledge Engineering", "authors": ["Jitin  Krishnan", "Patrick  Coronado", "Hemant  Purohit", "Huzefa  Rangwala"], "year": 2020, "n_citations": 0}
{"id": 5732636, "s2_id": "fdb39ff3c97221371ff1b6e260a5ea9ca56bd112", "title": "Deep Joint Source-Channel Coding for Wireless Image Transmission with Adaptive Rate Control", "abstract": "We present a novel adaptive deep joint source-channel coding (JSCC) scheme for wireless image transmission. The proposed scheme supports multiple rates using a single deep neural network (DNN) model and learns to dynamically control the rate based on the channel condition and image contents. Specifically, a policy network is introduced to exploit the tradeoff space between the rate and signal quality. To train the policy network, the Gumbel-Softmax trick is adopted to make the policy network differentiable and hence the whole JSCC scheme can be trained end-to-end. To the best of our knowledge, this is the first deep JSCC scheme that can automatically adjust its rate using a single network model. Experiments show that our scheme successfully learns a reasonable policy that decreases channel bandwidth utilization for high SNR scenarios or simple image contents. For an arbitrary target rate, our rate-adaptive scheme using a single model achieves similar performance compared to an optimized model specifically trained for that fixed target rate. To reproduce our results, we make the source code publicly available at https: //github.com/mingyuyng/Dynamic_JSCC.", "venue": "ArXiv", "authors": ["Mingyu  Yang", "Hun-Seok  Kim"], "year": 2021, "n_citations": 0}
{"id": 5738360, "s2_id": "97d7af123c08cfa108689ddb3957cf5987297172", "title": "A Novel Efficient Approach with Data-Adaptive Capability for OMP-based Sparse Subspace Clustering", "abstract": "Orthogonal Matching Pursuit (OMP) plays an important role in data science and its applications such as sparse subspace clustering and image processing. However, the existing OMP-based approaches lack of data adaptiveness so that the data cannot be represented well enough and may lose the accuracy. This paper proposes a novel approach to enhance the data-adaptive capability for OMP-based sparse subspace clustering. In our method a parameter selection process is developed to adjust the parameters based on the data distribution for information representation. Our theoretical analysis indicates that the parameter selection process can efficiently coordinate with any OMP-based methods to improve the clustering performance. Also a new Self-Expressive-Affinity (SEA) ratio metric is defined to measure the sparse representation conversion efficiency for spectral clustering to obtain data segmentations. Our experiments show that proposed approach can achieve better performances compared with other OMP-based sparse subspace clustering algorithms in terms of clustering accuracy, SEA ratio and representation quality, also keep the time efficiency and anti-noise ability.", "venue": "ArXiv", "authors": ["Jiaqiyu  Zhan", "Zhiqiang  Bai", "Yuesheng  Zhu"], "year": 2019, "n_citations": 2}
{"id": 5747468, "s2_id": "80b0ee8b3f738e535dcf8b8c1223be5f8e3c25ba", "title": "Language Modelling as a Multi-Task Problem", "abstract": "In this paper, we propose to study language modelling as a multi-task problem, bringing together three strands of research: multi-task learning, linguistics, and interpretability. Based on hypotheses derived from linguistic theory, we investigate whether language models adhere to learning principles of multi-task learning during training. To showcase the idea, we analyse the generalisation behaviour of language models as they learn the linguistic concept of Negative Polarity Items (NPIs). Our experiments demonstrate that a multi-task setting naturally emerges within the objective of the more general task of language modelling. We argue that this insight is valuable for multi-task learning, linguistics and interpretability research and can lead to exciting new findings in all three domains.", "venue": "EACL", "authors": ["Lucas  Weber", "Jaap  Jumelet", "Elia  Bruni", "Dieuwke  Hupkes"], "year": 2021, "n_citations": 2}
{"id": 5776585, "s2_id": "251795393f3f94279d531367da1fe81e0ab65b9e", "title": "LIAF-Net: Leaky Integrate and Analog Fire Network for Lightweight and Efficient Spatiotemporal Information Processing", "abstract": "Spiking neural networks (SNNs) based on the leaky integrate and fire (LIF) model have been applied to energy-efficient temporal and spatiotemporal processing tasks. Due to the bioplausible neuronal dynamics and simplicity, LIF-SNN benefits from event-driven processing, however, usually face the embarrassment of reduced performance. This may because, in LIF-SNN, the neurons transmit information via spikes. To address this issue, in this work, we propose a leaky integrate and analog fire (LIAF) neuron model so that analog values can be transmitted among neurons, and a deep network termed LIAF-Net is built on it for efficient spatiotemporal processing. In the temporal domain, LIAF follows the traditional LIF dynamics to maintain its temporal processing capability. In the spatial domain, LIAF is able to integrate spatial information through convolutional integration or fully connected integration. As a spatiotemporal layer, LIAF can also be used with traditional artificial neural network (ANN) layers jointly. In addition, the built network can be trained with backpropagation through time (BPTT) directly, which avoids the performance loss caused by ANN to SNN conversion. Experiment results indicate that LIAF-Net achieves comparable performance to the gated recurrent unit (GRU) and long short-term memory (LSTM) on bAbI question answering (QA) tasks and achieves state-of-the-art performance on spatiotemporal dynamic vision sensor (DVS) data sets, including MNIST-DVS, CIFAR10-DVS, and DVS128 Gesture, with much less number of synaptic weights and computational overhead compared with traditional networks built by LSTM, GRU, convolutional LSTM (ConvLSTM), or 3-D convolution (Conv3D). Compared with traditional LIF-SNN, LIAF-Net also shows dramatic accuracy gain on all these experiments. In conclusion, LIAF-Net provides a framework combining the advantages of both ANNs and SNNs for lightweight and efficient spatiotemporal information processing.", "venue": "IEEE transactions on neural networks and learning systems", "authors": ["Zhenzhi  Wu", "Hehui  Zhang", "Yihan  Lin", "Guoqi  Li", "Meng  Wang", "Ye  Tang"], "year": 2021, "n_citations": 2}
{"id": 5788616, "s2_id": "be2ce56434c8cf50c08f8be6f4f9b9f7c716eabd", "title": "A Convnet for Non-maximum Suppression", "abstract": "Non-maximum suppression (NMS) is used in virtually all state-of-the-art object detection pipelines. While essential object detection ingredients such as features, classifiers, and proposal methods have been extensively researched surprisingly little work has aimed to systematically address NMS. The de-facto standard for NMS is based on greedy clustering with a fixed distance threshold, which forces to trade-off recall versus precision. We propose a convnet designed to perform NMS of a given set of detections. We report experiments on a synthetic setup, crowded pedestrian scenes, and for general person detection. Our approach overcomes the intrinsic limitations of greedy NMS, obtaining better recall and precision.", "venue": "GCPR", "authors": ["Jan Hendrik Hosang", "Rodrigo  Benenson", "Bernt  Schiele"], "year": 2016, "n_citations": 49}
{"id": 5815111, "s2_id": "d3658797db2050d777347f7df2a58f7ffee86276", "title": "Automatic Analysis of EEGs Using Big Data and Hybrid Deep Learning Architectures", "abstract": "Brain monitoring combined with automatic analysis of EEGs provides a clinical decision support tool that can reduce time to diagnosis and assist clinicians in real-time monitoring applications (e.g., neurological intensive care units). Clinicians have indicated that a sensitivity of 95% with specificity below 5% was the minimum requirement for clinical acceptance. In this study, a high-performance automated EEG analysis system based on principles of machine learning and big data is proposed. This hybrid architecture integrates hidden Markov models (HMMs) for sequential decoding of EEG events with deep learning-based post-processing that incorporates temporal and spatial context. These algorithms are trained and evaluated using the Temple University Hospital EEG, which is the largest publicly available corpus of clinical EEG recordings in the world. This system automatically processes EEG records and classifies three patterns of clinical interest in brain activity that might be useful in diagnosing brain disorders: (1) spike and/or sharp waves, (2) generalized periodic epileptiform discharges, (3) periodic lateralized epileptiform discharges. It also classifies three patterns used to model the background EEG activity: (1) eye movement, (2) artifacts, and (3) background. Our approach delivers a sensitivity above 90% while maintaining a specificity below 5%. We also demonstrate that this system delivers a low false alarm rate, which is critical for any spike detection application.", "venue": "Front. Hum. Neurosci.", "authors": ["Meysam  Golmohammadi", "Amir Hossein Harati Nejad Torbati", "Silvia Lopez de Diego", "Iyad  Obeid", "Joseph  Picone"], "year": 2019, "n_citations": 32}
{"id": 5824881, "s2_id": "42cb800f8169a287bed42f4a0d822e1df311e00a", "title": "Differential equations as models of deep neural networks", "abstract": "In this work we systematically analyze general properties of differential equations used as machine learning models. We demonstrate that the gradient of the loss function with respect to to the hidden state can be considered as a generalized momentum conjugate to the hidden state, allowing application of the tools of classical mechanics. In addition, we show that not only residual networks, but also feedforward neural networks with small nonlinearities and the weights matrices deviating only slightly from identity matrices can be related to the differential equations. We propose a differential equation describing such networks and investigate its properties.", "venue": "ArXiv", "authors": ["Julius  Ruseckas"], "year": 2019, "n_citations": 2}
{"id": 5857378, "s2_id": "255f0fe65f5592659c285a1ecb733dec57aada23", "title": "Towards an Appropriate Query, Key, and Value Computation for Knowledge Tracing", "abstract": "In this paper, we propose a novel Transformer-based model for knowledge tracing, SAINT: Separated Self-AttentIve Neural Knowledge Tracing. SAINT has an encoder-decoder structure where the exercise and response embedding sequences separately enter, respectively, the encoder and the decoder. The encoder applies self-attention layers to the sequence of exercise embeddings, and the decoder alternately applies self-attention layers and encoder-decoder attention layers to the sequence of response embeddings. This separation of input allows us to stack attention layers multiple times, resulting in an improvement in area under receiver operating characteristic curve (AUC). To the best of our knowledge, this is the first work to suggest an encoder-decoder model for knowledge tracing that applies deep self-attentive layers to exercises and responses separately. We empirically evaluate SAINT on a large-scale knowledge tracing dataset, EdNet, collected by an active mobile education application, Santa, which has 627,347 users, 72,907,005 response data points as well as a set of 16,175 exercises gathered since 2016. The results show that SAINT achieves state-of-the-art performance in knowledge tracing with an improvement of 1.8% in AUC compared to the current state-of-the-art model.", "venue": "L@S", "authors": ["Youngduck  Choi", "Youngnam  Lee", "Junghyun  Cho", "Jineon  Baek", "Byungsoo  Kim", "Yeongmin  Cha", "Dongmin  Shin", "Chan  Bae", "Jaewe  Heo"], "year": 2020, "n_citations": 19}
{"id": 5903186, "s2_id": "0554c1dcee7f0593d12ca5a64d54c5e4af10199c", "title": "DNN Architecture for High Performance Prediction on Natural Videos Loses Submodule's Ability to Learn Discrete-World Dataset", "abstract": "Is cognition a collection of loosely connected functions tuned to different tasks, or can there be a general learning algorithm? If such an hypothetical general algorithm did exist, tuned to our world, could it adapt seamlessly to a world with different laws of nature? We consider the theory that predictive coding is such a general rule, and falsify it for one specific neural architecture known for high-performance predictions on natural videos and replication of human visual illusions: PredNet. Our results show that PredNet's high performance generalizes without retraining on a completely different natural video dataset. Yet PredNet cannot be trained to reach even mediocre accuracy on an artificial video dataset created with the rules of the Game of Life (GoL). We also find that a submodule of PredNet, a Convolutional Neural Network trained alone, reaches perfect accuracy on the GoL while being mediocre for natural videos, showing that PredNet's architecture itself is responsible for both the high performance on natural videos and the loss of performance on the GoL. Just as humans cannot predict the dynamics of the GoL, our results suggest that there might be a trade-off between high performance on sensory inputs with different sets of rules.", "venue": "The 2019 Conference on Artificial Life", "authors": ["Lana  Sinapayen", "Atsushi  Noda"], "year": 2019, "n_citations": 1}
{"id": 5922383, "s2_id": "7c79506ce14cb24695e7072c47e890f7f536d04f", "title": "Neighborhood Features Help Detecting Non-Technical Losses in Big Data Sets", "abstract": "Electricity theft occurs around the world in both developed and developing countries and may range up to 40% of the total electricity distributed. More generally, electricity theft belongs to non-technical losses (NTL), which occur during the distribution of electricity in power grids. In this paper, we build features from the neighborhood of customers. We first split the area in which the customers are located into grids of different sizes. For each grid cell we then compute the proportion of inspected customers and the proportion of NTL found among the inspected customers. We then analyze the distributions of features generated and show why they are useful to predict NTL. In addition, we compute features from the consumption time series of customers. We also use master data features of customers, such as their customer class and voltage of their connection. We compute these features for a Big Data base of 31M meter readings, 700K customers and 400K inspection results. We then use these features to train four machine learning algorithms that are particularly suitable for Big Data sets because of their parallelizable structure: logistic regression, k-nearest neighbors, linear support vector machine and random forest. Using the neighborhood features instead of only analyzing the time series has resulted in appreciable results for Big Data sets for varying NTL proportions of 1%-90%. This work can therefore be deployed to a wide range of different regions.", "venue": "2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT)", "authors": ["Patrick O. Glauner", "Jorge Augusto Meira", "Lautaro  Dolberg", "Radu  State", "Franck  Bettinger", "Yves  Rangoni"], "year": 2016, "n_citations": 21}
{"id": 5934645, "s2_id": "19eb2af6e93d270e2a62f7ba1a31a519c0bf8034", "title": "Model Comparison of Dark Energy models Using Deep Network", "abstract": "This work uses a combination of a variational auto-encoder and generative adversarial network to compare different dark energy models in light of observations, e.g., the distance modulus from type Ia supernovae. The network finds an analytical variational approximation to the true posterior of the latent parameters in the models, yielding consistent model comparison results with those derived by the standard Bayesian method, which suffers from a computationally expensive integral over the parameters in the product of the likelihood and the prior. The parallel computational nature of the network together with the stochastic gradient descent optimization technique leads to an efficient way to compare the physical models given a set of observations. The converged network also provides interpolation for a dataset, which is useful for data reconstruction.", "venue": "Research in Astronomy and Astrophysics", "authors": ["Shi-Yu  Li", "Yun-Long  Li", "Tong-Jie  Zhang"], "year": 2019, "n_citations": 6}
{"id": 5948794, "s2_id": "ba4ed125ee4a11e2a8b017487d32966a772e37bd", "title": "Learning from Interventions using Hierarchical Policies for Safe Learning", "abstract": "Learning from Demonstrations (LfD) via Behavior Cloning (BC) works well on multiple complex tasks. However, a limitation of the typical LfD approach is that it requires expert demonstrations for all scenarios, including those in which the algorithm is already well-trained. The recently proposed Learning from Interventions (LfI) overcomes this limitation by using an expert overseer. The expert overseer only intervenes when it suspects that an unsafe action is about to be taken. Although LfI significantly improves over LfD, the state-of-the-art LfI fails to account for delay caused by the expert's reaction time and only learns short-term behavior. We address these limitations by 1) interpolating the expert's interventions back in time, and 2) by splitting the policy into two hierarchical levels, one that generates sub-goals for the future and another that generates actions to reach those desired sub-goals. This sub-goal prediction forces the algorithm to learn long-term behavior while also being robust to the expert's reaction time. Our experiments show that LfI using sub-goals in a hierarchical policy framework trains faster and achieves better asymptotic performance than typical LfD.", "venue": "AAAI", "authors": ["Jing  Bi", "Vikas  Dhiman", "Tianyou  Xiao", "Chenliang  Xu"], "year": 2020, "n_citations": 2}
{"id": 5960621, "s2_id": "40c47f915e57f7d3eeaadfbf80aca853327dd0a2", "title": "A simple differentiable programming language", "abstract": "Automatic differentiation plays a prominent role in scientific computing and in modern machine learning, often in the context of powerful programming systems. The relation of the various embodiments of automatic differentiation to the mathematical notion of derivative is not always entirely clear---discrepancies can arise, sometimes inadvertently. In order to study automatic differentiation in such programming contexts, we define a small but expressive programming language that includes a construct for reverse-mode differentiation. We give operational and denotational semantics for this language. The operational semantics employs popular implementation techniques, while the denotational semantics employs notions of differentiation familiar from real analysis. We establish that these semantics coincide.", "venue": "Proc. ACM Program. Lang.", "authors": ["Martin  Abadi", "Gordon D. Plotkin"], "year": 2020, "n_citations": 28}
{"id": 5971135, "s2_id": "6339d9370249daf0567bb56ad977c49bfbf8b28d", "title": "Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval", "abstract": "We present Retrieve in Style (RIS), an unsupervised framework for facial feature transfer and retrieval on real images. Recent work shows capabilities of transferring local facial features by capitalizing on the disentanglement property of the StyleGAN latent space. RIS improves existing art on the following: 1) Introducing more effective feature disentanglement to allow for challenging transfers (i.e., hair, pose) that were not shown possible in SoTA methods. 2) Eliminating the need for per-image hyperparameter tuning, and for computing a catalog over a large batch of images. 3) Enabling fine-grained face retrieval using disentangled facial features (e.g., eyes). To our best knowledge, this is the first work to retrieve face images at this fine level. 4) Demonstrating robust, natural editing on real images. Our qualitative and quantitative analyses show RIS achieves both high-fidelity feature transfers and accurate fine-grained retrievals on real images. We also discuss the responsible applications of RIS. Our code is available at https://github.com/ mchong6/RetrieveInStyle.", "venue": "ArXiv", "authors": ["Min Jin Chong", "Wen-Sheng  Chu", "Abhishek  Kumar", "David  Forsyth"], "year": 2021, "n_citations": 1}
{"id": 5981743, "s2_id": "4fa32fec61c50f8339a05e097dacebe71cf9ab8e", "title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "venue": "ICLR", "authors": ["Samuel L. Smith", "Benoit  Dherin", "David G. T. Barrett", "Soham  De"], "year": 2021, "n_citations": 30}
{"id": 5989532, "s2_id": "076a15c25e9cd1a5d3fd4f48947a004d9872b4d9", "title": "Defending Label Inference and Backdoor Attacks in Vertical Federated Learning", "abstract": "Since there are multiple parties in collaborative learning settings like federated learning, curious parities might be honest but are attempted to infer other parties\u2019 private data through inference attacks while malicious parties might manipulate the learning process for their own purposes through backdoor attacks. However, most existing works only consider the federated learning scenario where data are partitioned by samples (HFL). The feature-partitioned federated learning (VFL) can be another important scenario in many real-world applications. Attacks and defenses in such scenarios are especially challenging when the attackers and the defenders are not able to access the features or model parameters of other participants. Previous work have only shown that private labels can be reconstructed from communication of per-sample gradients. In this paper, we first show that private labels can be reconstructed even when only batch-averaged gradients instead of sample-level gradients are revealed. It is a common presumption that batch-averaged information is safe to share, therefore batch label inference attack is a severe challenge to VFL. In addition, we show that a passive party (a party without labels) in VFL can even replace its corresponding labels in the active party with a target label through a gradientreplacement attack. To defend against batch label inference attack, we introduce a novel technique termed confusional autoencoder (CoAE), based on autoencoder and entropy regularization. We demonstrate that label inference attacks can be successfully blocked by this technique without hurting as much main task accuracy as compared to existing methods. Our CoAE technique is also effective in defending the gradient-replacement backdoor attack, making it an universal and practical defense strategy with no change to the original VFL protocol. We demonstrate the effectiveness of our approaches under both two-party and multi-party VFL settings. To the best of our knowledge, this is the first systematic study to deal with label inference and backdoor attacks in the feature-partitioned federated learning framework.", "venue": "ArXiv", "authors": ["Yang  Liu", "Zhihao  Yi", "Yan  Kang", "Yuanqin  He", "Wenhan  Liu", "Tianyuan  Zou", "Qiang  Yang"], "year": 2021, "n_citations": 0}
{"id": 5994145, "s2_id": "5e2c95172815de200f7e500de55005a01ff08311", "title": "Truth Discovery in Sequence Labels from Crowds", "abstract": "Annotations quality and quantity positively affect the performance of sequence labeling, a vital task in Natural Language Processing. Hiring domain experts to annotate a corpus set is very costly in terms of money and time. Crowdsourcing platforms, such as Amazon Mechanical Turk (AMT), have been deployed to assist in this purpose. However, these platforms are prone to human errors due to the lack of expertise; hence, one worker\u2019s annotations cannot be directly used to train the model. Existing literature in annotation aggregation more focuses on binary or multi-choice problems. In recent years, handling the sequential label aggregation tasks on imbalanced datasets with complex dependencies between tokens has been challenging. To conquer the challenge, we propose an optimization-based method that infers the best set of aggregated annotations using labels provided by workers. The proposed Aggregation method for Sequential Labels from Crowds (AggSLC) jointly considers the characteristics of sequential labeling tasks, workers\u2019 reliabilities, and advanced machine learning techniques. We evaluate AggSLC on different crowdsourced data for Named Entity Recognition (NER), Information Extraction tasks in biomedical (PICO), and the simulated dataset. Our results show that the proposed method outperforms the state-of-the-art aggregation methods. To achieve insights into the framework, we study AggSLC components\u2019 effectiveness through ablation studies by evaluating our model in the absence of the prediction module and inconsistency loss function. Theoretical analysis of our algorithm\u2019s convergence points that the proposed AggSLC halts after a finite number of iterations.", "venue": "ArXiv", "authors": ["Nasim  Sabetpour", "Adithya  Kulkarni", "Sihong  Xie", "Qi  Li"], "year": 2021, "n_citations": 0}
{"id": 6002590, "s2_id": "347b1fd3c01e0779aa2103e6aa617849318e3ad3", "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data", "abstract": "Multi-Task Learning (MTL) has emerged as a promising approach for transferring learned knowledge across different tasks. However, multi-task learning must deal with challenges such as: overfitting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Additionally, in Natural Language Processing (NLP), MTL alone has typically not reached the performance level possible through per-task fine-tuning of pretrained models. However, many fine-tuning approaches are both parameter inefficient, e.g. potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel transformer based architecture consisting of a new conditional attention mechanism as well as a set of task conditioned modules that facilitate weight sharing. Through this construction we achieve more efficient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model fixed. We also use a new multi-task data sampling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach we are able to surpass single-task fine-tuning methods while being parameter and data efficient. With our base model, we attain 2.2% higher performance compared to a full fine-tuned BERT large model on the GLUE benchmark, adding only 5.6% more trained parameters per task (whereas naive fine-tuning potentially adds 100% of the trained parameters per task) and needing only 64.6% of the data. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets.", "venue": "ICLR", "authors": ["Jonathan  Pilault", "Amine  Elhattami", "Christopher  Pal"], "year": 2021, "n_citations": 26}
{"id": 6007747, "s2_id": "046612d4cddbea8de2aaa0825d614426b672bdbf", "title": "Light-Field Microscopy for optical imaging of neuronal activity: when model-based methods meet data-driven approaches", "abstract": "Understanding how networks of neurons process information is one of the key challenges in modern neuroscience. A necessary step to achieve this goal is to be able to observe the dynamics of large populations of neurons over a large area of the brain. Light-field microscopy (LFM), a type of scanless microscope, is a particularly attractive candidate for high-speed three-dimensional (3D) imaging. It captures volumetric information in a single snapshot, allowing volumetric imaging at video frame-rates. Specific features of imaging neuronal activity using LFM call for the development of novel machine learning approaches that fully exploit priors embedded in physics and optics models. Signal processing theory and wave-optics theory could play a key role in filling this gap, and contribute to novel computational methods with enhanced interpretability and generalization by integrating model-driven and data-driven approaches. This paper is devoted to a comprehensive survey to state-of-the-art of computational methods for LFM, with a focus on model-based and data-driven approaches.", "venue": "ArXiv", "authors": ["Pingfan  Song", "Herman  Verinaz-Jadan", "Carmel L. Howe", "Amanda J. Foust", "Pier Luigi Dragotti"], "year": 2021, "n_citations": 0}
{"id": 6025159, "s2_id": "39d8254b8338f047ae443fef832e88c0a4899eee", "title": "Synthetic Ground Truth Generation for Evaluating Generative Policy Models", "abstract": "Generative Policy-based Models aim to enable a coalition of systems, be they devices or services to adapt according to contextual changes such as environmental factors, user preferences and different tasks whilst adhering to various constraints and regulations as directed by a managing party or the collective vision of the coalition. Recent developments have proposed new architectures to realize the potential of GPMs but as the complexity of systems and their associated requirements increases, there is an emerging requirement to have scenarios and associated datasets to realistically evaluate GPMs with respect to the properties of the operating environment, be it the future battlespace or an autonomous organization. In order to address this requirement, in this paper, we present a method of applying an agile knowledge representation framework to model requirements, both individualistic and collective that enables synthetic generation of ground truth data such that advanced GPMs can be evaluated robustly in complex environments. We also release conceptual models, annotated datasets, as well as means to extend the data generation approach so that similar datasets can be developed for varying complexities and different situations.", "venue": "ArXiv", "authors": ["Daniel  Cunnington", "Graham  White", "Geeth de Mel"], "year": 2019, "n_citations": 1}
{"id": 6030865, "s2_id": "20db5ac6e88e2457c82856354a2e5d521482f360", "title": "LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning", "abstract": "While pre-training and fine-tuning, e.g., BERT~\\citep{devlin2018bert}, GPT-2~\\citep{radford2019language}, have achieved great success in language understanding and generation tasks, the pre-trained models are usually too big for online deployment in terms of both memory cost and inference speed, which hinders them from practical online usage. In this paper, we propose LightPAFF, a Lightweight Pre-training And Fine-tuning Framework that leverages two-stage knowledge distillation to transfer knowledge from a big teacher model to a lightweight student model in both pre-training and fine-tuning stages. In this way the lightweight model can achieve similar accuracy as the big teacher model, but with much fewer parameters and thus faster online inference speed. LightPAFF can support different pre-training methods (such as BERT, GPT-2 and MASS~\\citep{song2019mass}) and be applied to many downstream tasks. Experiments on three language understanding tasks, three language modeling tasks and three sequence to sequence generation tasks demonstrate that while achieving similar accuracy with the big BERT, GPT-2 and MASS models, LightPAFF reduces the model size by nearly 5x and improves online inference speed by 5x-7x.", "venue": "ArXiv", "authors": ["Kaitao  Song", "Hao  Sun", "Xu  Tan", "Tao  Qin", "Jianfeng  Lu", "Hongzhi  Liu", "Tie-Yan  Liu"], "year": 2020, "n_citations": 4}
{"id": 6037838, "s2_id": "1026a37b56b4c9924a5604c522488aced90c0817", "title": "Scratch that! An Evolution-based Adversarial Attack against Neural Networks", "abstract": "We study black-box adversarial attacks for image classifiers in a constrained threat model, where adversaries can only modify a small fraction of pixels in the form of scratches on an image. We show that it is possible for adversaries to generate localized \\textit{adversarial scratches} that cover less than $5\\%$ of the pixels in an image and achieve targeted success rates of $98.77\\%$ and $97.20\\%$ on ImageNet and CIFAR-10 trained ResNet-50 models, respectively. We demonstrate that our scratches are effective under diverse shapes, such as straight lines or parabolic B\\a'ezier curves, with single or multiple colors. In an extreme condition, in which our scratches are a single color, we obtain a targeted attack success rate of $66\\%$ on CIFAR-10 with an order of magnitude fewer queries than comparable attacks. We successfully launch our attack against Microsoft's Cognitive Services Image Captioning API and propose various mitigation strategies.", "venue": "ArXiv", "authors": ["Malhar  Jere", "Briland  Hitaj", "Gabriela F. Cretu-Ciocarlie", "Farinaz  Koushanfar"], "year": 2019, "n_citations": 5}
{"id": 6068088, "s2_id": "20854a1c708b32631c08b7e95b6ec0f5ab4147b7", "title": "Revisiting Distillation and Incremental Classifier Learning", "abstract": "One of the key differences between the learning mechanism of humans and Artificial Neural Networks (ANNs) is the ability of humans to learn one task at a time. ANNs, on the other hand, can only learn multiple tasks simultaneously. Any attempts at learning new tasks incrementally cause them to completely forget about previous tasks. This lack of ability to learn incrementally, called Catastrophic Forgetting, is considered a major hurdle in building a true AI system. In this paper, our goal is to isolate the truly effective existing ideas for incremental learning from those that only work under certain conditions. To this end, we first thoroughly analyze the current state of the art (iCaRL) method for incremental learning and demonstrate that the good performance of the system is not because of the reasons presented in the existing literature. We conclude that the success of iCaRL is primarily due to knowledge distillation and recognize a key limitation of knowledge distillation, i.e, it often leads to bias in classifiers. Finally, we propose a dynamic threshold moving algorithm that is able to successfully remove this bias. We demonstrate the effectiveness of our algorithm on CIFAR100 and MNIST datasets showing near-optimal results. Our implementation is available at this https URL.", "venue": "ACCV", "authors": ["Khurram  Javed", "Faisal  Shafait"], "year": 2018, "n_citations": 38}
{"id": 6122654, "s2_id": "7651b17f40751ea76503f56340721cc843eda6f6", "title": "Hypernyms Through Intra-Article Organization in Wikipedia", "abstract": "We introduce a new measure for unsupervised hypernym detection and directionality. The motivation is to keep the measure computationally light and portatable across languages. We show that the relative physical location of words in explanatory articles captures the directionality property. Further, the phrases in section titles of articles about the word, capture the semantic similarity needed for hypernym detection task. We experimentally show that the combination of features coming from these two simple measures suffices to produce results comparable with the best unsupervised measures in terms of the average precision.", "venue": "ArXiv", "authors": ["Disha  Shrivastava", "Sreyash  Kenkre", "Santosh  Penubothula"], "year": 2018, "n_citations": 0}
{"id": 6122812, "s2_id": "dc5da5ac3aff86e4b0156c52d9641d05dc1eeace", "title": "MT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition using Deep Bidirectional Transformers", "abstract": "Conversational agents such as Cortana, Alexa and Siri are continuously working on increasing their capabilities by adding new domains. The support of a new domain includes the design and development of a number of NLU components for domain classification, intents classification and slots tagging (including named entity recognition). Each component only performs well when trained on a large amount of labeled data. Second, these components are deployed on limited-memory devices which requires some model compression. Third, for some domains such as the health domain, it is hard to find a single training data set that covers all the required slot types. To overcome these mentioned problems, we present a multi-task transformer-based neural architecture for slot tagging. We consider the training of a slot tagger using multiple data sets covering different slot types as a multi-task learning problem. The experimental results on the biomedical domain have shown that the proposed approach outperforms the previous state-of-the-art systems for slot tagging on the different benchmark biomedical datasets in terms of (time and memory) efficiency and effectiveness. The output slot tagger can be used by the conversational agent to better identify entities in the input utterances.", "venue": "ArXiv", "authors": ["Muhammad Raza Khan", "Morteza  Ziyadi", "Mohamed  AbdelHady"], "year": 2020, "n_citations": 15}
{"id": 6130191, "s2_id": "9130668c2927ee5989854e09ad565a2dd1bd6391", "title": "PDE-Driven Spatiotemporal Disentanglement", "abstract": "A recent line of work in the machine learning community addresses the problem of predicting high-dimensional spatiotemporal phenomena by leveraging specific tools from the differential equations theory. Following this direction, we propose in this article a novel and general paradigm for this task based on a resolution method for partial differential equations: the separation of variables. This inspiration allows us to introduce a dynamical interpretation of spatiotemporal disentanglement. It induces a principled model based on learning disentangled spatial and temporal representations of a phenomenon to accurately predict future observations. We experimentally demonstrate the performance and broad applicability of our method against prior state-of-the-art models on physical and synthetic video datasets.", "venue": "ICLR", "authors": ["J'er'emie  Dona", "Jean-Yves  Franceschi", "Sylvain  Lamprier", "Patrick  Gallinari"], "year": 2021, "n_citations": 9}
{"id": 6138335, "s2_id": "b60aece01679bec4a5167a57a58ad289c57c1ccb", "title": "Compositional Network Embedding", "abstract": "Network embedding has proved extremely useful in a variety of network analysis tasks such as node classification, link prediction, and network visualization. Almost all the existing network embedding methods learn to map the node IDs to their corresponding node embeddings. This design principle, however, hinders the existing methods from being applied in real cases. Node ID is not generalizable and, thus, the existing methods have to pay great effort in cold-start problem. The heterogeneous network usually requires extra work to encode node types, as node type is not able to be identified by node ID. Node ID carries rare information, resulting in the criticism that the existing methods are not robust to noise. \nTo address this issue, we introduce Compositional Network Embedding, a general inductive network representation learning framework that generates node embeddings by combining node features based on the principle of compositionally. Instead of directly optimizing an embedding lookup based on arbitrary node IDs, we learn a composition function that infers node embeddings by combining the corresponding node attribute embeddings through a graph-based loss. For evaluation, we conduct the experiments on link prediction under four different settings. The results verified the effectiveness and generalization ability of compositional network embeddings, especially on unseen nodes.", "venue": "ArXiv", "authors": ["Tianshu  Lyu", "Fei  Sun", "Peng  Jiang", "Wenwu  Ou", "Yan  Zhang"], "year": 2019, "n_citations": 0}
{"id": 6139035, "s2_id": "64e0ea7f851ec78ec34d7afdb1fee000e1b47a13", "title": "Ensuring Learning Guarantees on Concept Drift Detection with Statistical Learning Theory", "abstract": "Concept Drift (CD) detection intends to continuously identify changes in data stream behaviors, supporting researchers in the study and modeling of real-world phenomena. Motivated by the lack of learning guarantees in current CD algorithms, we decided to take advantage of the Statistical Learning Theory (SLT) to formalize the necessary requirements to ensure probabilistic learning bounds, so drifts would refer to actual changes in data rather than by chance. As discussed along this paper, a set of mathematical assumptions must be held in order to rely on SLT bounds, which are especially controversial in CD scenarios. Based on this issue, we propose a methodology to address those assumptions in CD scenarios and therefore ensure learning guarantees. Complementary, we assessed a set of relevant and known CD algorithms from the literature in light of our methodology. As main contribution, we expect this work to support researchers while designing and evaluating CD algorithms on different domains.", "venue": "ArXiv", "authors": ["Lucas  Pagliosa", "Rodrigo  Mello"], "year": 2020, "n_citations": 0}
{"id": 6145433, "s2_id": "b22a8e0253de400b2096e3faa4eb32e5ac4a8355", "title": "Reinforcement Learning In Two Player Zero Sum Simultaneous Action Games", "abstract": "Two player zero sum simultaneous action games are common in video games, financial markets, war, business competition, and many other settings. We first introduce the fundamental concepts of reinforcement learning in two player zero sum simultaneous action games and discuss the unique challenges this type of game poses. Then we introduce two novel agents that attempt to handle these challenges by using joint action Deep Q-Networks (DQN). The first agent, called the Best Response AgenT (BRAT), builds an explicit model of its opponent\u2019s policy using imitation learning, and then uses this model to find the best response to exploit the opponent\u2019s strategy. The second agent, Meta-Nash DQN, builds an implicit model of its opponent\u2019s policy in order to produce a context variable that is used as part of the Q-value calculation. An explicit minimax over Q-values is used to find actions close to Nash equilibrium. We find empirically that both agents converge to Nash equilibrium in a self-play setting for simple matrix games, while also performing well in games with larger state and action spaces. These novel algorithms are evaluated against vanilla RL algorithms as well as recent state of the art multi-agent and two agent algorithms. This work combines ideas from traditional reinforcement learning, game theory,", "venue": "ArXiv", "authors": ["Patrick  Phillips"], "year": 2021, "n_citations": 0}
{"id": 6146004, "s2_id": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd", "title": "Deep Reinforcement Learning from Human Preferences", "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.", "venue": "NIPS", "authors": ["Paul F. Christiano", "Jan  Leike", "Tom B. Brown", "Miljan  Martic", "Shane  Legg", "Dario  Amodei"], "year": 2017, "n_citations": 459}
{"id": 6148004, "s2_id": "a4de83f9ef1e809c8e9bdeb02bb41db0b98e0028", "title": "Exploring Memorization in Adversarial Training", "abstract": "It is well known that deep learning models have a propensity for fitting the entire training set even with random labels, which requires memorization of every training sample. In this paper, we investigate the memorization effect in adversarial training (AT) for promoting a deeper understanding of capacity, convergence, generalization, and especially robust overfitting of adversarially trained classifiers. We first demonstrate that deep networks have sufficient capacity to memorize adversarial examples of training data with completely random labels, but not all AT algorithms can converge under the extreme circumstance. Our study of AT with random labels motivates further analyses on the convergence and generalization of AT. We find that some AT methods suffer from a gradient instability issue, and the recently suggested complexity measures cannot explain robust generalization by considering models trained on random labels. Furthermore, we identify a significant drawback of memorization in AT that it could result in robust overfitting. We then propose a new mitigation algorithm motivated by detailed memorization analyses. Extensive experiments on various datasets validate the effectiveness of the proposed method.", "venue": "ArXiv", "authors": ["Yinpeng  Dong", "Ke  Xu", "Xiao  Yang", "Tianyu  Pang", "Zhijie  Deng", "Hang  Su", "Jun  Zhu"], "year": 2021, "n_citations": 4}
{"id": 6156779, "s2_id": "8a38e92a3a91f2634ac13ebbd6e33cb195309da9", "title": "Integrating Electrochemical Modeling with Machine Learning for Lithium-Ion Batteries", "abstract": "Mathematical modeling of lithium-ion batteries (LiBs) is a central challenge in advanced battery management. This paper presents a new approach to integrate a physics-based model with machine learning to achieve high-precision modeling for LiBs. This approach uniquely proposes to inform the machine learning model of the dynamic state of the physical model, enabling a deep integration between physics and machine learning. We propose two hybrid physics-machine learning models based on the approach, which blend a single particle model with thermal dynamics (SPMT) with a feedforward neural network (FNN) to perform physics-informed learning of a LiB's dynamic behavior. The proposed models are relatively parsimonious in structure and can provide considerable predictive accuracy even at high C-rates, as shown by extensive simulations.", "venue": "2021 American Control Conference (ACC)", "authors": ["Hao  Tu", "Scott  Moura", "Huazhen  Fang"], "year": 2021, "n_citations": 0}
{"id": 6172311, "s2_id": "b5cdff66fbe6a38f0b2802a66fc782cbb7a63bc0", "title": "Alleviating Privacy Attacks via Causal Learning", "abstract": "Machine learning models, especially deep neural networks have been shown to be susceptible to privacy attacks such as membership inference where an adversary can detect whether a data point was used for training a black-box model. Such privacy risks are exacerbated when a model's predictions are used on an unseen data distribution. To alleviate privacy attacks, we demonstrate the benefit of predictive models that are based on the causal relationships between input features and the outcome. We first show that models learnt using causal structure generalize better to unseen data, especially on data from different distributions than the train distribution. Based on this generalization property, we establish a theoretical link between causality and privacy: compared to associational models, causal models provide stronger differential privacy guarantees and are more robust to membership inference attacks. Experiments on simulated Bayesian networks and the colored-MNIST dataset show that associational models exhibit upto 80% attack accuracy under different test distributions and sample sizes whereas causal models exhibit attack accuracy close to a random guess.", "venue": "ICML", "authors": ["Shruti  Tople", "Amit  Sharma", "Aditya  Nori"], "year": 2020, "n_citations": 9}
{"id": 6177088, "s2_id": "24741d280869ad9c60321f5ab6e5f01b7852507d", "title": "Deep Speech: Scaling up end-to-end speech recognition", "abstract": "We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.", "venue": "ArXiv", "authors": ["Awni Y. Hannun", "Carl  Case", "Jared  Casper", "Bryan  Catanzaro", "Greg  Diamos", "Erich  Elsen", "Ryan  Prenger", "Sanjeev  Satheesh", "Shubho  Sengupta", "Adam  Coates", "Andrew Y. Ng"], "year": 2014, "n_citations": 1420}
{"id": 6185699, "s2_id": "bebaecd5800451dddd843703e94060f0463d8329", "title": "Visualizing image content to explain novel image discovery", "abstract": "The initial analysis of any large data set can be divided into two phases: (1) the identification of common trends or patterns and (2) the identification of anomalies or outliers that deviate from those trends. We focus on the goal of detecting observations with novel content, which can alert us to artifacts in the data set or, potentially, the discovery of previously unknown phenomena. To aid in interpreting and diagnosing the novel aspect of these selected observations, we recommend the use of novelty detection methods that generate explanations. In the context of large image data sets, these explanations should highlight what aspect of a given image is new (color, shape, texture, content) in a human-comprehensible form. We propose DEMUD-VIS, the first method for providing visual explanations of novel image content by employing a convolutional neural network (CNN) to extract image features, a method that uses reconstruction error to detect novel content, and an up-convolutional network to convert CNN feature representations back into image space. We demonstrate this approach on diverse images from ImageNet, freshwater streams, and the surface of Mars. Finally, we evaluate the utility of the visual explanations with a user study.", "venue": "Data Mining and Knowledge Discovery", "authors": ["Jake H. Lee", "Kiri L. Wagstaff"], "year": 2020, "n_citations": 1}
{"id": 6205200, "s2_id": "37df8ac712c44c2e0e3ab4c14c395e34f3cba733", "title": "Few Sample Knowledge Distillation for Efficient Network Compression", "abstract": "Deep neural network compression techniques such as pruning and weight tensor decomposition usually require fine-tuning to recover the prediction accuracy when the compression ratio is high. However, conventional fine-tuning suffers from the requirement of a large training set and the time-consuming training procedure. This paper proposes a novel solution for knowledge distillation from label-free few samples to realize both data efficiency and training/processing efficiency. We treat the original network as \"teacher-net\" and the compressed network as \"student-net\". A 1x1 convolution layer is added at the end of each layer block of the student-net, and we fit the block-level outputs of the student-net to the teacher-net by estimating the parameters of the added layers. We prove that the added layer can be merged without adding extra parameters and computation cost during inference. Experiments on multiple datasets and network architectures verify the method's effectiveness on student-nets obtained by various network pruning and weight decomposition methods. Our method can recover student-net's accuracy to the same level as conventional fine-tuning methods in minutes while using only 1% label-free data of the full training data.", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "authors": ["Tianhong  Li", "Jianguo  Li", "Zhuang  Liu", "Changshui  Zhang"], "year": 2020, "n_citations": 35}
{"id": 6207337, "s2_id": "55e7db76f9e426f97fc337539b09eaabbf1e9d55", "title": "Characterizing Online Engagement with Disinformation and Conspiracies in the 2020 U.S. Presidential Election", "abstract": "Identifying and characterizing disinformation in political discourse on social media is critical to ensure the integrity of elections and democratic processes around the world. Persistent manipulation of social media has resulted in increased concerns regarding the 2020 U.S. Presidential Election, due to its potential to influence individual opinions and social dynamics. In this work, we focus on the identification of distorted facts, in the form of unreliable and conspiratorial narratives in election-related tweets, to characterize discourse manipulation prior to the election. We apply a detection model to separate factual from unreliable (or conspiratorial) claims analyzing a dataset of 242 million election-related tweets. The identified claims are used to investigate targeted topics of disinformation, and conspiracy groups, most notably the far-right QAnon conspiracy group. Further, we characterize account engagements with unreliable and conspiracy tweets, and with the QAnon conspiracy group, by political leaning and tweet types. Finally, using a regression discontinuity design, we investigate whether Twitter\u2019s actions to curb QAnon activity on the platform were effective, and how QAnon accounts adapt to Twitter\u2019s restrictions.", "venue": "ArXiv", "authors": ["Karishma  Sharma", "Emilio  Ferrara", "Yan  Liu"], "year": 2021, "n_citations": 0}
{"id": 6213364, "s2_id": "b9be78b9b763e6ece9f2e9b2324739d528e35da4", "title": "Flexible mean field variational inference using mixtures of non-overlapping exponential families", "abstract": "Sparse models are desirable for many applications across diverse domains as they can perform automatic variable selection, aid interpretability, and provide regularization. When fitting sparse models in a Bayesian framework, however, analytically obtaining a posterior distribution over the parameters of interest is intractable for all but the simplest cases. As a result practitioners must rely on either sampling algorithms such as Markov chain Monte Carlo or variational methods to obtain an approximate posterior. Mean field variational inference is a particularly simple and popular framework that is often amenable to analytically deriving closed-form parameter updates. When all distributions in the model are members of exponential families and are conditionally conjugate, optimization schemes can often be derived by hand. Yet, I show that using standard mean field variational inference can fail to produce sensible results for models with sparsity-inducing priors, such as the spike-and-slab. Fortunately, such pathological behavior can be remedied as I show that mixtures of exponential family distributions with non-overlapping support form an exponential family. In particular, any mixture of a diffuse exponential family and a point mass at zero to model sparsity forms an exponential family. Furthermore, specific choices of these distributions maintain conditional conjugacy. I use two applications to motivate these results: one from statistical genetics that has connections to generalized least squares with a spike-and-slab prior on the regression coefficients; and sparse probabilistic principal component analysis. The theoretical results presented here are broadly applicable beyond these two examples.", "venue": "NeurIPS", "authors": ["Jeffrey P. Spence"], "year": 2020, "n_citations": 1}
{"id": 6217909, "s2_id": "7c5920c97f8bb1f91739b0d27746d655de95eedd", "title": "Neuronal Synchrony in Complex-Valued Deep Networks", "abstract": "Deep learning has recently led to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to cortical circuits. The challenge is to identify which neuronal mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile representations. \nWe introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter, we demonstrate the potential of the approach in several simple experiments. Thus, neuronal synchrony could be a flexible mechanism that fulfills multiple functional roles in deep networks.", "venue": "ICLR", "authors": ["David P. Reichert", "Thomas  Serre"], "year": 2014, "n_citations": 64}
{"id": 6220127, "s2_id": "e199f4c6f911c81baff64e63a3b04d636e5ecd81", "title": "Regularized Finite Dimensional Kernel Sobolev Discrepancy", "abstract": "We show in this note that the Sobolev Discrepancy introduced in Mroueh et al in the context of generative adversarial networks, is actually the weighted negative Sobolev norm $||.||_{\\dot{H}^{-1}(\\nu_q)}$, that is known to linearize the Wasserstein $W_2$ distance and plays a fundamental role in the dynamic formulation of optimal transport of Benamou and Brenier. Given a Kernel with finite dimensional feature map we show that the Sobolev discrepancy can be approximated from finite samples. Assuming this discrepancy is finite, the error depends on the approximation error in the function space induced by the finite dimensional feature space kernel and on a statistical error due to the finite sample approximation.", "venue": "ArXiv", "authors": ["Youssef  Mroueh"], "year": 2018, "n_citations": 3}
{"id": 6246604, "s2_id": "d63c7b3b86276a6085e8ff7a104a3fd8864b8c02", "title": "Induction of Selective Bayesian Classifiers", "abstract": "In this paper, we examine previous work on the naive Bayesian classifier and review its limitations, which include a sensitivity to correlated features. We respond to this problem by embedding the naive Bayesian induction scheme within an algorithm that carries out a greedy search through the space of features. We hypothesize that this approach will improve asymptotic accuracy in domains that involve correlated features without reducing the rate of learning in ones that do not. We report experimental results on six natural domains, including comparisons with decision-tree induction, that support these hypotheses. In closing, we discuss other approaches to extending naive Bayesian classifiers and outline some directions for future research.", "venue": "UAI", "authors": ["Pat  Langley", "Stephanie  Sage"], "year": 1994, "n_citations": 767}
{"id": 6251265, "s2_id": "262d20d74cf3a740225e2987d47a34869606e884", "title": "A Unified Approach to Robust Mean Estimation", "abstract": "In this paper, we develop connections between two seemingly disparate, but central, models in robust statistics: Huber's epsilon-contamination model and the heavy-tailed noise model. We provide conditions under which this connection provides near-statistically-optimal estimators. Building on this connection, we provide a simple variant of recent computationally-efficient algorithms for mean estimation in Huber's model, which given our connection entails that the same efficient sample-pruning based estimators is simultaneously robust to heavy-tailed noise and Huber contamination. Furthermore, we complement our efficient algorithms with statistically-optimal albeit computationally intractable estimators, which are simultaneously optimally robust in both models. We study the empirical performance of our proposed estimators on synthetic datasets, and find that our methods convincingly outperform a variety of practical baselines.", "venue": "ArXiv", "authors": ["Adarsh  Prasad", "Sivaraman  Balakrishnan", "Pradeep  Ravikumar"], "year": 2019, "n_citations": 15}
{"id": 6254894, "s2_id": "8f12c142dd892923872c003928d24a7b5c890e35", "title": "Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces", "abstract": "Pre-trained word representations became a key component in many NLP tasks. However, the global geometry of the word embeddings remains poorly understood. In this paper, we demonstrate that a typical word embeddings cloud is shaped as a high-dimensional simplex with interpretable vertices and propose a simple yet effective method for enumeration of these vertices. We show that the proposed method can detect and describe vertices of the simplex for GloVe and fasttext spaces. Intro and related works Neural networks for language processing have advanced rapidly in recent years. In particular, pre-trained word representations became a key component in many neural language understanding models. Word embeddings generated by neural network methods such as word2vec (Mikolov et al. 2013), glove (Pennington, Socher, and Manning 2014), fasttext (Mikolov et al. 2018) are well known to unsupervisedly exhibit linear analogical behavior (Mikolov, Yih, and Zweig 2013), (Levy and Goldberg 2014). Although such linguistic regularity suggests the existence of some global order, interpretable directions, and the regular shape of the embeddings cloud, however, nor PCA neither t-SNE doesn\u2019t reveal any clear macrostructure of the embeddings space (Figure 1, left). Typically, one uses a dataset with labeled words to guide a supervised search for directions and transformations of the embeddings space, meaningful in terms of sentiment analysis (Yu et al. 2017), semantic categories (Senel et al. 2018), (Hennigen, Williams, and Cotterell 2020), or even the sound symbolism (Yamshchikov, Shibaev, and Tikhonov 2019). Some works suggest that the embedding process should lead to a convex shape of the cloud, i.e. an n-dimensional simplex (Demeter, Kimmel, and Downey 2020). However, the global geometry of the word embeddings remains poorly understood and unsupervised analysis of the structure of embeddings is still an open field of research. In this paper, we demonstrate that a typical word embeddings cloud is shaped as a high-dimensional simplex with interpretable vertices and propose a simple yet effective method for enumeration of these vertices. We show that the proposed method can detect and describe vertices of the simplex for GloVe and fasttext spaces. Approach Our method relies on the following intuition: let\u2019s assume that the points in the embedding space fill a convex ndimensional polyhedron. If the density of distribution of points in space is regular enough, then the axes found by the PCA algorithm for the point cloud should be parallel to the axes connecting some corners of the polyhedron. This is obviously true at least for the case of the uniform density and the case when the density is decreasing as the distance to the geometric center of the cloud is growing. Instead of proving the validity of these assumptions, we search the vertices of the polytope and then check the found polytope for convexity. Now, if the axis found by the PCA is parallel to the axis connecting which two vertices (corners) of the polytope, then in the projection onto this axis, the extreme points on both ends will be points that located in the corners of the polytope. Each PCA axis gives us two vertices, while we can expect that the first found vertices will be the most contrasting in some sense. The found vertices then need to be cleaned from repetitions and false vertices.At the stage of cleaning, the algorithm compares the found vertices by top K nearest words and glues vertices that are similar above a certain threshold, and then for each vertex, it checks what percentage of words lies outside the triangle formed by this vertex with two other randomly selected ones. If the percentage of words outside the triangle is high, then the point is not a simplex vertex. Figure 1 (right) and Figure 2 show examples of projections onto the plane formed by several of the triplets of vertices for the fasttext and GloVe spaces. It\u2019s clear that: \u2022 the majority of the cloud\u2019s projection onto such a plane lies inside the triangle formed by vertices, i.e. the cloud appears to be an n-dimensional simplex; \u2022 the algorithm successfully finds the simplex vertices; \u2022 top K words of vertices are usually clearly interpretable. Results and Conclusion All experiments were carried out on publicly available GloVe and fasttext word embeddings. In both cases, we used spaces with 300 dimensions and top 50K words by frequency. In the case of both datasets, the method was able Figure 1: Projections of the same word embeddings space (fasttext): PCA (upper-left), t-SNE (bottom-left), our method (right). Figure 2: Examples of projections by our method for different word embeddings spaces: glove (left) and fasttext (right). to find the simplex vertices. The projections on the plane formed by the triplets of these angles (Figure 2) demonstrate an understandable macrostructure of space, in contrast to the standard PCA and t-SNE methods (Figure 1). For any triple of vertices on average 98% of the words are located inside the triangle of these vertices while projected on the plane of this triple (and 15% are located outside the incircle). The found vertices are usually well described by the top 5 of their words. In both spaces, among the first vertices, there were ones corresponding to dates, economics, toponyms, food, common names, religion, sport, art, etc. On the other hand, artifacts specific to the training dataset are also detected \u2013 parts of markup syntax, elements of the pronunciation alphabet, camel-cased words from news titles, etc. Figure 2 also demonstrates how cosine distance to such vertices can be used for the interpretation of other words. Our experiments show that the word embeddings cloud has the shape of the n-dimensional simplex with the dense center and a bunch of interpretable vertices. We present the method to find and describe the vertices of this simplex. The adaptation of the proposed method for the modern text embeddings spaces analysis is an item of future work.", "venue": "ArXiv", "authors": ["Alexey  Tikhonov"], "year": 2021, "n_citations": 0}
{"id": 6257512, "s2_id": "89d9ce6d973443a9a959af2b8529a8a296a479a2", "title": "Conformal Prediction Intervals for Neural Networks Using Cross Validation", "abstract": "Neural networks are among the most powerful nonlinear models used to address supervised learning problems. Similar to most machine learning algorithms, neural networks produce point predictions and do not provide any prediction interval which includes an unobserved response value with a specified probability. In this paper, we proposed the $k$-fold prediction interval method to construct prediction intervals for neural networks based on $k$-fold cross validation. Simulation studies and analysis of 10 real datasets are used to compare the finite-sample properties of the prediction intervals produced by the proposed method and the split conformal (SC) method. The results suggest that the proposed method tends to produce narrower prediction intervals compared to the SC method while maintaining the same coverage probability. Our experimental results also reveal that the proposed $k$-fold prediction interval method produces effective prediction intervals and is especially advantageous relative to competing approaches when the number of training observations is limited.", "venue": "ArXiv", "authors": ["Saeed  Khaki", "Dan  Nettleton"], "year": 2020, "n_citations": 1}
{"id": 6284043, "s2_id": "39b7fa66f34169832212ef062c58d20cb7cc5875", "title": "IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation", "abstract": "Text attribute transfer aims to automatically rewrite sentences such that they possess certain linguistic attributes, while simultaneously preserving their semantic content. This task remains challenging due to a lack of supervised parallel data. Existing approaches try to explicitly disentangle content and attribute information, but this is difficult and often results in poor content-preservation and ungrammaticality. In contrast, we propose a simpler approach, Iterative Matching and Translation (IMaT), which: (1) constructs a pseudo-parallel corpus by aligning a subset of semantically similar sentences from the source and the target corpora; (2) applies a standard sequence-to-sequence model to learn the attribute transfer; (3) iteratively improves the learned transfer function by refining imperfections in the alignment. In sentiment modification and formality transfer tasks, our method outperforms complex state-of-the-art systems by a large margin. As an auxiliary contribution, we produce a publicly-available test set with human-generated transfer references.", "venue": "EMNLP", "authors": ["Zhijing  Jin", "Di  Jin", "Jonas  Mueller", "Nicholas  Matthews", "Enrico  Santus"], "year": 2019, "n_citations": 29}
{"id": 6288803, "s2_id": "5fb20f31c07578f1398c12fa00261b4fbfd4b218", "title": "Interleaved Multitask Learning for Audio Source Separation with Independent Databases", "abstract": "Deep Neural Network-based source separation methods usually train independent models to optimize for the separation of individual sources. Although this can lead to good performance for well-defined targets, it can also be computationally expensive. The multitask alternative of a single network jointly optimizing for all targets simultaneously usually requires the availability of all target sources for each input. This requirement hampers the ability to create large training databases. In this paper, we present a model that decomposes the learnable parameters into a shared parametric model (encoder) and independent components (decoders) specific to each source. We propose an interleaved training procedure that optimizes the sub-task decoders independently and thus does not require each sample to possess a ground truth for all of its composing sources. Experimental results on MUSDB18 with the proposed method show comparable performance to independently trained models, with less trainable parameters, more efficient inference, and an encoder transferable to future target objectives. The results also show that using the proposed interleaved training procedure leads to better Source-to-Interference energy ratios when compared to the simultaneous optimization of all training objectives, even when all composing sources are available.", "venue": "ArXiv", "authors": ["Clement S. J. Doire", "Olumide  Okubadejo"], "year": 2019, "n_citations": 5}
{"id": 6303246, "s2_id": "0464413cc9b9dcbaad0cec602fa143d6a7a9e212", "title": "LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised Classification", "abstract": "Semi-supervised learning has received a lot of recent attention as it alleviates the need for large amounts of labelled data which can often be expensive, requires expert knowledge and be time consuming to collect. Recent developments in deep semi-supervised classification have reached unprecedented performance and the gap between supervised and semi-supervised learning is ever-decreasing. This improvement in performance has been based on the inclusion of numerous technical tricks, strong augmentation techniques and costly optimisation schemes with multi-term loss functions. We propose a new framework, LaplaceNet, for deep semi-supervised classification that has a greatly reduced model complexity. We utilise a hybrid energyneural network where graph based pseudo-labels, generated by minimising the graphical Laplacian, are used to iteratively improve a neural-network backbone. Our model outperforms state-of-the-art methods for deep semi-supervised classification, over several benchmark datasets. Furthermore, we consider the application of strong-augmentations to neural networks theoretically and justify the use of a multi-sampling approach for semi-supervised learning. We demonstrate, through rigorous experimentation, that a multi-sampling augmentation approach improves generalisation and reduces the sensitivity of the network to augmentation. Code coming soon!", "venue": "ArXiv", "authors": ["Philip  Sellars", "Angelica I. Avil\u00e9s-Rivero", "Carola-Bibiane  Sch\u00f6nlieb"], "year": 2021, "n_citations": 3}
{"id": 6321405, "s2_id": "8e1e7cab65a0676e0b5818a94ef73a4a51211ad2", "title": "Neural network algorithm and its application in temperature control of distillation tower", "abstract": "Distillation process is a complex process of conduction, mass transfer and heat conduction, which is mainly manifested as follows: The mechanism is complex and changeable with uncertainty; the process is multivariate and strong coupling; the system is nonlinear, hysteresis and time-varying. Therefore, traditional control methods are difficult to accurately control, but neural networks can greatly improve this problem. This article introduces the basic concepts of distillation tower temperature control, comprehensively introduces the application of various neural network algorithms in distillation tower temperature control, and compares their advantages and disadvantages and their effect. At present, there are many researches on neural network control of distillation tower temperature. The methods are different and each has its own merits. This article has carried out a systematic review to provide reference for the development of related industries.", "venue": "ArXiv", "authors": ["Ningrui  Zhao", "Jinwei  Lu"], "year": 2021, "n_citations": 7}
{"id": 6327230, "s2_id": "4d5f904f923e5e031fb500a9e9ef7699ea9283de", "title": "The Differentiable Cross-Entropy Method", "abstract": "We study the cross-entropy method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant that enables us to differentiate the output of CEM with respect to the objective function's parameters. In the machine learning setting this brings CEM inside of the end-to-end learning pipeline where this has otherwise been impossible. We show applications in a synthetic energy-based structured prediction task and in non-convex continuous control. In the control setting we show how to embed optimal action sequences into a lower-dimensional space. DCEM enables us to fine-tune CEM-based controllers with policy optimization.", "venue": "ICML", "authors": ["Brandon  Amos", "Denis  Yarats"], "year": 2020, "n_citations": 23}
{"id": 6332085, "s2_id": "db8b3d45537b0b6f15db9f8381bfb5e9681b65bc", "title": "On Kernel Method\u2013Based Connectionist Models and Supervised Deep Learning Without Backpropagation", "abstract": "We propose a novel family of connectionist models based on kernel machines and consider the problem of learning layer by layer a compositional hypothesis class (i.e., a feedforward, multilayer architecture) in a supervised setting. In terms of the models, we present a principled method to \u201ckernelize\u201d (partly or completely) any neural network (NN). With this method, we obtain a counterpart of any given NN that is powered by kernel machines instead of neurons. In terms of learning, when learning a feedforward deep architecture in a supervised setting, one needs to train all the components simultaneously using backpropagation (BP) since there are no explicit targets for the hidden layers (Rumelhart, Hinton, & Williams, 1986). We consider without loss of generality the two-layer case and present a general framework that explicitly characterizes a target for the hidden layer that is optimal for minimizing the objective function of the network. This characterization then makes possible a purely greedy training scheme that learns one layer at a time, starting from the input layer. We provide instantiations of the abstract framework under certain architectures and objective functions. Based on these instantiations, we present a layer-wise training algorithm for an l-layer feedforward network for classification, where l\u22652 can be arbitrary. This algorithm can be given an intuitive geometric interpretation that makes the learning dynamics transparent. Empirical results are provided to complement our theory. We show that the kernelized networks, trained layer-wise, compare favorably with classical kernel machines as well as other connectionist models trained by BP. We also visualize the inner workings of the greedy kernelized models to validate our claim on the transparency of the layer-wise algorithm.", "venue": "Neural Computation", "authors": ["Shiyu  Duan", "Shujian  Yu", "Yunmei  Chen", "Jose C. Principe"], "year": 2020, "n_citations": 10}
{"id": 6381274, "s2_id": "0c7b5e2cab4806552bc55bbd927c7565622fb9ab", "title": "k\u2010Space deep learning for reference\u2010free EPI ghost correction", "abstract": "Nyquist ghost artifacts in echo planar imaging (EPI) are originated from phase mismatch between the even and odd echoes. However, conventional correction methods using reference scans often produce erroneous results especially in high\u2010field MRI due to the nonlinear and time\u2010varying local magnetic field changes. Recently, it was shown that the problem of ghost correction can be reformulated as k\u2010space interpolation problem that can be solved using structured low\u2010rank Hankel matrix approaches. Another recent work showed that data driven Hankel matrix decomposition can be reformulated to exhibit similar structures as deep convolutional neural network. By synergistically combining these findings, we propose a k\u2010space deep learning approach that immediately corrects the phase mismatch without a reference scan in both accelerated and non\u2010accelerated EPI acquisitions.", "venue": "Magnetic resonance in medicine", "authors": ["Juyoung  Lee", "Yoseob  Han", "Jong Chul Ye"], "year": 2019, "n_citations": 15}
{"id": 6386837, "s2_id": "d944d7899f1a8160ec60473731a1d6744b6b1a84", "title": "Hyperprior Induced Unsupervised Disentanglement of Latent Representations", "abstract": "We address the problem of unsupervised disentanglement of latent representations learnt via deep generative models. In contrast to current approaches that operate on the evidence lower bound (ELBO), we argue that statistical independence in the latent space of VAEs can be enforced in a principled hierarchical Bayesian manner. To this effect, we augment the standard VAE with an inverse-Wishart (IW) prior on the covariance matrix of the latent code. By tuning the IW parameters, we are able to encourage (or discourage) independence in the learnt latent dimensions. Extensive experimental results on a range of datasets (2DShapes, 3DChairs, 3DFaces and CelebA) show our approach to outperform the \u03b2-VAE and is competitive with the state-of-the-art FactorVAE. Our approach achieves significantly better disentanglement and reconstruction on a new dataset (CorrelatedEllipses) which introduces correlations between the factors of variation.", "venue": "AAAI", "authors": ["Abdul Fatir Ansari", "Harold  Soh"], "year": 2019, "n_citations": 19}
{"id": 6388366, "s2_id": "428e040c24f92919d8827697011963a7a0ae5393", "title": "Self-awareness in intelligent vehicles: Feature based dynamic Bayesian models for abnormality detection", "abstract": "Abstract The evolution of Intelligent Transportation Systems in recent times necessitates the development of self-awareness in agents. Before the intensive use of Machine Learning, the detection of abnormalities was manually programmed by checking every variable and creating huge nested conditions that are very difficult to track. This paper aims to introduce a novel method to develop self-awareness in autonomous vehicles that mainly focuses on detecting abnormal situations around the considered agents. Multi-sensory time-series data from the vehicles are used to develop the data-driven Dynamic Bayesian Network (DBN) models used for future state prediction and the detection of dynamic abnormalities. Moreover, an initial level collective awareness model that can perform joint anomaly detection in co-operative tasks is proposed. The GNG algorithm learns the DBN models\u2019 discrete node variables; probabilistic transition links connect the node variables. A Markov Jump Particle Filter (MJPF) is applied to predict future states and detect when the vehicle is potentially misbehaving using learned DBNs as filter parameters. In this paper, datasets from real experiments of autonomous vehicles performing various tasks used to learn and test a set of switching DBN models.", "venue": "Robotics Auton. Syst.", "authors": ["Divya Thekke Kanapram", "Pablo  Mar\u00edn-Plaza", "Lucio  Marcenaro", "David  Mart\u00edn", "Arturo de la Escalera", "Carlo S. Regazzoni"], "year": 2020, "n_citations": 1}
{"id": 6391027, "s2_id": "92ea32858de6e9cd62288045673c215f3caf8add", "title": "Spoken Dialect Identification in Twitter using a Multi-filter Architecture", "abstract": "This paper presents our approach for SwissText & KONVENS 2020 shared task 2, which is a multi-stage neural model for Swiss German (GSW) identification on Twitter. Our model outputs either GSW or non-GSW and is not meant to be used as a generic language identifier. Our architecture consists of two independent filters where the first one favors recall, and the second one filter favors precision (both towards GSW). Moreover, we do not use binary models (GSW vs. not-GSW) in our filters but rather a multi-class classifier with GSW being one of the possible labels. Our model reaches F1-score of 0.982 on the test set of the shared task.", "venue": "SwissText/KONVENS", "authors": ["Mohammadreza  Banaei", "R'emi  Lebret", "Karl  Aberer"], "year": 2020, "n_citations": 1}
{"id": 6393078, "s2_id": "9204a01fc6ebe6823b2a801654481dfa97b2dd6a", "title": "BreathRNNet: Breathing Based Authentication on Resource-Constrained IoT Devices using RNNs", "abstract": "Recurrent neural networks (RNNs) have shown promising results in audio and speech processing applications due to their strong capabilities in modelling sequential data. In many applications, RNNs tend to outperform conventional models based on GMM/UBMs and i-vectors. Increasing popularity of IoT devices makes a strong case for implementing RNN based inferences for applications such as acoustics based authentication, voice commands, and edge analytics for smart homes. Nonetheless, the feasibility and performance of RNN based inferences on resources-constrained IoT devices remain largely unexplored. In this paper, we investigate the feasibility of using RNNs for an end-to-end authentication system based on breathing acoustics. We evaluate the performance of RNN models on three types of devices; smartphone, smartwatch, and Raspberry Pi and show that unlike CNN models, RNN models can be easily ported onto resource-constrained devices without a significant loss in accuracy.", "venue": "ArXiv", "authors": ["Jagmohan  Chauhan", "Suranga  Seneviratne", "Yining  Hu", "Archan  Misra", "Aruna  Seneviratne", "Youngki  Lee"], "year": 2017, "n_citations": 1}
{"id": 6396049, "s2_id": "b81edea765ed486b6def45821ec5950b38d76fd7", "title": "A simple data discretizer", "abstract": "Data discretization is an important step in the process of machine learning, since it is easier for classifiers to deal with discrete attributes rather than continuous attributes. Over the years, several methods of performing discretization such as Boolean Reasoning, Equal Frequency Binning, Entropy have been proposed, explored, and implemented. In this article, a simple supervised discretization approach is introduced. The prime goal of MIL is to maximize classification accuracy of classifier, minimizing loss of information while discretization of continuous attributes. The performance of the suggested approach is compared with the supervised discretization algorithm Minimum Information Loss (MIL), using the state-of-the-art rule inductive algorithms- J48 (Java implementation of C4.5 classifier). The presented approach is, indeed, the modified version of MIL. The empirical results show that the modified approach performs better in several cases in comparison to the original MIL algorithm and Minimum Description Length Principle (MDLP) .", "venue": "ArXiv", "authors": ["Gourab  Mitra", "Shashidhar  Sundareisan", "Bikash Kanti Sarkar"], "year": 2017, "n_citations": 3}
{"id": 6412743, "s2_id": "e50afeba9b6373357edbcf64fef0145c3f374642", "title": "Reinforced Epidemic Control: Saving Both Lives and Economy", "abstract": "Saving lives or economy is a dilemma for epidemic control in most cities while smart-tracing technology raises people's privacy concerns. In this paper, we propose a solution for the life-or-economy dilemma that does not require private data. We bypass the private-data requirement by suppressing epidemic transmission through a dynamic control on inter-regional mobility that only relies on Origin-Designation (OD) data. We develop DUal-objective Reinforcement-Learning Epidemic Control Agent (DURLECA) to search mobility-control policies that can simultaneously minimize infection spread and maximally retain mobility. DURLECA hires a novel graph neural network, namely Flow-GNN, to estimate the virus-transmission risk induced by urban mobility. The estimated risk is used to support a reinforcement learning agent to generate mobility-control actions. The training of DURLECA is guided with a well-constructed reward function, which captures the natural trade-off relation between epidemic control and mobility retaining. Besides, we design two exploration strategies to improve the agent's searching efficiency and help it get rid of local optimums. Extensive experimental results on a real-world OD dataset show that DURLECA is able to suppress infections at an extremely low level while retaining 76\\% of the mobility in the city. Our implementation is available at this https URL.", "venue": "ArXiv", "authors": ["Sirui  Song", "Zefang  Zong", "Yong  Li", "Xue  Liu", "Yang  Yu"], "year": 2020, "n_citations": 7}
{"id": 6429665, "s2_id": "127f464c2dc8d85b7612a6924495f79e5458710f", "title": "Move Evaluation in Go Using Deep Convolutional Neural Networks", "abstract": "Abstract: The game of Go is more challenging than other board games, due to the difficulty of constructing a position or move evaluation function. In this paper we investigate whether deep convolutional networks can be used to directly represent and learn this knowledge. We train a large 12-layer convolutional neural network by supervised learning from a database of human professional games. The network correctly predicts the expert move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GnuGo in 97% of games, and matched the performance of a state-of-the-art Monte-Carlo tree search that simulates a million positions per move.", "venue": "ICLR", "authors": ["Chris J. Maddison", "Aja  Huang", "Ilya  Sutskever", "David  Silver"], "year": 2015, "n_citations": 125}
{"id": 6433121, "s2_id": "3d60a2d7b8bb5b759b1e5e8e6e57202b0d5eaf3d", "title": "Country-wide high-resolution vegetation height mapping with Sentinel-2", "abstract": "Sentinel-2 multi-spectral images collected over periods of several months were used to estimate vegetation height for Gabon, respectively Switzerland. A deep convolutional network was trained to extract suitable spectral and textural features from reflectance images and to regress per-pixel vegetation height. In Gabon, reference heights for training and validation were derived from airborne LiDAR measurements. In Switzerland, reference heights were taken from an existing canopy height model derived via photogrammetric surface reconstruction. The resulting maps have a mean absolute error (MAE) of 1.7m in Switzerland, respectively 4.3m in Gabon, and correctly reproduce vegetation heights up to >50m. They also show good qualitative agreement with existing vegetation height maps. Our work demonstrates that, given a moderate amount of reference data, dense vegetation height maps with 10m ground sampling distance (GSD) can be derived at country scale from Sentinel-2 imagery.", "venue": "Remote Sensing of Environment", "authors": ["Nico  Lang", "Konrad  Schindler", "Jan Dirk Wegner"], "year": 2019, "n_citations": 35}
{"id": 6440212, "s2_id": "434d9e9735f48852a6d7b33d4013737dc15c487a", "title": "Optimal Decision-Theoretic Classification Using Non-Decomposable Performance Metrics", "abstract": "We provide a general theoretical analysis of expected out-of-sample utility, also referred to as decision-theoretic classification, for non-decomposable binary classification metrics such as F-measure and Jaccard coefficient. Our key result is that the expected out-of-sample utility for many performance metrics is provably optimized by a classifier which is equivalent to a signed thresholding of the conditional probability of the positive class. Our analysis bridges a gap in the literature on binary classification, revealed in light of recent results for non-decomposable metrics in population utility maximization style classification. Our results identify checkable properties of a performance metric which are sufficient to guarantee a probability ranking principle. We propose consistent estimators for optimal expected out-of-sample classification. As a consequence of the probability ranking principle, computational requirements can be reduced from exponential to cubic complexity in the general case, and further reduced to quadratic complexity in special cases. We provide empirical results on simulated and benchmark datasets evaluating the performance of the proposed algorithms for decision-theoretic classification and comparing them to baseline and state-of-the-art methods in population utility maximization for non-decomposable metrics.", "venue": "ArXiv", "authors": ["Nagarajan  Natarajan", "Oluwasanmi  Koyejo", "Pradeep  Ravikumar", "Inderjit S. Dhillon"], "year": 2015, "n_citations": 3}
{"id": 6440572, "s2_id": "a538e92e2804d39af36e37647af76eff6a1ce5a4", "title": "Joint Causal Inference from Multiple Contexts", "abstract": "The gold standard for discovering causal relations is by means of experimentation. Over the last decades, alternative methods have been proposed that can infer causal relations between variables from certain statistical patterns in purely observational data. We introduce Joint Causal Inference (JCI), a novel approach to causal discovery from multiple data sets from different contexts that elegantly unifies both approaches. JCI is a causal modeling framework rather than a specific algorithm, and it can be implemented using any causal discovery algorithm that can take into account certain background knowledge. JCI can deal with different types of interventions (e.g., perfect, imperfect, stochastic, etc.) in a unified fashion, and does not require knowledge of intervention targets or types in case of interventional data. We explain how several well-known causal discovery algorithms can be seen as addressing special cases of the JCI framework, and we also propose novel implementations that extend existing causal discovery methods for purely observational data to the JCI setting. We evaluate different JCI implementations on synthetic data and on flow cytometry protein expression data and conclude that JCI implementations can considerably outperform state-of-the-art causal discovery algorithms.", "venue": "J. Mach. Learn. Res.", "authors": ["Joris M. Mooij", "Sara  Magliacane", "Tom  Claassen"], "year": 2020, "n_citations": 73}
{"id": 6477330, "s2_id": "0bc900e7583199250818cf0abf63c05770c2a217", "title": "Sampling from Discrete Energy-Based Models with Quality/Efficiency Trade-offs", "abstract": "Energy-Based Models (EBMs) allow for extremely flexible specifications of probability distributions. However, they do not provide a mechanism for obtaining exact samples from these distributions. Monte Carlo techniques can aid us in obtaining samples if some proposal distribution that we can easily sample from is available. For instance, rejection sampling can provide exact samples but is often difficult or impossible to apply due to the need to find a proposal distribution that upperbounds the target distribution everywhere. Approximate Markov chain Monte Carlo sampling techniques like Metropolis-Hastings are usually easier to design, exploiting a local proposal distribution that performs local edits on an evolving sample. However, these techniques can be inefficient due to the local nature of the proposal distribution and do not provide an estimate of the quality of their samples. In this work, we propose a new approximate sampling technique, Quasi Rejection Sampling (QRS), that allows for a trade-off between sampling efficiency and sampling quality, while providing explicit convergence bounds and diagnostics. QRS capitalizes on the availability of high-quality global proposal distributions obtained from deep learning models. We demonstrate the effectiveness of QRS sampling for discrete EBMs over text for the tasks of controlled text generation with distributional constraints and paraphrase generation. We show that we can sample from such EBMs with arbitrary precision at the cost of sampling efficiency.", "venue": "ArXiv", "authors": ["Bryan  Eikema", "Germ'an  Kruszewski", "Hady  Elsahar", "Marc  Dymetman"], "year": 2021, "n_citations": 0}
{"id": 6503883, "s2_id": "d59c3aa8f30460ac9e1f2de40889b462db5cf3cc", "title": "Meta-learning via Language Model In-context Tuning", "abstract": "The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. To tackle this problem in NLP, we propose in-context tuning, which recasts adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, the labeled examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label from the input sequences on a collection of tasks. We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to first-order MAML which adapts the model with gradient descent, our method better leverages the inductive bias of LMs to perform pattern matching, and outperforms MAML by an absolute 6% AUC ROC score on BinaryClfs, with increasing advantage w.r.t. model size. Compared to nonfine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning directly learns to learn from in-context examples. On BinaryClfs, in-context tuning improves the average AUC-ROC score by an absolute 10%, and reduces the variance with respect to example ordering by 6x and example choices by 2x.", "venue": "ArXiv", "authors": ["Yanda  Chen", "Ruiqi  Zhong", "Sheng  Zha", "George  Karypis", "He  He"], "year": 2021, "n_citations": 1}
{"id": 6522515, "s2_id": "129327cdbd9e536408d652a506119d5b598a0cfd", "title": "Sample-efficient Cross-Entropy Method for Real-time Planning", "abstract": "Trajectory optimizers for model-based reinforcement learning, such as the Cross-Entropy Method (CEM), can yield compelling results even in high-dimensional control tasks and sparse-reward environments. However, their sampling inefficiency prevents them from being used for real-time planning and control. We propose an improved version of the CEM algorithm for fast planning, with novel additions including temporally-correlated actions and memory, requiring 2.7-22x less samples and yielding a performance increase of 1.2-10x in high-dimensional control problems.", "venue": "CoRL", "authors": ["Cristina  Pinneri", "Shambhuraj  Sawant", "Sebastian  Blaes", "Jan  Achterhold", "Joerg  Stueckler", "Michal  Rolinek", "Georg  Martius"], "year": 2020, "n_citations": 11}
{"id": 6539024, "s2_id": "f68d160d9d782d5ee614b478aa42466e95a96574", "title": "Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies", "abstract": "We consider off-policy policy evaluation when the trajectory data are generated by multiple behavior policies. Recent work has shown the key role played by the state or state-action stationary distribution corrections in the infinite horizon context for off-policy policy evaluation. We propose estimated mixture policy (EMP), a novel class of partially policy-agnostic methods to accurately estimate those quantities. With careful analysis, we show that EMP gives rise to estimates with reduced variance for estimating the state stationary distribution correction while it also offers a useful induction bias for estimating the state-action stationary distribution correction. In extensive experiments with both continuous and discrete environments, we demonstrate that our algorithm offers significantly improved accuracy compared to the state-of-the-art methods.", "venue": "ICLR", "authors": ["Xinyun  Chen", "Lu  Wang", "Yizhe  Hang", "Heng  Ge", "Hongyuan  Zha"], "year": 2020, "n_citations": 4}
{"id": 6557271, "s2_id": "403c69991bcfd4d37e77da69d264023d25456d5c", "title": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace", "abstract": "Affective computing has been largely limited in terms of available data resources. The need to collect and annotate diverse in-the-wild datasets has become apparent with the rise of deep learning models, as the default approach to address any computer vision task. Some in-the-wild databases have been recently proposed. However: i) their size is small, ii) they are not audiovisual, iii) only a small part is manually annotated, iv) they contain a small number of subjects, or v) they are not annotated for all main behavior tasks (valence-arousal estimation, action unit detection and basic expression classification). To address these, we substantially extend the largest available in-the-wild database (Aff-Wild) to study continuous emotions such as valence and arousal. Furthermore, we annotate parts of the database with basic expressions and action units. As a consequence, for the first time, this allows the joint study of all three types of behavior states. We call this database Aff-Wild2. We conduct extensive experiments with CNN and CNN-RNN architectures that use visual and audio modalities; these networks are trained on Aff-Wild2 and their performance is then evaluated on 10 publicly available emotion databases. We show that the networks achieve state-of-the-art performance for the emotion recognition tasks. Additionally, we adapt the ArcFace loss function in the emotion recognition context and use it for training two new networks on Aff-Wild2 and then re-train them in a variety of diverse expression recognition databases. The networks are shown to improve the existing state-of-the-art. The database, emotion recognition models and source code are available at this http URL.", "venue": "BMVC", "authors": ["Dimitrios  Kollias", "Stefanos  Zafeiriou"], "year": 2019, "n_citations": 97}
{"id": 6578580, "s2_id": "57e40b71dd64b98330eeebfaf8ec9c06b495a13f", "title": "Memorization in Deep Neural Networks: Does the Loss Function Matter?", "abstract": "Deep Neural Networks, often owing to the overparameterization, are shown to be capable of exactly memorizing even randomly labelled data. Empirical studies have also shown that none of the standard regularization techniques mitigate such overfitting. We investigate whether choice of loss function can affect this memorization. We empirically show, with benchmark data sets MNIST and CIFAR-10, that a symmetric loss function as opposed to either cross entropy or squared error loss results in significant improvement in the ability of the network to resist such overfitting. We then provide a formal definition for robustness to memorization and provide theoretical explanation as to why the symmetric losses provide this robustness. Our results clearly bring out the role loss functions alone can play in this phenomenon of memorization.", "venue": "PAKDD", "authors": ["Deep  Patel", "P. S. Sastry"], "year": 2021, "n_citations": 0}
{"id": 6586785, "s2_id": "d6559f35be0679c6b3371a2e44e3be293704b600", "title": "Deep Exponential Families", "abstract": "We describe \\textit{deep exponential families} (DEFs), a class of latent variable models that are inspired by the hidden structures used in deep neural networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We perform inference using recent \"black box\" variational inference techniques. We then evaluate various DEFs on text and combine multiple DEFs into a model for pairwise recommendation data. In an extensive study, we show that going beyond one layer improves predictions for DEFs. We demonstrate that DEFs find interesting exploratory structure in large data sets, and give better predictive performance than state-of-the-art models.", "venue": "AISTATS", "authors": ["Rajesh  Ranganath", "Linpeng  Tang", "Laurent  Charlin", "David M. Blei"], "year": 2015, "n_citations": 138}
{"id": 6594229, "s2_id": "e71b4358eda88cca3c684e4880e303363572a2e7", "title": "Semi-supervised structured output prediction by local linear regression and sub-gradient descent", "abstract": "We propose a novel semi-supervised structured output prediction method based on local linear regression in this paper. The existing semi-supervise structured output prediction methods learn a global predictor for all the data points in a data set, which ignores the differences of local distributions of the data set, and the effects to the structured output prediction. To solve this problem, we propose to learn the missing structured outputs and local predictors for neighborhoods of different data points jointly. Using the local linear regression strategy, in the neighborhood of each data point, we propose to learn a local linear predictor by minimizing both the complexity of the predictor and the upper bound of the structured prediction loss. The minimization problem is solved by sub-gradient descent algorithms. We conduct experiments over two benchmark data sets, and the results show the advantages of the proposed method.", "venue": "ArXiv", "authors": ["Yihua  Zhou", "Jingbin  Wang", "Lihui  Shi", "Haoxiang  Wang", "Xin  Du", "Guilherme  Silva"], "year": 2016, "n_citations": 2}
{"id": 6621999, "s2_id": "54d71afa5ec350958a920b637e37f78e2654563d", "title": "Deepfake Detection using Spatiotemporal Convolutional Networks", "abstract": "Better generative models and larger datasets have led to more realistic fake videos that can fool the human eye but produce temporal and spatial artifacts that deep learning approaches can detect. Most current Deepfake detection methods only use individual video frames and therefore fail to learn from temporal information. We created a benchmark of the performance of spatiotemporal convolutional methods using the Celeb-DF dataset. Our methods outperformed state-of-the-art frame-based detection methods. Code for our paper is publicly available at this https URL.", "venue": "ArXiv", "authors": ["Oscar de Lima", "Sean  Franklin", "Shreshtha  Basu", "Blake  Karwoski", "Annet  George"], "year": 2020, "n_citations": 22}
{"id": 6637935, "s2_id": "d03ca175e2b2745126e792fdc31dfadae4c63afa", "title": "A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks", "abstract": "Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.", "venue": "NeurIPS", "authors": ["Kimin  Lee", "Kibok  Lee", "Honglak  Lee", "Jinwoo  Shin"], "year": 2018, "n_citations": 555}
{"id": 6695995, "s2_id": "e50c56ba36f7c0358840c0e2eb259d685bcefc87", "title": "Pricing options and computing implied volatilities using neural networks", "abstract": "This paper proposes a data-driven approach, by means of an Artificial Neural Network (ANN), to value financial options and to calculate implied volatilities with the aim of accelerating the corresponding numerical methods. With ANNs being universal function approximators, this method trains an optimized ANN on a data set generated by a sophisticated financial model, and runs the trained ANN as an agent of the original solver in a fast and efficient way. We test this approach on three different types of solvers, including the analytic solution for the Black-Scholes equation, the COS method for the Heston stochastic volatility model and Brent\u2019s iterative root-finding method for the calculation of implied volatilities. The numerical results show that the ANN solver can reduce the computing time significantly.", "venue": "Risks", "authors": ["Shuaiqiang  Liu", "Cornelis W. Oosterlee", "Sander M. Bohte"], "year": 2019, "n_citations": 61}